<!DOCTYPE html>



<html lang="en-US" 
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Navigating the Unsupervised Learning Landscape" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Navigating the Unsupervised Learning Landscape" />
<meta property="og:description" content="Navigating the Unsupervised Learning Landscape" />
<link rel="canonical" href="http://localhost:4000/2017/05/04/unsupervised-landscape.html" />
<meta property="og:url" content="http://localhost:4000/2017/05/04/unsupervised-landscape.html" />
<meta property="og:site_name" content="Eugenio Culurciello Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-05-04T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Navigating the Unsupervised Learning Landscape" />
<meta name="twitter:site" content="@culurciello" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2017-05-04T00:00:00-04:00","datePublished":"2017-05-04T00:00:00-04:00","description":"Navigating the Unsupervised Learning Landscape","headline":"Navigating the Unsupervised Learning Landscape","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/05/04/unsupervised-landscape.html"},"url":"http://localhost:4000/2017/05/04/unsupervised-landscape.html"}</script>
<!-- End Jekyll SEO tag -->


  <title>Navigating the Unsupervised Learning Landscape | Eugenio Culurciello Blog
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://www.favicon-generator.org/
-->



<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">
<link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">

<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png">
<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png">
<link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png">

<link rel="icon" type="image/png" sizes="192x192"  href="/assets/img/favicons/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

<link rel="manifest" href="/assets/img/favicons/manifest.json">
<meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'>
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
  

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous">

  <!-- Font Awesome -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
    integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ="
    crossorigin="anonymous">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">




  <!-- JavaScripts -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>

  <!--
  JS selector for site.
-->


  





<script defer src="/assets/js/dist/post.min.js"></script>






</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">

  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/logo.jpeg" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Eugenio Culurciello Blog</a>
    </div>

    <div class="site-subtitle font-italic">Eugenio Culurciello Blog</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://github.com/culurciello" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github-alt"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/culurciello" aria-label="twitter"
        class="order-4"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['culurciello','gmail.com'].join('@')" aria-label="email"
        class="order-5"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        class="order-6"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    
      
        <span class="icon-border order-2"></span>
      

      <span id="mode-toggle-wrapper" class="order-1">
        <!--
  Switch the mode between dark and light.
-->

<i class="mode-toggle fas fa-adjust"></i>

<script type="text/javascript">

  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      var self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.updateMermaid();
      });

    } /* constructor() */


    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }

        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }

      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.updateMermaid();

    } /* flipMode() */

  } /* ModeToggle */

  let toggle = new ModeToggle();

  $(".mode-toggle").click(function() {

    toggle.flipMode();

  });

</script>

      </span>
    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center topbar-down">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          

        

      

        
        <span>
          
          
          <a href="/2017">
            2017
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/05">
            05
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/04">
            04
          </a>
        </span>

        

      

        
          <span>Navigating the Unsupervised Learning Landscape</span>

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->


<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->


<!-- Add attribute 'hide-bullet' to the checkbox list -->




  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->
  
  

  

  



<!-- return -->
<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>Navigating the Unsupervised Learning Landscape</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Eugenio Culurciello
          </span>
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Thu, May  4, 2017, 12:00 AM -0400"
  

  
  prep="on" >

  
  

  
    May  4, 2017
  

  <i class="unloaded">2017-05-04T00:00:00-04:00</i>

</span>

        </div>

        <div>
          <!-- lastmod -->
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2295 words">12 min</span>


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <h1 id="navigating-the-unsupervised-learning-landscape">Navigating the Unsupervised Learning Landscape</h1>

<p>Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with <strong>little data</strong>. Very little data.</p>

<p>Today Deep Learning models are trained on large supervised datasets.Meaning that for each data, there is a corresponding label. In the case of the popular ImageNet dataset, there are 1M images labeled by humans. 1000 images for each of the 1000 classes. It can take some effort to create such dataset, many months of work. Imagine now creating a dataset with 1M classes. Imagine having to label each frame of a video dataset, with 100M frames. This is <strong>not scalable</strong>.</p>

<p>Now, think about how you got trained when you were very little. Yes you got some supervision, but when your parents told you that is a “cat” they would not tell you “cat” every split second you were looking at a cat for the rest of your life! That is what supervised learning is today: I tell you over and over what a “cat” is, maybe 1M times. Then your Deep Learning model gets it.</p>

<p>Ideally, we would like to have a model that behaves more like our brain. That needs just a few labels here and there to make sense of the multitude of classes of the world.  And with classes I mean  objects classes, action classes, environment classes, object parts classes, and the list goes on and on.</p>

<p>As you will see in this review, the most successful models are the ones that predict future representation of a video. One issue that many of these techniques have, and are trying to resolve, is that  training for a good overall representation needs to be performed on <strong>videos</strong>, rather than still images.  This is the only way to apply the learned representation to real-life tasks.</p>

<h1 id="general-concepts">General concepts</h1>

<p>The main goal of unsupervised learning research is to pre-train a model (called “discriminator” or “encoder”) network to be used for other tasks.  The  encoder features should be general enough to be used in a categorization tasks:  for example to train on ImageNet and provide good results, as close as possible as supervised models.</p>

<p>Up to date, supervised models always perform better than unsupervised pre-trained models. That is because the supervision allows to model to better encode the characteristics of the dataset. But supervision can also be decremental if the model is then applied to other tasks.  In this regards, the hope is that unsupervised training can provide more general features for learning to perform any tasks.</p>

<p>If real-life applications are the targets, as in autonomous driving, action recognition, object detection and recognition in live feeds, then these algorithms need to be trained on video data.</p>

<h1 id="auto-encoders">Auto-encoders</h1>

<p>Originated largely from <a href="http://redwood.psych.cornell.edu/papers/olshausen_field_1997.pdf">Bruno Olshausen and David Field</a> in 1996. This paper showed that coding theory can be applied to the receptive field in the visual cortex. They showed that the primary visual vortex (V1) in our brain uses principles of sparsity to create a minimal set of base functions that can be also used to reconstruct the input image.</p>

<p>A great review is <a href="https://piotrmirowski.files.wordpress.com/2014/03/piotrmirowski_2014_reviewautoencoders.pdf">here</a></p>

<p>Yann LeCun group also worked a lot in  <a href="http://www.cs.nyu.edu/~yann/research/deep/">this area</a>. In this page you can see a great animation of how sparse filters V1-like are learned.</p>

<p>Stacked-auto encoders are also used, by repeating the process of traininggreedily layer by layer.</p>

<p>Auto-encoders methods are also called “direct-mapping” methods.</p>

<h1 id="auto-encoders--sparse-coding--stacked-auto-encoders-advantages-and-disadvantages">Auto-encoders / sparse-coding / stacked auto-encoders advantages and disadvantages</h1>

<p>Advantages:</p>

<ul>
  <li>simple technique: reconstruct input</li>
  <li>multiple layer can be stacked</li>
  <li>intuitive and based on neuroscience research</li>
</ul>

<p>Disadvantages:</p>

<ul>
  <li>each layer is trained greedily</li>
  <li>no global optimization</li>
  <li>does not match performance of supervised learning</li>
  <li>multiple layer ineffective</li>
  <li>reconstruction of input may not be ideal metric for learning a general-purpose representation</li>
</ul>

<h1 id="clustering-learning">Clustering Learning</h1>

<p>One technique uses k-means clustering to learn filters at multiple layers.</p>

<p>Our group named this technique: <a href="http://arxiv.org/abs/1301.2820">Clustering Learning</a>, <a href="http://arxiv.org/abs/1306.0152">Clustering Connections</a> and <a href="http://arxiv.org/abs/1511.06241">Convolutional Clustering</a>, which very recently achieved very good results on the popular STL-10 unsupervised dataset.</p>

<p>Our work in this area was developed independently to the work of  <a href="http://www-cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf">Adam Coates and Andrew Ng</a>.</p>

<p>Restricted Boltzmann machines (RBMs), deep Boltzmann machines (DBMs), <a href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">Deep belief networks (DBNs)</a> have been notoriously hard to train because of the numerical difficulties of solving their partition function. As such they have not been used widely to solve problems.</p>

<h1 id="clustering-learning-advantages-and-disadvantages">Clustering learning advantages and disadvantages:</h1>

<p>Advantages:</p>

<ul>
  <li>simple technique: cluster similar outputs</li>
  <li>multiple layer can be stacked</li>
  <li>intuitive and based on neuroscience research</li>
</ul>

<p>Disadvantages:</p>

<ul>
  <li>each layer is trained greedily</li>
  <li>no global optimization</li>
  <li>in some cases matches performance of supervised learning</li>
  <li>multiple layer increasingly ineffective == diminishing returns</li>
</ul>

<h1 id="generative-models">Generative models</h1>

<p>Generative models try to create a categorization (discriminator or encoder) network and a model that generates images (generative model) at the same time.  This method originated from the pioneering work of  <a href="http://arxiv.org/abs/1406.2661">Ian Goodfellow and Yoshua Bengio</a>. Here is a great and  <a href="https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Generative-Adversarial-Networks">recent summary of GAN by Ian</a>.</p>

<p>The generative adversarial model by Alec Radford, Luke Metz, Soumith Chintala named  <a href="https://arxiv.org/abs/1511.06434">DCGAN</a>  instantiates one such model that got really awesome results.</p>

<p>A good explanation of this model is  <a href="https://ishmaelbelghazi.github.io/ALI/">here</a>. See this system diagram:</p>

<p>The DCGAN discriminator is designed to tell if an input image is real, or coming from the dataset, or fake, coming from the generator. The generator takes a random noise vector (say 1024 values) at the input and generates an image.</p>

<p>In the DCGAN, the generator network is:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*j2mhYb69kgUbqP1N.jpg" alt="" /></p>

<p>while the discriminator is a standard neural network. See code below for details.</p>

<p>The key is to train both networks in parallel while not completely overfitting and thus copying the dataset.  The learned features need to generalize to unseen examples, so learning the dataset would not be of use.</p>

<p>Training code of DCGAN in Torch7 is also <a href="https://github.com/soumith/dcgan.torch">provided</a>, which allow for <a href="https://www.facebook.com/yann.lecun/posts/10153269667222143">great experimentation</a>.</p>

<p>After both generator and discriminator network are trained, one can use both. The main goal was to train a nice discriminator network to be used for other tasks, for example categorization on other datasets. The generator can be used to generate images out of random vectors. These images have very interesting properties. First of all, they offer smooth transitions from the input space. See an example here, where they show the images produced by moving between 9 random input vectors:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*e-wogcQm9ZIffSC8.jpg" alt="" /></p>

<p>Also the input vector space offers mathematical properties, showing the learned features are organized by similarity:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*NtL2VRkjZ3nzhdK_.jpg" alt="" /></p>

<p>The smooth space learned by the generator suggests that the discriminator also has similar properties, making it a great general feature extractor for encoding images.  This should help the typical problem of CNN trained in discontinuous image datasets, where <a href="https://arxiv.org/abs/1312.6199">adversarial noise makes them fail</a>.</p>

<p>A <a href="https://arxiv.org/pdf/1606.03498v1.pdf">recent update</a> to the GAN training, provided a 21% error rate on CIFAR-10 with only 1000 labeled samples.</p>

<p>A recent paper on  <a href="https://arxiv.org/abs/1606.03657">infoGAN</a>  was able to produce very sharp images with image features that can be dis-entangled and have more interesting meaning. They, however, did not report the performance of the learned features in a task or dataset for comparison.</p>

<p>Interesting summary on generative models are also <a href="https://openai.com/blog/generative-models/">here</a> and <a href="https://code.facebook.com/posts/1587249151575490/a-path-to-unsupervised-learning-through-adversarial-networks/">here</a>.</p>

<p>Another very interesting example is given <a href="https://arxiv.org/abs/1605.05396">here</a> where the authors use generative adversarial training to learn to produce images out of textual descriptions. See this example:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*p7Uz62sSON4c_GX0.jpg" alt="" /></p>

<p>What I appreciate the most about this work is that the network is using the textual description as input to the generator, as opposed to random vectors, and can thus control accurately the output of the generator. A picture of the network model is here:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*OQVnRQ_xmpZodLen.jpg" alt="" /></p>

<h1 id="generative-adversarial-model-advantages-and-disadvantages">Generative adversarial model advantages and disadvantages</h1>

<p>Advantages:</p>

<ul>
  <li>global training of entire network</li>
  <li>simple to code and implement</li>
</ul>

<p>Disadvantages:</p>

<ul>
  <li>hard to train, conversion problems</li>
  <li>in some cases matches performance of supervised learning</li>
  <li>need to prove usability of representation (a problem of ALL unsupervised algorithms)</li>
</ul>

<h1 id="learn-from-data-models">Learn-from-data models</h1>

<p>These models <strong>learn directly from unlabeled data</strong>, by <strong>devising unsupervised learning tasks that do not require labels</strong>, and learning aims at solving the task.</p>

<p><a href="http://arxiv.org/abs/1603.09246">Unsupervised learning of visual representations by solving jigsaw puzzles</a> is a clever trick. The author break the image into a puzzle and train a deep neural network to solve the puzzle. The resulting network has one of the highest performance of pre-trained networks.</p>

<p><a href="https://arxiv.org/abs/1511.06811">Unsupervised learning of visual representations from image patches and locality</a>  is also a clever trick.  Here they take two patches of the same image closely located. These patches are statistically of the same object. A third patch is taken from a random picture and location, statistically not of the same object as the other 2 patches. Then a deep neural network is trained to discriminate between 2 patches of same object or different objects. The resulting network has one of the highest performance of pre-trained networks.</p>

<p><a href="http://arxiv.org/abs/1604.03650">Unsupervised learning of visual representations from stereo image reconstructions</a> takes a stereo image, say the left frame, and reconstruct the right frame.  Albeit this work was not aimed at unsupervised learning, it can be! This method also generates interesting  <a href="https://github.com/piiswrong/deep3d">3D movies form stills</a>.</p>

<p><a href="http://arxiv.org/abs/1406.6909">Unsupervised Learning of Visual Representations using surrogate categories</a>uses patches of images to create a very large number of surrogate classes. These image patches are then augmented, and then used to train a supervised network based on the augmented surrogate classes. This gives one of the best results in unsupervised feature learning.</p>

<p><a href="http://arxiv.org/abs/1505.00687">Unsupervised Learning of Visual Representations using Videos</a>  uses an encoder-decoder LSTM pair. The encoder LSTM runs through a sequence of video frames to generate an internal representation. This representation is then decoded through another LSTM to produce a target sequence. To make this unsupervised, one way is to predict the same sequence as the input. Another way is to predict future frames.</p>

<p>Another paper (MIT: Vondrick, Torralba) using videos with very compelling results is  <a href="http://arxiv.org/abs/1504.08023">here</a>.  The great idea behind this work is to predict the representation of future frames from a video input. This is an elegant approach. The model used is here:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*5tj0NdCvdjDfjLzh.jpg" alt="" /></p>

<p>One problem of this technique is that a standard neural network trained on static frames is used to interpret a video. Such networks does not learn the temporal dynamics of video and the smooth transformation of objects moving in space. Thus we argue this network is ill-suited to predict future representations in video.</p>

<p>To overcome this issue, our group is creating a large video dataset  <a href="https://engineering.purdue.edu/elab/eVDS/">eVDS</a>created to train new  <a href="https://medium.com/@culurciello/a-new-kind-of-deep-neural-networks-749bcde19108">network models</a>  (recursive and feed-back) directly on video data.</p>

<h1 id="predictive-networks">Predictive networks</h1>

<p>Predictive deep neural networks are models that are designed to predict future representations of the future.</p>

<p>PredNet is a network designed to predict future frames in video. See great examples here:  <a href="https://coxlab.github.io/prednet/">https://coxlab.github.io/prednet/</a></p>

<p>PredNet is a very clever neural network model that in our opinion will have a major role in the future of neural networks.  PredNet learns a neural representation that extends beyond the single frames of supervised CNN.</p>

<p>PredNet combine bio-inspired bi-directional [models of the human brain] (https://papers.nips.cc/paper/1083-unsupervised-pixel-prediction.pdf). It uses  <a href="http://www.nature.com/neuro/journal/v2/n1/full/nn0199_79.html">predictive coding</a>  and using [feedback connections in neural models] (http://arxiv.org/abs/1608.03425). This is the PredNet module and example of 2 stacked layer:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*qpC55hirsGdR78yZ.jpg" alt="" /></p>

<p>This model also has the following advantages:</p>

<ul>
  <li>uses unlabeled data to train!</li>
  <li>embedded loss function computing error at each layer</li>
  <li>ability to perform online-learning by monitoring the error signal: when it is not able to predict an output, it knows it needs to learn it</li>
</ul>

<p>One issue with PredNet is that predicting the future input frame is a relatively easy task some simple motion-based filters at the first layer. In our experiments PredNet this learns to do a great job at re-constructing an input frame,  but higher layer do not learn a good representations. In our experiments, higher layer in fact are not able to solve simple tasks like categorization.</p>

<p>Predicting the future frame is in fact not necessary. What we would like to do is to predict future representations of next frames, just like <a href="https://arxiv.org/abs/1504.08023">Carl Vondrick does</a>.</p>

<p>A very  <a href="https://arxiv.org/abs/1607.06854">interesting model is the PVM</a>  from BrainCorporation. This model aims at capturing bidirectional and recurrent connections in the brain, and also provide local layer-wise training.</p>

<p>This model is very interesting because it offers connectivity similar to new  <a href="https://medium.com/@culurciello/a-new-kind-of-deep-neural-networks-749bcde19108">network models</a>  (recursive and feed-back connections). These connections provide temporal information and generative abilities.</p>

<p>It is also interesting because it can be trained locally, with each PVM unit trying to predict its future output, and adjusting only local weights appropriately. This is quite different from the way deep neural network are trained today: back-propagating errors throughput the entire network.</p>

<p>The PVM unit is given below. It has inputs from multiple lower layers, to which it also provides feedback, and lateral connections. The feedback is time-delayed to form recurrent loops.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/1*2Yrhvp-DM5PtmFHtrqiM-g.jpeg" alt="" /></p>

<p>More details about the PVM system are  <a href="http://blog.piekniewski.info/2016/11/04/predictive-vision-in-a-nutshell/">given here</a>.</p>

<h1 id="learning-features-by-looking-at-objects-move">Learning features by looking at objects move</h1>

<p>This  <a href="https://people.eecs.berkeley.edu/~pathak/unsupervised_video/">recent paper</a>  (April 2017) trains unsupervised models by looking at motion of objects in videos.  Motion is extracted as optical flow, and used as a segmentation mask for moving objects. Even though optical flow signal does not give anywhere close to a good segmentation mask, the averaging effect of training on a large data allow the resulting network to do a good job. Examples below.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2017-05-04-unsupervised-landscape/0*iJ9_xtxOY0o2Ed3U.jpg" alt="" /></p>

<p>This work is very exciting because it is follows <a href="http://www.sciencedirect.com/science/article/pii/S004269891100068X">neuroscience theories</a> of how the visual cortex develops by learning to segment moving objects.</p>

<h1 id="the-future">The future</h1>

<p>Is your to make.</p>

<p>Unsupervised training is very much an open topic, where you can make a large contribution by:</p>

<ul>
  <li>creating a new unsupervised task to train networks, e.g.: solve a puzzle, compare image patches, generate images, …)</li>
  <li>thinking of tasks that create great unsupervised features, e.g.: what is object and what is background, same on stereo images, same on video frames ~= similar to how our human visual system develops</li>
</ul>

<h1 id="about-the-author">About the author</h1>

<p>I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://e-lab.github.io/html/contact-eugenio-culurciello.html">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>, and more…</p>


      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
        

        <!-- tags -->
        

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          
          <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
          </div>
          

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=Navigating the Unsupervised Learning Landscape - Eugenio Culurciello Blog&url=http://localhost:4000/2017/05/04/unsupervised-landscape.html" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=Navigating the Unsupervised Learning Landscape - Eugenio Culurciello Blog&u=http://localhost:4000/2017/05/04/unsupervised-landscape.html" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://telegram.me/share?text=Navigating the Unsupervised Learning Landscape - Eugenio Culurciello Blog&url=http://localhost:4000/2017/05/04/unsupervised-landscape.html" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->

<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  














  

  

















  
  </div> <!-- .access -->

  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->




  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
          





  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/2023/12/05/Artificial-scientists.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Dec  5
  

  <i class="unloaded">2023-12-05T00:00:00-05:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Artificial scientists and autonomous laboratories</h3>
            <div class="text-muted small">
              <p>
                





                

designed by Dall-E-2

Artificial scientists and autonomous laboratories

What is next for ChatGPT?

One of the goal of machine learning is to reach the levels of artificial intelligence. AI, the ...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2023/05/24/Should-we-worry-about-AI.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    May 24
  

  <i class="unloaded">2023-05-24T00:00:00-04:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Should we worry about AI (machine-learning)?</h3>
            <div class="text-muted small">
              <p>
                





                Should we worry about AI (machine-learning)?

I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind…



Artificial intell...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2023/03/30/A-picture-is-worth-a-1000-words.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Mar 30
  

  <i class="unloaded">2023-03-30T00:00:00-04:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>A picture is worth a 1000 words</h3>
            <div class="text-muted small">
              <p>
                





                A picture is worth a 1000 words

Evolution of large language models into artificial brains through multiple sensory modalities



As surprising as it is our very human predictive abilities hide a p...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->



    <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/2017/05/01/snowflake.html" class="btn btn-outline-primary"
    prompt="Older">
    <p>Snowflake</p>
  </a>
  

  
  <a href="/2017/05/05/new-nn.html" class="btn btn-outline-primary"
    prompt="Newer">
    <p>A new kind of deep neural networks</p>
  </a>
  

</div>


    

    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



  <!--
  image lazy load: https://github.com/ApoorvSaxena/lozad.js
-->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script>

<script type="text/javascript">
  const imgs = document.querySelectorAll('.post-content img');
  const observer = lozad(imgs);
  observer.observe();
</script>




        <!-- The Footer -->

<footer>
  <div class="container pl-lg-4 pr-lg-4">
    <div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3">
      <div class="footer-left">
        <p class="mb-0">
          © 2023
          <a href="https://twitter.com/culurciello">Eugenio Culurciello</a>.
          <!--  -->
        </p>
      </div>

      <div class="footer-right">
        <p class="mb-0">

          <!--Using the <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> theme <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>. -->
        </p>
      </div>
    </div>
  </div>
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="http://localhost:4000{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


  </body>

</html>

