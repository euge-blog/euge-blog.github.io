<!DOCTYPE html>



<html lang="en-US" 
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="The fall of RNN / LSTM" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The fall of RNN / LSTM" />
<meta property="og:description" content="The fall of RNN / LSTM" />
<link rel="canonical" href="http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html" />
<meta property="og:url" content="http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html" />
<meta property="og:site_name" content="Eugenio Culurciello Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-04-13T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="The fall of RNN / LSTM" />
<meta name="twitter:site" content="@culurciello" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2018-04-13T00:00:00-04:00","datePublished":"2018-04-13T00:00:00-04:00","description":"The fall of RNN / LSTM","headline":"The fall of RNN / LSTM","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html"},"url":"http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html"}</script>
<!-- End Jekyll SEO tag -->


  <title>The fall of RNN / LSTM | Eugenio Culurciello Blog
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://www.favicon-generator.org/
-->



<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">
<link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">

<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png">
<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png">
<link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png">

<link rel="icon" type="image/png" sizes="192x192"  href="/assets/img/favicons/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

<link rel="manifest" href="/assets/img/favicons/manifest.json">
<meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'>
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
  

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous">

  <!-- Font Awesome -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
    integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ="
    crossorigin="anonymous">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">




  <!-- JavaScripts -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>

  <!--
  JS selector for site.
-->


  





<script defer src="/assets/js/dist/post.min.js"></script>






</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">

  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/logo.jpeg" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Eugenio Culurciello Blog</a>
    </div>

    <div class="site-subtitle font-italic">Eugenio Culurciello Blog</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://github.com/culurciello" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github-alt"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/culurciello" aria-label="twitter"
        class="order-4"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['culurciello','gmail.com'].join('@')" aria-label="email"
        class="order-5"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        class="order-6"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    
      
        <span class="icon-border order-2"></span>
      

      <span id="mode-toggle-wrapper" class="order-1">
        <!--
  Switch the mode between dark and light.
-->

<i class="mode-toggle fas fa-adjust"></i>

<script type="text/javascript">

  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      var self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.updateMermaid();
      });

    } /* constructor() */


    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }

        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }

      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.updateMermaid();

    } /* flipMode() */

  } /* ModeToggle */

  let toggle = new ModeToggle();

  $(".mode-toggle").click(function() {

    toggle.flipMode();

  });

</script>

      </span>
    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center topbar-down">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          

        

      

        
        <span>
          
          
          <a href="/2018">
            2018
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/04">
            04
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/13">
            13
          </a>
        </span>

        

      

        
          <span>The fall of RNN / LSTM</span>

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->


<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->


<!-- Add attribute 'hide-bullet' to the checkbox list -->




  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->
  
  

  

  



<!-- return -->
<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>The fall of RNN / LSTM</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Eugenio Culurciello
          </span>
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Fri, Apr 13, 2018, 12:00 AM -0400"
  

  
  prep="on" >

  
  

  
    Apr 13, 2018
  

  <i class="unloaded">2018-04-13T00:00:00-04:00</i>

</span>

        </div>

        <div>
          <!-- lastmod -->
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1864 words">10 min</span>


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <h1 id="the-fall-of-rnn--lstm">The fall of RNN / LSTM</h1>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-04-13-Fall-RNN-LSTM/1*Fr-q6mQNYjJr6hTglTic7g.jpeg" alt="" /></p>

<p>We fell for Recurrent neural networks (RNN), Long-short term memory (LSTM), and all their variants.  <strong>Now it is time to drop them!</strong></p>

<p>It is the year 2014 and LSTM and RNN make a great come-back from the dead. We all read  <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Colah’s blog</a>  and  <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy’s ode to RNN</a>. But we were all young and unexperienced. For a few years this was the way to solve sequence learning, sequence translation (seq2seq), which also resulted in amazing results in speech to text comprehension and the raise of  <a href="http://www.businessinsider.com/apples-siri-using-neural-networks-2016-8">Siri</a>,  <a href="https://venturebeat.com/2014/04/03/behind-cortana-how-microsoft-aims-to-stand-out-with-its-personal-assistant/">Cortana</a>,  <a href="https://cloud.google.com/speech-to-text/">Google voice assistant</a>,  <a href="https://www.zdnet.com/article/amazon-echo-the-four-hard-problems-amazon-had-to-solve-to-make-it-work/">Alexa</a>. Also let us not forget machine translation, which resulted in the ability to translate documents into different languages or  <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">neural machine translation</a>, but also translate  <a href="https://arxiv.org/abs/1502.03044">images into text</a>,  <a href="https://arxiv.org/abs/1511.02793">text into images</a>, and  <a href="https://www.cs.utexas.edu/~vsub/naacl15_project.html">captioning video</a>, and … well you got the idea.</p>

<p>Then in the following years (2015–16) came  <a href="https://arxiv.org/abs/1512.03385">ResNet</a>  and  <a href="https://arxiv.org/abs/1502.03044">Attention</a>. One could then better understand that LSTM were a clever bypass technique. Also attention showed that MLP network could be replaced by  <em>averaging</em>  networks influenced by a  <em>context vector</em>. More on this later.</p>

<p>It only took 2 more years, but today we can definitely say:</p>

<blockquote>
  <p>“Drop your RNN and LSTM, they are no good!”</p>
</blockquote>

<p>But do not take our words for it, also see evidence that Attention based networks are used more and more by  <a href="https://arxiv.org/abs/1706.03762">Google</a>,  <a href="https://code.facebook.com/posts/1978007565818999/a-novel-approach-to-neural-machine-translation/">Facebook</a>,  <a href="https://einstein.ai/research/non-autoregressive-neural-machine-translation">Salesforce</a>, to name a few. All these companies have replaced RNN and variants for attention based models, and it is just the beginning. RNN have the days counted in all applications, because they require more resources to train and run than attention-based models. See  <a href="https://towardsdatascience.com/memory-attention-sequences-37456d271992">this post</a>  for more info.</p>

<h1 id="but-why">But why?</h1>

<p>Remember RNN and LSTM and derivatives use mainly sequential processing over time. See the horizontal arrow in the diagram below:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-04-13-Fall-RNN-LSTM/1*NKhwsOYNUT5xU7Pyf6Znhg.png" alt="" /></p>

<p>Sequential processing in RNN, from:  <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>

<p>This arrow means that long-term information has to sequentially travel through all cells before getting to the present processing cell. This means it can be easily corrupted by being multiplied many time by small numbers &lt; 0. This is the cause of  <a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/">vanishing gradients</a>.</p>

<p>To the rescue, came the LSTM module, which today can be seen as multiple switch gates, and a bit like  <a href="https://arxiv.org/abs/1512.03385">ResNet</a>  it can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-04-13-Fall-RNN-LSTM/1*J5W8FrASMi93Z81NlAui4w.png" alt="" /></p>

<p>Sequential processing in LSTM, from:  <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>

<p>But not all of it, as you can see from the figure above. Still we have a sequential path from older past cells to the current one. In fact the path is now even more complicated, because it has additive and forget branches attached to it. No question LSTM and GRU and derivatives are able to learn a lot of longer term information! See results  <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">here</a>; but they can remember sequences of 100s, not 1000s or 10,000s or more.</p>

<p>And one issue of RNN is that they are  <a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5">not hardware friendly</a>. Let me explain: it takes a lot of resources we do not have to train these network fast. Also it takes much resources to run these model in the cloud, and given that the demand for speech-to-text is growing rapidly, the cloud is not scalable. We will need to process at the edge, right into the Amazon Echo! See note below for more details.</p>

<h1 id="what-do-you-do">What do you do?</h1>

<p>At this time (September 2018) I would seriously consider this approach  <a href="https://arxiv.org/abs/1808.03867">here</a>. This is a 2D convolutional based neural network with causal convolution that can outperform both RNN/LSTM and Attention based models like the  <a href="https://arxiv.org/abs/1706.03762">Transformer</a>.</p>

<p>The  <a href="https://arxiv.org/abs/1706.03762">Transformer</a>  has definitely been a great suggestion from 2017 until the paper above. It has great advantages in training and in number of parameters, as we discussed  <a href="https://towardsdatascience.com/memory-attention-sequences-37456d271992">here</a>.</p>

<p><strong>Alternatively:</strong>  If sequential processing is to be avoided, then we can find units that “look-ahead” or better “look-back”, since most of the time we deal with real-time causal data where we know the past and want to affect future decisions. Not so in translating sentences, or analyzing recorded videos, for example, where we have all data and can reason on it more time. Such look-back/ahead units are neural attention modules, which we previously explained  <a href="https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d">here</a>.</p>

<p>To the rescue, and combining multiple neural attention modules, comes the “<em>hierarchical neural attention encoder</em>”, shown in the figure below:</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-04-13-Fall-RNN-LSTM/1*I5s9DOjKW3QTKaT0tHrNLg.png" alt="" /></p>

<p>Hierarchical neural attention encoder</p>

<p>A better way to look into the past is to use attention modules to summarize all past encoded vectors into a context vector Ct.</p>

<p>Notice there is a hierarchy of attention modules here, very similar to the hierarchy of neural networks. This is also similar to  <a href="https://arxiv.org/abs/1803.01271">Temporal convolutional network (TCN)</a>, reported in Note 3 below.</p>

<p>In the  <em>hierarchical neural attention encoder</em> multiple layers of attention can look at a small portion of recent past, say 100 vectors, while layers above can look at 100 of these attention modules, effectively integrating the information of 100 x 100 vectors. This extends the ability of the  <em>hierarchical neural attention encoder</em>  to 10,000 past vectors.</p>

<blockquote>
  <p>This is the way to look back more into the past and be able to influence the future.</p>
</blockquote>

<p>But more importantly look at the length of the path needed to propagate a representation vector to the output of the network: in hierarchical networks it is proportional to  <em>log(N)</em> where N are the number of hierarchy layers. This is in contrast to the T steps that a RNN needs to do, where T is the maximum length of the sequence to be remembered, and T » N.</p>

<blockquote>
  <p>It is easier to remember sequences if you hop 3–4 times, as opposed to hopping 100 times!</p>
</blockquote>

<p>This architecture is similar to a  <a href="https://arxiv.org/abs/1410.5401">neural Turing machine</a>, but lets the neural network decide what is read out from memory via attention. This means an actual neural network will decide which vectors from the past are important for future decisions.</p>

<p>But what about storing to memory? The architecture above stores all previous representation in memory, unlike neural Turning machines. This can be rather inefficient: think about storing the representation of every frame in a video — most times the representation vector does not change frame-to-frame, so we really are storing too much of the same! What can we do is add another unit to prevent correlated data to be stored. For example by not storing vectors too similar to previously stored ones. But this is really a hack, the best would be to be let the application guide what vectors should be saved or not. This is the focus of current research studies. Stay tuned for more information.</p>

<blockquote>
  <p>So in summary forget RNN and variants. Use attention. Attention really is all you need!</p>
</blockquote>

<p><strong>Tell your friends!</strong> It is very surprising to us to see so many companies still use RNN/LSTM for speech to text, many unaware that these networks are so inefficient and not scalable. Please tell them about this post.</p>

<h1 id="additional-information"><strong>Additional information</strong></h1>

<p><strong>About training RNN/LSTM:</strong>  RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designer and ultimately limits the applicability of neural networks solutions. In short, LSTM require 4 linear layer (MLP layer) per cell to run at and for each sequence time-step. Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units. And it is easy to add more computational units, but hard to add more memory bandwidth (note enough lines on a chip, long wires from processors to memory, etc). As a result, RNN/LSTM and variants are not a good match for hardware acceleration, and we talked about this issue before  <a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5">here</a>  and  <a href="https://towardsdatascience.com/memory-attention-sequences-37456d271992">here</a>. A solution will be compute in memory-devices like the ones we work on at  <a href="http://fwdnxt.com/">FWDNXT.</a></p>

<h1 id="notes">Notes</h1>

<p><strong>Note 1:</strong>  <em>Hierarchical neural attention</em>  is similar to the ideas in  <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WaveNet</a>. But instead of a convolutional neural network we use hierarchical attention modules. Also:  <em>Hierarchical neural attention</em>  can be also bi-directional.</p>

<p><strong>Note 2:</strong>  RNN and LSTM are memory-bandwidth limited problems (<a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5">see this for details</a>). The processing unit(s) need as much memory bandwidth as the number of operations/s they can provide, making it impossible to fully utilize them! The external bandwidth is never going to be enough, and a way to slightly ameliorate the problem is to use internal fast caches with high bandwidth. The best way is to use techniques that do not require large amount of parameters to be moved back and forth from memory, or that can be re-used for multiple computation per byte transferred (high arithmetic intensity).</p>

<p><strong>Note 3</strong>: Here is a  <a href="https://arxiv.org/abs/1803.01271">paper comparing CNN to RNN</a>. Temporal convolutional network (TCN) “outperform canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory”.</p>

<p><strong>Note 4</strong>: Related to this topic, is the fact that  <a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004592">we know little</a>  of how our human brain learns and remembers sequences. “We often learn and recall long sequences in smaller segments, such as a phone number 858 534 22 30 memorized as four segments. Behavioral experiments suggest that humans and some animals employ this strategy of breaking down cognitive or behavioral sequences into chunks in a wide variety of tasks” — these chunks remind me of small convolutional or attention like networks on smaller sequences, that then are hierarchically strung together like in the  <em>hierarchical neural attention encoder</em>  and Temporal convolutional network (TCN).  <a href="https://www.ncbi.nlm.nih.gov/pubmed/23541152">More studies</a>  make me think that working memory is similar to RNN networks that uses recurrent real neuron networks, and their capacity is very low. On the other hand both the cortex and  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3242327/">hippocampus</a>  give us the ability to remember really long sequences of steps (like: where did I park my car at airport 5 days ago), suggesting that more parallel pathways may be involved to recall long sequences, where attention mechanism gate important chunks and force hops in parts of the sequence that is not relevant to the final goal or task.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-04-13-Fall-RNN-LSTM/1*hghzwOLy6IBZ7fTwkWuuVQ.jpeg" alt="" /></p>

<p><strong>Note 5:</strong>  The above evidence shows we do not read sequentially, in fact we interpret characters, words and sentences as a group. An attention-based or convolutional module perceives the sequence and projects a representation in our mind. We would not be misreading this if we processed this information sequentially! We would stop and notice the inconsistencies!</p>

<p><strong>Note 6:</strong>  A  <a href="https://blog.openai.com/language-unsupervised/">recent paper</a>  trained unsupervised with attention/transformer and showed amazing performance in transfer learning. The VGG of NLP? This works is also an extension of  <a href="https://arxiv.org/abs/1801.06146">pioneering work by Jeremy and Sebastian</a>, where an LSTM with ad-hoc training procedures was able to learn unsupervised to predict the next word in a sequence of text, and then also able to transfer that knowledge to new tasks. A comparison of the effectiveness of LSTM and Transformer (attention based) is given  <a href="https://blog.openai.com/language-unsupervised/">here</a>  and shows that attention is usually attention wins, and that “The LSTM only<br />
outperforms the Transformer on one dataset — MRPC.”</p>

<p><strong>Note7</strong>:  <a href="https://jalammar.github.io/illustrated-transformer/">Here</a>  you can find a great explanation of the Transformer architecture and data flow!</p>

<h1 id="about-the-author">About the author</h1>

<p>I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://e-lab.github.io/html/contact-eugenio-culurciello.html">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>, and more…</p>


      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
        

        <!-- tags -->
        

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          
          <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
          </div>
          

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=The fall of RNN / LSTM - Eugenio Culurciello Blog&url=http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=The fall of RNN / LSTM - Eugenio Culurciello Blog&u=http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://telegram.me/share?text=The fall of RNN / LSTM - Eugenio Culurciello Blog&url=http://localhost:4000/2018/04/13/Fall-RNN-LSTM.html" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->

<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  














  

  

















  
  </div> <!-- .access -->

  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->




  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
          





  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/2025/01/03/AI-in-2025.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Jan  3
  

  <i class="unloaded">2025-01-03T00:00:00-05:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Artificial Intelligence, AI in 2025 and beyond</h3>
            <div class="text-muted small">
              <p>
                





                Artificial Intelligence, AI in 2025 and beyond

SOME THOUGHTS…

We are reaching the second quarter of the 21st century: it is an exciting milestone. I have been in “adult” life in the last 25 years...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2024/12/17/what-about-my-hair.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Dec 17, 2024
  

  <i class="unloaded">2024-12-17T00:00:00-05:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>What about my hair?</h3>
            <div class="text-muted small">
              <p>
                





                What about my hair?

For all the advances in technology and all the promises of space exploration I still think we are behind in our understanding of life and our own abilities.

For decades we tal...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2024/10/31/llm-use-cases.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Oct 31, 2024
  

  <i class="unloaded">2024-10-31T00:00:00-04:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Insights into the use of Generative AI systems</h3>
            <div class="text-muted small">
              <p>
                





                Insights into the use of Generative AI systems

Generative AI models, such as large language models (LLMs), have demonstrated remarkable capabilities in mimicking human-like communication. We here ...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->



    <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/2018/04/12/tracking-nn.html" class="btn btn-outline-primary"
    prompt="Older">
    <p>Tracking with deep networks</p>
  </a>
  

  
  <a href="/2018/06/13/notes-transformers.html" class="btn btn-outline-primary"
    prompt="Newer">
    <p>Notes on Transformer neural network for sequence to sequence learning</p>
  </a>
  

</div>


    

    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



  <!--
  image lazy load: https://github.com/ApoorvSaxena/lozad.js
-->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script>

<script type="text/javascript">
  const imgs = document.querySelectorAll('.post-content img');
  const observer = lozad(imgs);
  observer.observe();
</script>




        <!-- The Footer -->

<footer>
  <div class="container pl-lg-4 pr-lg-4">
    <div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3">
      <div class="footer-left">
        <p class="mb-0">
          © 2025
          <a href="https://twitter.com/culurciello">Eugenio Culurciello</a>.
          <!--  -->
        </p>
      </div>

      <div class="footer-right">
        <p class="mb-0">

          <!--Using the <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> theme <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>. -->
        </p>
      </div>
    </div>
  </div>
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="http://localhost:4000{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


  </body>

</html>

