<!DOCTYPE html>



<html lang="en-US" 
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Artificial Intelligence, AI in 2018 and beyond" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Artificial Intelligence, AI in 2018 and beyond" />
<meta property="og:description" content="Artificial Intelligence, AI in 2018 and beyond" />
<link rel="canonical" href="http://localhost:4000/2018/01/03/AI-in-2018.html" />
<meta property="og:url" content="http://localhost:4000/2018/01/03/AI-in-2018.html" />
<meta property="og:site_name" content="Eugenio Culurciello Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-01-03T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Artificial Intelligence, AI in 2018 and beyond" />
<meta name="twitter:site" content="@culurciello" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2018-01-03T00:00:00-05:00","datePublished":"2018-01-03T00:00:00-05:00","description":"Artificial Intelligence, AI in 2018 and beyond","headline":"Artificial Intelligence, AI in 2018 and beyond","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/01/03/AI-in-2018.html"},"url":"http://localhost:4000/2018/01/03/AI-in-2018.html"}</script>
<!-- End Jekyll SEO tag -->


  <title>Artificial Intelligence, AI in 2018 and beyond | Eugenio Culurciello Blog
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://www.favicon-generator.org/
-->



<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">
<link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">

<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png">
<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png">
<link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png">

<link rel="icon" type="image/png" sizes="192x192"  href="/assets/img/favicons/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

<link rel="manifest" href="/assets/img/favicons/manifest.json">
<meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'>
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
  

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous">

  <!-- Font Awesome -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
    integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ="
    crossorigin="anonymous">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">




  <!-- JavaScripts -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>

  <!--
  JS selector for site.
-->


  





<script defer src="/assets/js/dist/post.min.js"></script>






</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">

  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/logo.jpeg" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Eugenio Culurciello Blog</a>
    </div>

    <div class="site-subtitle font-italic">Eugenio Culurciello Blog</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://github.com/culurciello" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github-alt"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/culurciello" aria-label="twitter"
        class="order-4"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['culurciello','gmail.com'].join('@')" aria-label="email"
        class="order-5"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        class="order-6"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    
      
        <span class="icon-border order-2"></span>
      

      <span id="mode-toggle-wrapper" class="order-1">
        <!--
  Switch the mode between dark and light.
-->

<i class="mode-toggle fas fa-adjust"></i>

<script type="text/javascript">

  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      var self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.updateMermaid();
      });

    } /* constructor() */


    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }

        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }

      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.updateMermaid();

    } /* flipMode() */

  } /* ModeToggle */

  let toggle = new ModeToggle();

  $(".mode-toggle").click(function() {

    toggle.flipMode();

  });

</script>

      </span>
    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center topbar-down">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          

        

      

        
        <span>
          
          
          <a href="/2018">
            2018
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/01">
            01
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/03">
            03
          </a>
        </span>

        

      

        
          <span>Artificial Intelligence, AI in 2018 and beyond</span>

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->


<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->


<!-- Add attribute 'hide-bullet' to the checkbox list -->




  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->
  
  

  

  



<!-- return -->
<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>Artificial Intelligence, AI in 2018 and beyond</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Eugenio Culurciello
          </span>
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Wed, Jan  3, 2018, 12:00 AM -0500"
  

  
  prep="on" >

  
  

  
    Jan  3, 2018
  

  <i class="unloaded">2018-01-03T00:00:00-05:00</i>

</span>

        </div>

        <div>
          <!-- lastmod -->
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3130 words">17 min</span>


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <h1 id="artificial-intelligence-ai-in-2018-and-beyond">Artificial Intelligence, AI in 2018 and beyond</h1>

<blockquote>
  <p>Or how machine learning is evolving into AI</p>
</blockquote>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-01-03-AI-in-2018/1*zCw67waCee0FsVcWcyRhxw.png" alt="" /></p>

<p>These are my opinions on where deep neural network and machine learning is headed in the larger field of artificial intelligence, and how we can get more and more sophisticated machines that can help us in our daily routines.</p>

<p>Please note that these are not  predictions  of  forecasts, but more  a detailed analysis of the trajectory of the fields, the trends and the technical needs we have to achieve useful artificial intelligence.</p>

<p>Not all machine learning is targeting artificial intelligences, and there are low-hanging fruits, which we will examine here also.</p>

<h1 id="goals">Goals</h1>

<p>The goal of the field is to achieve human and super-human abilities in machines that can help us in every-day lives.  Autonomous vehicles, smart homes, artificial assistants,  security cameras are a first target. Home cooking and cleaning robots are a second target, together with surveillance drones and robots.  Another one is assistants on mobile devices or always-on assistants. Another is full-time companion assistants that can hear and see what we experience in our life.  One ultimate goal is a fully autonomous synthetic entity that can behave at or beyond human level performance in everyday tasks.</p>

<p>See more about these goals  <a href="https://medium.com/@culurciello/will-artificial-intelligence-take-over-the-universe-33e214ef29a9">here</a>, and  <a href="https://towardsdatascience.com/uncertainty-in-deep-learning-and-neural-network-i-do-not-feel-that-way-f11ceca4341a">here</a>, and  <a href="https://aboveintelligent.com/artificial-intelligence-past-present-future-cb954fc7da3b">here</a>.</p>

<h1 id="software">Software</h1>

<p>Software is defined here as neural networks architectures trained with an optimization algorithm to solve a specific task.</p>

<p>Today neural networks are the  de-facto tool for learning to solve tasks that involve  learning  supervised to categorize from a large dataset.</p>

<p>But this is not artificial intelligence, which requires acting in the real world often learning without supervision and from experiences never seen before, often combining previous knowledge in disparate circumstances to solve the current challenge.</p>

<blockquote>
  <p>How do we get from the current neural networks to AI?</p>
</blockquote>

<p><strong>Neural network architectures</strong>  — when the field  boomed, a few years back,  we often said it had the advantage to learn the parameters of an algorithms automatically from data, and as such was superior to hand-crafted features.But we conveniently forgot to mention one little detail…  the neural network architecture that is at the foundation of training to solve a specific task is not learned from data! In fact it is still designed by hand. Hand-crafted from experience, and it is currently one of the major limitations of the field.  There is research in this  direction: <a href="http://papers.nips.cc/paper/6460-learning-to-learn-by-gradient-descent-by-gradient-descent">here</a> and <a href="https://arxiv.org/abs/1707.07012">here</a> (for example),  but much more is needed.  Neural network architectures are the fundamental core of learning algorithms.  Even if our learning algorithms are capable of mastering a new task, if the neural network is not correct, they will not be able to.  The problem on learning neural network architecture from data is that it currently takestoo long to experiment with multiple architectures on a large dataset.  One has to try training multiple architectures from scratch and see which one works best. Well this is exactly the time-consuming trial-and-error procedure we are using today! We ought to overcome this limitation and put more brain-power on this very important issue.</p>

<p><strong>Unsupervised learning</strong>  —we cannot always be there for our neural networks, guiding them at every stop of their lives and every experience.  We cannot afford to correct them at every instance, and provide feedback on their performance. We have our lives to live! But that is exactly what we do today with supervised neural networks: we offer help at every instance to make them perform correctly. Instead humans learn from just a handful of examples, and can self-correct and learn more complex data in a continuous fashion. We have talked about unsupervised learning extensively  <a href="https://medium.com/intuitionmachine/navigating-the-unsupervised-learning-landscape-951bd5842df9">here</a>.</p>

<p><strong>Predictive neural networks —</strong>  A major limitation of current neural networks is that they do not possess one of the most important features of human brains: their predictive power.  One major theory about how the human brain work is by constantly making predictions:  <a href="https://en.wikipedia.org/wiki/Predictive_coding">predictive coding</a>.  If you think about it, we experience it every day. As you lift an object that you thought was light but turned out heavy. It surprises you, because as you approached to pick it up, you have predicted how it was going to affect you and your body, or your environment in overall.</p>

<p>Prediction allows not only to understand the world, but also to know when we do not, and when we should learn.  In fact  we save information about things we do not know and surprise us, so next time they will not! And  cognitiveabilities are clearly linked to our attention  mechanism  in the brain: our  innateability to  forego  of 99.9% of our sensory inputs, only to focus on the very important data for our survival — where is the threat and where do we run to to avoid it.  Or, in the modern world, where is my cell-phone as we walk out the door in a rush.</p>

<p>Building predictive neural networks is at the core of interacting with the real world, and acting in a complex environment.  As such this is the core network for any work in reinforcement learning.  See more below.</p>

<p>We have talked extensively about the topic of predictive neural networks, and were one of the pioneering groups to study them and create them. For more details on predictive neural networks, see <a href="https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108">here</a>, and <a href="https://medium.com/@culurciello/predictive-neural-networks-for-reinforcement-learning-490738725839">here</a>, and <a href="https://towardsdatascience.com/adversarial-predictive-networks-3aa7026d53d2">here</a>.</p>

<p><strong>Limitations</strong> <strong>of</strong> <strong>current</strong> <strong>neural networks</strong>  — We have talked about before on the limitation of neural networks as they are today.  Cannot predict, reason on content, and have temporal  instabilities — we need a <em>new kind of neural networks</em> that you can about read <a href="https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108">here</a>.</p>

<p><em>Neural Network Capsules</em> are one approach to solve the limitation of current neural networks. We reviewed them <a href="https://medium.com/mlreview/deep-neural-network-capsules-137be2877d44">here</a>. We argue here that Capsules have to be extended with a few additional features:</p>

<ul>
  <li><strong>operation on video frames</strong>: this is easy, as all we need to do is to make capsules routing look at multiple data-points in the recent past. This is equivalent to an associative memory on the most recent important data points. Notice these  <em>are not</em> the most recent representations of recent frames, but rather they are the top most recent  <em>different</em>  representations. Different representations with different content can be obtained for example by saving only representations that differ more than a pre-defined value. This important detail allows to save relevant information on the most recent history only, and not a useless series of correlated data-points.</li>
  <li><strong>predictive neural network abilities</strong>: this is already part of the dynamic routing, which forces layers to predict the next layer representations. This is a very powerful self-learning technique that in our opinion beats all other kinds of unsupervised representation learning we have developed so far as a community. Capsules need now to be able to predict long-term spatiotemporal relationships, and this is not currently implemented.</li>
</ul>

<p><strong>Continuous learning</strong> — this is important because neural networks need to continue to learn new data-points continuously for their life.  Current neural networks are not able to learn new data without being re-trained from scratch at every instance.  Neural networks need to be able to self-assess the need of new training and the fact that they do know something.  This is also needed to perform in real-life and for reinforcement learning tasks, where we want to teach machines to do new tasks without forgetting older ones.</p>

<p>For more detail, see <a href="https://medium.com/@vlomonaco/why-continuous-learning-is-the-key-towards-machine-intelligence-1851cb57c308">this excellent blog post</a> by <a href="http://vincenzolomonaco.com/">Vincenzo Lomonaco</a>.</p>

<p><strong>Transfer learning</strong> — or how do we have these algorithms learn on their own by watching videos, just like we do when we want to learn how to cook something new?  That is an ability that requires all the components we listed above, and also is important for reinforcement learning. Now you can really train your machine to do what you want by just giving an example, the same way we humans do every!</p>

<p><strong>Reinforcement learning —</strong> this is the holy grail of deep neural network research: teach machines how to learn to act in an environment, the real world! This requires self-learning, continuous learning, predictive power, and a lot more we do not know. There is much work in the field of reinforcement learning, but to the author it is really only scratching the surface of the problem, still millions of miles away from it.  We already talked about this <a href="https://towardsdatascience.com/learning-and-performing-in-the-real-world-7e53eb46d9c3">here</a>.</p>

<p>Reinforcement learning is often referred as the “cherry on the cake”, meaning that it is just minor training on top of a plastic synthetic brain. But how can we get a “generic” brain that then solve all problems easily? It is a chicken-in-the-egg problem!  Today to solve reinforcement learning problems, one by one, we use standard neural networks:</p>

<ul>
  <li>a deep neural network that takes large data inputs, like video or audio and compress it into representations</li>
  <li>a sequence-learning neural network, such as RNN, to learn tasks</li>
</ul>

<p>Both these components are obvious solutions to the problem, and currently are clearly wrong, but that is what everyone uses because they are some of the  <a href="https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d">available building blocks</a>. As such results are unimpressive: yes we can learn to play video-games from scratch, and  master fully-observable games like chess and go, but I do not need to tell you that is nothing compared to solving problems in a complex world. Imagine an AI that can play  <a href="https://en.wikipedia.org/wiki/Horizon_Zero_Dawn">Horizon Zero Dawn</a>better than humans… I want to see that!</p>

<p>But this is what we want. Machine that can operate like us.</p>

<p>Our  proposal  for  reinforcement  learning work is detailed  <a href="https://towardsdatascience.com/learning-and-performing-in-the-real-world-7e53eb46d9c3">here</a>.  It uses a predictive neural network that can operate continuously and an associativememory to store recent experiences.</p>

<p><strong>No more</strong> <strong>recurrent</strong> <strong>neural networks</strong>  — recurrent neural network (RNN) have their days counted. RNN are particularly bad at parallelizing for training and also slow even on special custom machines, due to their very high memory bandwidth usage — as such they are memory-bandwidth-bound, rather than computation-bound, see <a href="https://medium.com/@culurciello/computation-and-memory-bandwidth-in-deep-neural-networks-16cbac63ebd5">here for more details</a>. <a href="https://towardsdatascience.com/memory-attention-sequences-37456d271992">Attention based neural network</a> are more efficient and faster to train and deploy, and they suffer much less from scalability in training and deployment. Attention in neural network has the potential to really revolutionize a lot of architectures, yet it has not been as recognized as it should. The combination of associative memories and attention is at the heart of the next wave of neural network advancements.</p>

<p>Attention has already showed to be able to learn sequences as well as RNNs and at up to <a href="https://towardsdatascience.com/memory-attention-sequences-37456d271992">100x less computation</a>! Who can ignore that?</p>

<p>We recognize that attention based neural network are going to slowly  supplantspeech  recognition  based on RNN, and also find their ways in  reinforcementlearning architecture and AI in general.</p>

<p><strong>Localization</strong> <strong>of information in</strong> <strong>categorization</strong> <strong>neural networks</strong> — We have talked about how we can localize and  detect  key-points in images and video extensively  <a href="https://towardsdatascience.com/segmenting-localizing-and-counting-object-instances-in-an-image-878805fef7fc">here</a>. This is practically a solved problem, that will be  embeddedin  <a href="https://towardsdatascience.com/a-new-kind-of-deep-neural-networks-749bcde19108">future neural network architectures</a>.</p>

<h1 id="hardware">Hardware</h1>

<p>Hardware for deep learning is at the core of progress. Let us now forget that the  rapid  expansion of deep learning in 2008–2012 and in the recent years is mainly due to hardware:</p>

<ul>
  <li>cheap image  sensors  in every phone allowed to collect  huge datasets — yes helped by social media, but only to a second  extent</li>
  <li>GPUs allowed to accelerate the training of deep neural networks</li>
</ul>

<p>And  <a href="https://towardsdatascience.com/hardware-for-deep-learning-8d9b03df41a">we have talked about hardware extensively before</a>. But we need to give you a recent update! Last 1–2 years saw a boom in the are of machine learning hardware, and in particular on the one targeting deep neural networks. We have significant experience here, and we are  <a href="http://fwdnxt.com/">FWDNXT</a>, the makers of  <a href="https://arxiv.org/abs/1708.02579">SnowFlake</a>: deep neural network accelerator.</p>

<p>There are several companies working in this space: NVIDIA (obviously), Intel, Nervana, Movidius, Bitmain, Cambricon, Cerebras, DeePhi, Google, Graphcore, Groq, Huawei, ARM, Wave Computing. All are developing custom high-performance micro-chips that will be able to train and run deep neural networks.</p>

<p>The key is to provide the lowest power and the highest measured performance while computing recent useful neural networks operations, not raw theoretical operations per seconds — as many claim to do.</p>

<p>But few people in the field understand how hardware can really change machine learning, neural networks and AI in general. And few understand what is important in micro-chips and how to develop them.</p>

<p>Here is our list:</p>

<ul>
  <li><strong>training or inference?</strong> — many companies are creating micro-chips that can provide training of neural networks. This is to gain a portion of the market of NVIDIA, which is the de-facto training hardware to date. But training is a small part of the story and the applications of deep neural networks. For every training step there are a million deployments in actual applications. For example one of the object detection neural network you can now use on the cloud today: it was trained once, and yes on a lot of images, but once trained it can be use by millions of computers on billions of data. What we are trying to say here: training hardware matter as little as the number of times you trained compared to the number of times you use. And making a chipset for training requires extra hardware and extra tricks. This translates into higher power for the same performance, and thus not the best possible for current deployments. Training hardware is important, and a easy modification of inference hardware, but it is not as important as many think.</li>
  <li><strong>Applications</strong>  — hardware that can provide training faster and at lower power is really important in the field, because it will allow to create and test new models and applications faster. But the real significant step forward will be in hardware for applications, mostly in inference. There are many applications today that are not possible or practical because hardware, and not software, is missing or inefficient.  For example our phones can be speech-based assistants, and are currently sub-optimal because they cannot operate always-on.  Even our home assistants are tied to the power supplies, and cannot follow us around the house unless we sprinkle multiple micraphones or devices around. But maybe the largest application of all is removing the phone screen from our lives, and embedding it into our visual system. Without super-efficient hardware all this and many more applications (small robots) will not be possible.</li>
  <li><strong>winners and losers</strong> — in hardware, the winner will be the ones that can operate at the lowest possible power per unit performance, and move into the market quickly. Imagine replacing SoC in cell-phones. Happens every year. Now imagine embedding neural network accelerators into memories. This may conquer much of the market faster and with significant penetration. That is what we call a winner.</li>
</ul>

<p>About neuromorphic neural networks hardware, please see  <a href="https://towardsdatascience.com/neuromorphic-and-spiking-or-not-894a836dc3b3">here</a>.</p>

<h1 id="applications">Applications</h1>

<p>We talked briefly about applications in the Goals section above, but we really need to go into details here. How is AI and neural network going to get into our daily life?</p>

<p>Here is our list:</p>

<ul>
  <li><strong>categorizing images and video</strong><strong>s</strong>  — already here in many cloud services. The next steps are doing the same in smart camera feeds — also here today from many providers. Neural nets hardware will allow to remove the cloud and process more and more data locally: a winner for privacy and saving Internet bandwidth.</li>
  <li><strong>speech-based assistants</strong>  — they are becoming a part of our lives, as they play music and control basic devices in our “smart” homes. But dialogue is such a basic human activity, we often give it for  granted.  Small devices you can talk to are a <em>revolution</em> that is happening right now.  Speech-based assistants are getting better and better at serving us. But they are still tied to the power  grid.  The real assistant we want is moving with us.  How about our cell-phone? Well again hardware wins here, because it will make that possible. Alexa and Cortana and Siri will be always on and always with you.  Your phone will be your smart home — very soon.  That is again another  victory  of the smart phone.  But we also want it in our car and as we move around town.  We need local processing of voice, and less and less cloud. More  privacy  and less bandwidth costs. Again hardware will give us all that in 1–2 years.</li>
  <li><strong>the real artificial assistants</strong> — voice is great, but what we really want is something that can also see what we see. Analyze our environment as we move around. See an example  <a href="https://medium.com/@culurciello/shopper-artificial-intelligence-to-help-you-shop-d252bd995f78">here</a>  and  ultimately  <a href="http://hyper-reality.co/">here</a>. This is the real AI assistant we can fall in love with. And neural network hardware will again grant your wish, as analyzing video feed is very  computationallyexpensive, and currently at the theoretical limits on current silicon hardware. In other words  a lot harder to do than speech-based assistants.But it is not impossible, and many smart startups like  <a href="https://www.aipoly.com/">AiPoly</a>  already have all the software for it, but lack powerful hardware for running it on phones. Notice also that replacing the phone screen with a  <strong>wearable glasses-like device</strong>  will really make our assistant part of us!</li>
</ul>

<blockquote>
  <p>What we want is <a href="https://en.wikipedia.org/wiki/Her_(film)">Her</a> from the movie <a href="https://en.wikipedia.org/wiki/Her_(film)">Her</a>!</p>
</blockquote>

<ul>
  <li><strong>the cooking robot</strong> — the next biggest appliances will be a cooking and <strong>cleaning robot</strong>. Here we may soon have the hardware, but we are clearly lacking the software. We need transfer learning, continuous learning and reinforcement learning. All working like a charm. Because you see: every recipe is different, every cooking ingredient looks different. We cannot hard-code all these options. We really need a <strong>synthetic entity</strong> that can learn and generalize well to do this. We are far from it, but not as far. Just a handful of years away at the current pace of progress. I sure will work on this, as I have done in the last few years~</li>
</ul>

<blockquote>
  <p>This blog post will evolve, like our algorithms and our machines. Please check it again soon.</p>
</blockquote>

<p><strong>PS.:</strong>  More about performance, roofline-plots and bandwidth  <a href="http://www.telesens.co/2018/07/26/understanding-roofline-charts/">here</a>.</p>

<h1 id="about-the-author">About the author</h1>

<p>I have  almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://e-lab.github.io/html/contact-eugenio-culurciello.html">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>, and more…</p>

<h1 id="more-references">More references</h1>

<p>For interesting additional reading, please see:</p>

<p><a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993"></a></p>

<h2 id="building-machines-that-learn-and-think-like-people--behavioral-and-brain-sciences--cambridge-core">Building machines that learn and think like people | Behavioral and Brain Sciences | Cambridge Core</h2>

<h3 id="building-machines-that-learn-and-think-like-people---volume-40---brenden-m-lake-tomer-d-ullman-joshua-b-tenenbaum">Building machines that learn and think like people - Volume 40 - Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum…</h3>

<p>www.cambridge.org</p>

<p><a href="https://arxiv.org/abs/1612.03969"></a></p>

<h2 id="161203969-tracking-the-world-state-with-recurrent-entity-networks">[1612.03969] Tracking the World State with Recurrent Entity Networks</h2>

<h3 id="abstract-we-introduce-a-new-model-the-recurrent-entity-network-entnet-it-is-equipped-with-a-dynamic-long-term">Abstract: We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term…</h3>

<p>arxiv.org</p>

<p><a href="http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/"></a></p>

<h2 id="ai-and-deep-learning-in-2017---a-year-in-review">AI and Deep Learning in 2017 - A Year in Review</h2>

<h3 id="the-year-is-coming-to-an-end-i-did-not-write-nearly-as-much-as-i-had-planned-to-but-im-hoping-to-change-that-next">The year is coming to an end. I did not write nearly as much as I had planned to. But I’m hoping to change that next…</h3>

<p>www.wildml.com</p>

<p><a href="http://rodneybrooks.com/my-dated-predictions/"></a></p>

<h2 id="my-dated-predictions">My Dated Predictions</h2>

<h3 id="with-all-new-technologies-there-are-predictions-of-how-good-it-will-be-for-humankind-or-how-bad-it-will-be-a-common">With all new technologies there are predictions of how good it will be for humankind, or how bad it will be. A common…</h3>

<p>rodneybrooks.com</p>


      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
        

        <!-- tags -->
        

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          
          <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
          </div>
          

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=Artificial Intelligence, AI in 2018 and beyond - Eugenio Culurciello Blog&url=http://localhost:4000/2018/01/03/AI-in-2018.html" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=Artificial Intelligence, AI in 2018 and beyond - Eugenio Culurciello Blog&u=http://localhost:4000/2018/01/03/AI-in-2018.html" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://telegram.me/share?text=Artificial Intelligence, AI in 2018 and beyond - Eugenio Culurciello Blog&url=http://localhost:4000/2018/01/03/AI-in-2018.html" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->

<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  














  

  

















  
  </div> <!-- .access -->

  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->




  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
          





  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/2023/05/24/Should-we-worry-about-AI.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    May 24
  

  <i class="unloaded">2023-05-24T00:00:00-04:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Should we worry about AI (machine-learning)?</h3>
            <div class="text-muted small">
              <p>
                





                Should we worry about AI (machine-learning)?

I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind…



Artificial intell...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2023/03/30/A-picture-is-worth-a-1000-words.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Mar 30
  

  <i class="unloaded">2023-03-30T00:00:00-04:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>A picture is worth a 1000 words</h3>
            <div class="text-muted small">
              <p>
                





                A picture is worth a 1000 words

Evolution of large language models into artificial brains through multiple sensory modalities



As surprising as it is our very human predictive abilities hide a p...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2023/01/25/Transformers-GPT.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Jan 25
  

  <i class="unloaded">2023-01-25T00:00:00-05:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?</h3>
            <div class="text-muted small">
              <p>
                





                How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?



language models and transformers in  https://huggingface.co/spaces/stabilityai/stable-diffusio...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->



    <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/2017/12/31/2017-an-year-in-review.html" class="btn btn-outline-primary"
    prompt="Older">
    <p>2017: an year in review</p>
  </a>
  

  
  <a href="/2018/03/22/you-are-product.html" class="btn btn-outline-primary"
    prompt="Newer">
    <p>You are the product</p>
  </a>
  

</div>


    

    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



  <!--
  image lazy load: https://github.com/ApoorvSaxena/lozad.js
-->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script>

<script type="text/javascript">
  const imgs = document.querySelectorAll('.post-content img');
  const observer = lozad(imgs);
  observer.observe();
</script>




        <!-- The Footer -->

<footer>
  <div class="container pl-lg-4 pr-lg-4">
    <div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3">
      <div class="footer-left">
        <p class="mb-0">
          © 2023
          <a href="https://twitter.com/culurciello">Eugenio Culurciello</a>.
          <!--  -->
        </p>
      </div>

      <div class="footer-right">
        <p class="mb-0">

          <!--Using the <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> theme <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>. -->
        </p>
      </div>
    </div>
  </div>
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="http://localhost:4000{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


  </body>

</html>

