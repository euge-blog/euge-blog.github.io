<!DOCTYPE html>



<html lang="en-US" 
  
>

  <!--
  The Head
-->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  

    

  

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Biological and artificial intelligence" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Biological and artificial intelligence" />
<meta property="og:description" content="Biological and artificial intelligence" />
<link rel="canonical" href="http://localhost:4000/2018/12/09/bio-artificial-AI.html" />
<meta property="og:url" content="http://localhost:4000/2018/12/09/bio-artificial-AI.html" />
<meta property="og:site_name" content="Eugenio Culurciello Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-12-09T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Biological and artificial intelligence" />
<meta name="twitter:site" content="@culurciello" />
<meta name="google-site-verification" content="google_meta_tag_verification" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2018-12-09T00:00:00-05:00","datePublished":"2018-12-09T00:00:00-05:00","description":"Biological and artificial intelligence","headline":"Biological and artificial intelligence","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2018/12/09/bio-artificial-AI.html"},"url":"http://localhost:4000/2018/12/09/bio-artificial-AI.html"}</script>
<!-- End Jekyll SEO tag -->


  <title>Biological and artificial intelligence | Eugenio Culurciello Blog
  </title>

  <!--
  The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps
  Generated by: https://www.favicon-generator.org/
-->



<link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">
<link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon">

<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png">
<link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png">
<link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png">

<link rel="icon" type="image/png" sizes="192x192"  href="/assets/img/favicons/android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png">

<link rel="manifest" href="/assets/img/favicons/manifest.json">
<meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'>
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">


  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
  <link rel="dns-prefetch" href="https://fonts.gstatic.com">

  <!-- GA -->
  

  <!-- jsDelivr CDN -->
  <link rel="preconnect" href="cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="cdn.jsdelivr.net">

  <!-- Bootstrap -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"
    integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous">

  <!-- Font Awesome -->
  <link rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"
    integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ="
    crossorigin="anonymous">

  <!--
  CSS selector for site.
-->

<link rel="stylesheet" href="/assets/css/style.css">




  <!-- JavaScripts -->

  <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>

  <script defer
    src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script>

  <!--
  JS selector for site.
-->


  





<script defer src="/assets/js/dist/post.min.js"></script>






</head>


  <body data-spy="scroll" data-target="#toc">

    <!--
  The Side Bar
-->

<div id="sidebar" class="d-flex flex-column align-items-end">

  <div class="profile-wrapper text-center">
    <div id="avatar">
      <a href="/" alt="avatar" class="mx-auto">
        
        <img src="/assets/logo.jpeg" alt="avatar" onerror="this.style.display='none'">
      </a>
    </div>

    <div class="site-title mt-3">
      <a href="/">Eugenio Culurciello Blog</a>
    </div>

    <div class="site-subtitle font-italic">Eugenio Culurciello Blog</div>

  </div><!-- .profile-wrapper -->

  <ul class="w-100">
    <!-- home -->
    <li class="nav-item">
      <a href="/" class="nav-link">
        <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i>
        <span>HOME</span>
      </a>
    </li>
    <!-- the real tabs -->
    

  </ul> <!-- ul.nav.flex-column -->

  <div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center">

    
      

      
      <a href="https://github.com/culurciello" aria-label="github"
        class="order-3"
        target="_blank" rel="noopener">
        <i class="fab fa-github-alt"></i>
      </a>
      

    
      

      
      <a href="https://twitter.com/culurciello" aria-label="twitter"
        class="order-4"
        target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
      

    
      

      
      <a href="
          javascript:location.href = 'mailto:' + ['culurciello','gmail.com'].join('@')" aria-label="email"
        class="order-5"
        >
        <i class="fas fa-envelope"></i>
      </a>
      

    
      

      
      <a href="/feed.xml" aria-label="rss"
        class="order-6"
        >
        <i class="fas fa-rss"></i>
      </a>
      

    

    
      
        <span class="icon-border order-2"></span>
      

      <span id="mode-toggle-wrapper" class="order-1">
        <!--
  Switch the mode between dark and light.
-->

<i class="mode-toggle fas fa-adjust"></i>

<script type="text/javascript">

  class ModeToggle {
    static get MODE_KEY() { return "mode"; }
    static get DARK_MODE() { return "dark"; }
    static get LIGHT_MODE() { return "light"; }

    constructor() {
      if (this.hasMode) {
        if (this.isDarkMode) {
          if (!this.isSysDarkPrefer) {
            this.setDark();
          }
        } else {
          if (this.isSysDarkPrefer) {
            this.setLight();
          }
        }
      }

      var self = this;

      /* always follow the system prefers */
      this.sysDarkPrefers.addListener(function() {
        if (self.hasMode) {
          if (self.isDarkMode) {
            if (!self.isSysDarkPrefer) {
              self.setDark();
            }

          } else {
            if (self.isSysDarkPrefer) {
              self.setLight();
            }
          }

          self.clearMode();
        }

        self.updateMermaid();
      });

    } /* constructor() */


    setDark() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE);
    }

    setLight() {
      $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
      sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE);
    }

    clearMode() {
      $('html').removeAttr(ModeToggle.MODE_KEY);
      sessionStorage.removeItem(ModeToggle.MODE_KEY);
    }

    get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); }

    get isSysDarkPrefer() { return this.sysDarkPrefers.matches; }

    get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; }

    get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; }

    get hasMode() { return this.mode != null; }

    get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); }

    /* get the current mode on screen */
    get modeStatus() {
      if (this.isDarkMode
        || (!this.hasMode && this.isSysDarkPrefer) ) {
        return ModeToggle.DARK_MODE;
      } else {
        return ModeToggle.LIGHT_MODE;
      }
    }

    updateMermaid() {
      if (typeof mermaid !== "undefined") {
        let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default");
        let config = { theme: expectedTheme };

        /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */
        $(".mermaid").each(function() {
          let svgCode = $(this).prev().children().html();
          $(this).removeAttr("data-processed");
          $(this).html(svgCode);
        });

        mermaid.initialize(config);
        mermaid.init(undefined, ".mermaid");
      }
    }

    flipMode() {
      if (this.hasMode) {
        if (this.isSysDarkPrefer) {
          if (this.isLightMode) {
            this.clearMode();
          } else {
            this.setLight();
          }

        } else {
          if (this.isDarkMode) {
            this.clearMode();
          } else {
            this.setDark();
          }
        }

      } else {
        if (this.isSysDarkPrefer) {
          this.setLight();
        } else {
          this.setDark();
        }
      }

      this.updateMermaid();

    } /* flipMode() */

  } /* ModeToggle */

  let toggle = new ModeToggle();

  $(".mode-toggle").click(function() {

    toggle.flipMode();

  });

</script>

      </span>
    

  </div> <!-- .sidebar-bottom -->

</div><!-- #sidebar -->


    <!--
  The Top Bar
-->

<div id="topbar-wrapper" class="row justify-content-center topbar-down">
  <div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between">
    <span id="breadcrumb">

    

    

      

        
          

        

      

        
        <span>
          
          
          <a href="/2018">
            2018
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/12">
            12
          </a>
        </span>

        

      

        
        <span>
          
          
          <a href="/09">
            09
          </a>
        </span>

        

      

        
          <span>Biological and artificial intelligence</span>

        

      

    

    </span><!-- endof #breadcrumb -->

    <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i>

    <div id="topbar-title">
      Post
    </div>

    <i id="search-trigger" class="fas fa-search fa-fw"></i>
    <span id="search-wrapper" class="align-items-center">
      <i class="fas fa-search fa-fw"></i>
      <input class="form-control" id="search-input" type="search"
        aria-label="search" placeholder="Search...">
      <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i>
    </span>
    <span id="search-cancel" >Cancel</span>
  </div>

</div>


    <div id="main-wrapper">
      <div id="main">

        <!--
  Refactor the HTML structure.
-->



<!--
  In order to allow a wide table to scroll horizontally,
  we suround the markdown table with `<div class="table-wrapper">` and `</div>`
-->


<!--
  Fixed kramdown code highlight rendering:
  https://github.com/penibelst/jekyll-compress-html/issues/101
  https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901
-->


<!-- Add attribute 'hide-bullet' to the checkbox list -->




  

  

  <!-- lazy-load images <https://github.com/ApoorvSaxena/lozad.js#usage> -->
  
  

  

  



<!-- return -->
<div class="row">

  <div id="post-wrapper" class="col-12 col-lg-11 col-xl-8">

    <div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

      <h1 data-toc-skip>Biological and artificial intelligence</h1>

      <div class="post-meta text-muted d-flex flex-column">
        <!-- Published date and author -->
        <div>
          <span class="semi-bold">
            Eugenio Culurciello
          </span>
          <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago "
  
    data-toggle="tooltip"
    data-placement="bottom"
    title="Sun, Dec  9, 2018, 12:00 AM -0500"
  

  
  prep="on" >

  
  

  
    Dec  9, 2018
  

  <i class="unloaded">2018-12-09T00:00:00-05:00</i>

</span>

        </div>

        <div>
          <!-- lastmod -->
          

          <!-- read time -->
          <!--
  Calculate the post's reading time, and display the word count in tooltip
 -->


<!-- words per minute  -->







<!-- return element -->
<span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2910 words">16 min</span>


          <!-- page views -->
          

        </div>

      </div> <!-- .post-meta -->

      <div class="post-content">

        

        <h1 id="biological-and-artificial-intelligence">Biological and artificial intelligence</h1>

<p>commentary on this  <a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">blog post</a>  by Surya Ganguli</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*gASE2UL4wROHIZj083sghw.jpeg" alt="" /></p>

<p>I wanted to provide some commentary on this  <a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">blog post</a>  by Surya Ganguli, which appeared on December 5th 2018 (here: the “article”).</p>

<p>My background is neuromorphic microchip and system design as well as in deep-learning hardware and software for almost 20 years.</p>

<p>The post by Surya Ganguli is of great value and I recommend all interested in the topic to read it. I would like to provide some thoughts based on my experience and technology design path, so we can better prepare for a future where artificial intelligence will have learned all there is to learn from biological brains, and also be able to perform at or above biological intelligence levels.</p>

<h1 id="biologically-plausible-credit-assignment">Biologically plausible credit assignment</h1>

<p>This section mentions the challenge of using more effective local learning rules. It says: “the brain solves it using a local learning rule”. Today, most deep learning models are trained with back-propagation that spans all layers in the networks. They can thus propagate effectively errors in the cost function to all the weights in the network in one single pass. In biological neural networks instead, from what is know to-date, learning occurs locally, confined between two layers and often between a small set of neurons and synapses.</p>

<p>Inventing and testing local learning techniques has been one of the algorithmic research work that we and many others are trying to tackle. Our group has worked on a techniques called Clustering Learning (see  <a href="https://arxiv.org/abs/1301.2820">here</a>  and  <a href="https://arxiv.org/abs/1306.0152">here</a>) and culminated in Aysegul Dundar seminal  <a href="https://arxiv.org/abs/1511.06241">work</a>. These techniques all try to learn locally, on a layer by layer basis. There are many more, like auto-encoders, sparse-coding, belief networks, Boltzmann machines, and many more that paved the way.</p>

<p>Why are we mixing “unsupervised” and “local learning rules” here? Well because one way to train unsupervised is to reconstruct or work on layer-by-layer errors, and this is one form of unsupervised learning.</p>

<p>What did we learn by doing all this work and research? We learned that local learning function work well to pre-train neural network weights and that they can be very useful in cases where training data is scarce, and thus we need way to pre-train in an unsupervised manner. Hardly the case today, where deep learning dataset are large and many. Today we use these labeled dataset to train neural networks, and  if you have a large dataset at your disposal back-propagation over many layers always beats any unsupervised and layer-by-layer learning techniques.  And this is well known to all that work in deep learning today.</p>

<p>The question is: what do you do if you do not have such rich and large datasets? Well, then you have to use unsupervised learning techniques to bootstrap learning. We read, analyzed and reported on unsupervised learning techniques  <a href="https://medium.com/intuitionmachine/navigating-the-unsupervised-learning-landscape-951bd5842df9">here</a>. And we found something that may surprise many of you: the best unsupervised techniques still train networks end-to-end with back-propagation. Like the popular GAN these days. And this is interesting: back-propagating signals from many layers makes the signal more important that if you just used it in a single layer.  That is because error signals can affect much more weights if back-propagated across many layers. Biology may or may not do this because of signal-to-noise issues, and nevertheless is restricted to using error signal only locally. In digital circuits we can do a bit better today because we can afford larger number of bits and higher noise thresholds. We will come back to this later in section “biology vs silicon hardware” to reflect on power efficiency and difference in materials also.</p>

<p>Where does this leave us with unsupervised techniques? These are still one only option if our dataset is tiny, and we have no way to use transfer-learning (learning in another large dataset that is close to our target dataset, and then transfer trained weights).</p>

<p>One final thought is that one can also trade off performance of unsupervised learning system for size, in some cases. This means that  if you can afford making your unsupervised network much larger, you will surely come closer to its performance in a supervised setting with back-prop.  You will still miss multi-layer adaptation (network weights optimization) that comes from propagating over multiple layers. But this means having more neurons and thus using more power.</p>

<p>And local-learning is also very inefficient for a more dramatic reason: it forgets that it learned some solutions in another areas of the brain and then has to learn it again in many localized area. Remember for example in the visual cortex orientation filters are learned over and over again in all of the spatial retinal maps like in V1 and probably beyond. This is largely inefficient.</p>

<p>The question is: would you rather have a small number of efficient neurons of many that are not? The brain evolved to the latter. I know, I know: you are thinking why is that the brain uses few watts of power and computers / AI chips take a power grid down? Ok, we will talk more about this later here.</p>

<p>Bottom line:</p>

<blockquote>
  <p>if you can back-propagate on many layers, do so!</p>
</blockquote>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*EgmZS56pBobadhOpIGDOTQ.jpeg" alt="" /></p>

<h1 id="synaptic-complexity">Synaptic complexity</h1>

<p><a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">The article</a>  mentions that “ current AI systems are leaving major performance gains on the table by ignoring the dynamical complexity of biological synapses”. Biology uses synapses to communicate electrical pulses or spikes from one cell to the next. It does so by a complex chemical machine that releases neurotransmitters that in turn open or close pathways of ions into cells. In artificial neural networks, instead, we use wires.</p>

<p>My response is that of course deep learning uses complex synapses! The most basic form of synapse is a non-linear function that sits at the output of neurons. This function can model both the neuron non-linear threshold and also the synapse dynamics. If a separate synapse model is desired,  a simple matrix can model it (or a 1x1 convolution).</p>

<p>There are also many different ways in which we take into account the effect of a synaptic model, both in inference and also in training. I will list here only a few:</p>

<ul>
  <li>layer architecture</li>
  <li>different non-linearities</li>
  <li>different bit-width</li>
  <li>scaling and normalization</li>
  <li>drop-out and other regularization techniques</li>
</ul>

<p>You may ask yourself: why did biology use complex transfer function in each synapse? Because it has to use different kind of cells and ions and materials depending on the are of the brain. Because it evolved over millions of years without a common blue-print. Because it had to re-invent the wheel every so few million years. Because one ion dynamics are different from another ion. Because … the list goes on.</p>

<p>But the main major point here is that synapses also can help a neuron transfer function by adding variability and introducing more dynamics. As such you see we agree with the author. Only you can thus create a neuron that uses learnable dynamics based on data, and thus combine the transfer function of neuron and synapse into one. And we have the current artificial neuron!</p>

<p>Deep learning aficionados will say that artificial synapses may be better than the fixed transfer function primitives of biological synapses, because the former are trained on data to provide the best function approximation.</p>

<p>Bottom line:</p>

<blockquote>
  <p>we do use synapse dynamics in artificial neural networks</p>
</blockquote>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*s-3Drwyk-HmGihWQdpiEOg.jpeg" alt="" /></p>

<h1 id="taking-cues-from-systems-level-modular-brain-architecture">Taking cues from systems-level modular brain architecture</h1>

<p><a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">The article</a>  mentions: “the remarkable modularity of the modern mammalian brain, conserved across species separated by 100 million years of independent evolution, suggests that this systems-level modularity might be beneficial to implement in AI systems”.</p>

<p>There is a large variety of neural architectural design that occurs every single days in the field of artificial neural networks, deep leaning. These are:</p>

<ul>
  <li><a href="https://medium.com/@culurciello/neural-networks-building-blocks-a5c47bcd7c8d">building blocks</a></li>
  <li><a href="https://towardsdatascience.com/neural-network-architectures-156e5bad51ba">architectures</a></li>
</ul>

<p>The advantage of artificial system is that we can experiment very rapidly today on many neural network architectures, to the point that we can even now search entire families of architectures automatically, with global optimization, and be able to create new and diverse sets of artificial primitives:</p>

<ul>
  <li><a href="https://towardsdatascience.com/neural-network-architectures-156e5bad51ba">learning neural network architectures</a></li>
</ul>

<p>Again I also want to point out that biological brains do so at a very slow time scale of millions of years, without a plan, without a global controller other than daily rewards and survival, by using random searches that is horridly inefficient.</p>

<p>Bottom line:</p>

<blockquote>
  <p>artificial neural networks have a immensely large collection of primitives, modules, architectures that exceeds biology by far, if not today, in just a few more years</p>
</blockquote>

<h1 id="biological-and-artificial-hardware">Biological and artificial hardware</h1>

<p>Most of us forget that the human brain and artificial neural networks today are based on completely different sets of materials and chemical elements.</p>

<blockquote>
  <p>Brains are made of cells, and microchips are made in silicon and metal</p>
</blockquote>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*yPF8VioUD54HVpSPEsN7tQ.png" alt="" /></p>

<p>If you only have cells immersed in a conductive ionic medium, transmitting electrical signal is hard for sure! Yet biological brains evolved to do just that. The had no other way to send signals across long distance but to do so with pulses. We talked about this a lot  <a href="https://towardsdatascience.com/can-we-copy-the-brain-9ddbff5e0dde">here</a>  and  <a href="https://towardsdatascience.com/neuromorphic-and-spiking-or-not-894a836dc3b3">here</a>. So that is really why biology uses spiking neural networks (1-bit signals, if you will). If it could signal with continuous levels without being swamped by noise, it would have! Because it would save a lot of energy.</p>

<p>In one sentence:  biology has leaky squishy fluidy tubes used as wires, while silicon has metal wires with nice insulation.  And this is the crux of the problem and the main difference of why one system has evolved in this way.</p>

<p>On the other hand electrical circuits used in todays computers are all digital and multi-bits. We started with analog signals too, just the old biological brains (and our eyes), because an analog signal has in theory infinite number of values (symbols) it can transmit. But then all infinite possibilities are narrowed to a few because of electronic fundamental noise, in the same way that electrical noise in biological medium forced it to become 1-bit pulses.</p>

<p>If you have 1-bit, how can you use it to represent many values? Time between pulses on one neuron or a group of many neurons of 1-bit can represent larger ensembles like in digital circuits.</p>

<p>You see: when you have neurons their output can have any number of bits you can afford in your system: 1-bit if you have to, or 4 or 8 or 16-bits if you can. And since many 1-bit neurons can represent larger bit numbers, then effectively you can decide:</p>

<blockquote>
  <p>do you want more 1-bit neurons or less multi-bit neurons?</p>
</blockquote>

<p>The balance depends on which technological medium you use and all its parameters.</p>

<p>Remember back-propagation works better with many bits, as its neurons needs to store smaller and smaller numbers while back-propagating over many layers. Today, we can back-propagate with just 8–9 bits per neuron (and another 8 in common across a group of neurons).</p>

<p>Learning is of course also vastly affected by the medium at your disposal. In artificial neural networks signal can travel further and with more noise immunity, so multi-bits and less neurons are the way to go. Learning also can use these properties to afford algorithms such as back-propagation and more global optimization as opposed to local rules dominated by short-range signals in a leaky medium.</p>

<p>An biological brain evolved over millions of years by making the same mistakes many times and replicating solutions in many places with many differences that are more the product of random search than intelligent design (with this I mean a global system design that predated the actual realization). lol.</p>

<p>Do we have the best digital circuit we can have? It can always get smaller. One day transistors may be just a few atoms, if we find ways to combat thermal noise and scaling. And there are new materials and new silicon devices coming up. One thing is sure: it will be hard to swerve away from silicon-based technology given our investment over the last 60 years. I have no crystal sphere here, so time will tell.</p>

<p>Often we heard that comparison about bird wings: how in biology we have flapping wings and in airplanes we do not. Most here forget to take into account the differences in size and also in materials used in both system. Small wings made of feathers are light and easy to activate by muscles, but large metal wings are not easy to move and the material may not be able to withstand the large forces of motion because of material properties. These two are in fact so different that it almost makes no sense to compare them! This is really the same in case of neuron and synapses: their structure and operation make sense in one domain, but not necessarily in others.</p>

<p>Bottom line:</p>

<blockquote>
  <p>cells and silicon each have paths of their own</p>
</blockquote>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*QtKnA0znK7Wxc_utsadKbg.jpeg" alt="" /></p>

<h1 id="learning-speed">Learning speed</h1>

<p><a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">The article</a>  mentions “It is clear in all three cases that humans receive vastly smaller amounts of labelled training data” forgets the fact that these networks are relatively small compared to biological ones and that biology evolved over millions of years, so effectively the species had 1,000x or millions of time more time and samples to learn from!</p>

<p>I think the article makes a point that once you have trained architecture biological ones learn with much less data and time. This is trues if you do not consider all the time biological network took to evolve and all the myriad of experiences that evolution integrated in the DNA of creatures.</p>

<p>We ought to remember that a lot of artificial neural network training is done from scratch, from tabula rasa. But once you have trained an artificial neural network you can transfer its weight to a new but similar domain or application without having to train again from zero and thus needing much less samples and time.</p>

<p>Bottom line:</p>

<blockquote>
  <p>biology learns a lot slower and much more inefficiently than artificial ones trained end-to-end</p>
</blockquote>

<h1 id="learning-to-operate-in-the-real-world">Learning to operate in the real world</h1>

<p><a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">The article</a>  makes good points that we need new neural architectures and learning techniques that can learn to provide complex functionality in the real world with limited training samples, and while performing continual learning.</p>

<p>Predictive neural networks are already making ways into learning unsupervised complex environments while avoiding to have to train the bulk of parameters on very sparse rewards.</p>

<p>We talked about ways to learn and operate in the real world  <a href="https://medium.com/@culurciello/at-the-limits-of-learning-46122b99dfc5">here</a>  and  <a href="https://medium.com/@culurciello/learning-and-performing-in-the-real-world-part-2-1fd3f393c2fd">here</a>  and  <a href="https://towardsdatascience.com/learning-and-performing-in-the-real-world-7e53eb46d9c3">here</a>. To summarize, we are still a long way from having robots explore large areas to find a target, or learn to do tasks like clean up a room or cook a meal.</p>

<p>The main missing ingredients on which the entire community is hard at work are:</p>

<ul>
  <li>imitation learning from videos</li>
  <li>predictive networks to predict how environment and actor will change once an action is performed</li>
  <li>mental simulations: predictions of sequences, revisiting what learned in imitation</li>
  <li>self-learning experimenting with mental sims and real tests, the reward is both getting to a goal and also making the rights steps towards it</li>
</ul>

<p>Q-learning, policy gradients, actor-critic etc algorithms were a good starting point, but do not possess the quality to scale to complex tasks.</p>

<p>It would be really great if we could learn from how the human brain learned to do these things, and how different parts of the brain cooperate to solve complex tasks. But we do not have the scientific tools to study learning on billions of neurons, nor the ability record from large neural populations (10,000 or more). Also, having worked in this area for quite some time in the past, I am unsure we even have a path to obtaining these tools in the next decades.</p>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*du-OpB-yrgEmt8A9mQL9vA.jpeg" alt="" /></p>

<h1 id="energy-efficiency">Energy efficiency</h1>

<p><a href="https://hai.stanford.edu/news/the_intertwined_quest_for_understanding_biological_intelligence_and_creating_artificial_intelligence/">The article</a>  mentions “The human brain spends only 20 watts of power, while supercomputers operate in the megawatt range”. We have deep learning accelerator microchips that can perform trillions of operations per seconds, at the level of super-computers that operate in the same ball-park of the human brains. We may not have these devices today, but the ones, like myself, that are working on it have a path in current silicon technologies to provide complex capabilities, the most advanced ones that deep learning and AI currently provides, in a 10 watts envelope or below. Our company  <a href="http://fwdnxt.com/">FWDXNT</a>works on such advanced systems, as well as many other companies such as Intel, NVIDIA, AMD, ARM, to name a few.</p>

<p>Bottom line:</p>

<blockquote>
  <p>the power efficiency gap between the human brain and what is possible in artificial neural networks is narrowing, and will soon converge to the limits of what physics allow</p>
</blockquote>

<p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/2018-12-09-bio-artificial-AI/1*ojG5kGNu29JOU333bd3FMA.jpeg" alt="" /></p>

<h1 id="conclusion">Conclusion</h1>

<p>A comparison between artificial and biological neural networks is something we are drawn to often, and it makes some sense in some ways, for example when we have to take rough inspiration or when we want to compare to abilities in one or another species.  In general, though, the two are diverging at a fast speed, due to the fact that we do not have good tools to study biological brains at scale, while we have very fast artificial tools we can experiment and create with quickly.</p>

<p>Final bottom line:</p>

<blockquote>
  <p>biological and artificial neural networks are made of very different material with very different properties and it only makes partial sense to compare their inner workings</p>
</blockquote>

<h1 id="notes">Notes</h1>

<p><strong>PS 1:</strong>  an older version of some of these discussions are  <a href="https://towardsdatascience.com/can-we-copy-the-brain-9ddbff5e0dde">here</a>  and  <a href="https://towardsdatascience.com/neuromorphic-and-spiking-or-not-894a836dc3b3">here</a>.</p>

<h1 id="about-the-author">About the author</h1>

<p>I have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://e-lab.github.io/html/contact-eugenio-culurciello.html">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>, and more…</p>


      </div>

      <div class="post-tail-wrapper text-muted">

        <!-- categories -->
        

        <!-- tags -->
        

        <div class="post-tail-bottom
          d-flex justify-content-between align-items-center mt-3 pt-5 pb-2">
          
          <div class="license-wrapper">
            This post is licensed under
            <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>
            by the author.
          </div>
          

          <!--
 Post sharing snippet
-->

<div class="share-wrapper">
  <span class="share-label text-muted mr-1">Share</span>
  <span class="share-icons">
    
    

    
      
        <a href="https://twitter.com/intent/tweet?text=Biological and artificial intelligence - Eugenio Culurciello Blog&url=http://localhost:4000/2018/12/09/bio-artificial-AI.html" data-toggle="tooltip" data-placement="top"
          title="Twitter" target="_blank" rel="noopener" aria-label="Twitter">
          <i class="fa-fw fab fa-twitter"></i>
        </a>
    
      
        <a href="https://www.facebook.com/sharer/sharer.php?title=Biological and artificial intelligence - Eugenio Culurciello Blog&u=http://localhost:4000/2018/12/09/bio-artificial-AI.html" data-toggle="tooltip" data-placement="top"
          title="Facebook" target="_blank" rel="noopener" aria-label="Facebook">
          <i class="fa-fw fab fa-facebook-square"></i>
        </a>
    
      
        <a href="https://telegram.me/share?text=Biological and artificial intelligence - Eugenio Culurciello Blog&url=http://localhost:4000/2018/12/09/bio-artificial-AI.html" data-toggle="tooltip" data-placement="top"
          title="Telegram" target="_blank" rel="noopener" aria-label="Telegram">
          <i class="fa-fw fab fa-telegram"></i>
        </a>
    

    <i class="fa-fw fas fa-link small" onclick="copyLink()"
        data-toggle="tooltip" data-placement="top" title="Copy link"></i>

  </span>
</div>


        </div><!-- .post-tail-bottom -->

      </div><!-- div.post-tail -->

    </div> <!-- .post -->


  </div> <!-- #post-wrapper -->

  

  

  <!--
  The Pannel on right side (Desktop views)
-->

<div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down">

  <div class="access">

  














  

  

















  
  </div> <!-- .access -->

  

</div> <!-- #panel-wrapper -->


</div> <!-- .row -->

<div class="row">
  <div class="col-12 col-lg-11 col-xl-8">
    <div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4">

    <!--
 Recommend the other 3 posts according to the tags and categories of the current post,
 if the number is not enough, use the other latest posts to supplement.
-->

<!-- The total size of related posts  -->


<!-- An random integer that bigger than 0  -->


<!-- Equals to TAG_SCORE / {max_categories_hierarchy}  -->








  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  
    
  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  


  

  

  

  

  

  








<!-- Fill with the other newlest posts  -->




  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
      
    
  
    
    
      
      
        
        
        
          





  <div id="related-posts" class="mt-5 mb-2 mb-sm-4">
    <h3 class="pt-2 mt-1 mb-4 ml-1"
      data-toc-skip>Further Reading</h3>
    <div class="card-deck mb-4">
    
      
      
      <div class="card">
        <a href="/2023/12/13/robotics-2024-part1.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Dec 13, 2023
  

  <i class="unloaded">2023-12-13T00:00:00-05:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Robotics and AI in 2024 and beyond — part 1</h3>
            <div class="text-muted small">
              <p>
                





                Robotics and AI in 2024 and beyond — part 1



a robot generated by a robot (DALL-E2)

Robots, such as the one populating sci-fi movies, are not yet part of our lives. We do not have robots that ca...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2023/12/05/Artificial-scientists.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    Dec  5, 2023
  

  <i class="unloaded">2023-12-05T00:00:00-05:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Artificial scientists and autonomous laboratories</h3>
            <div class="text-muted small">
              <p>
                





                

designed by Dall-E-2

Artificial scientists and autonomous laboratories

What is next for ChatGPT?

One of the goal of machine learning is to reach the levels of artificial intelligence. AI, the ...
              </p>
            </div>
          </div>
        </a>
      </div>
    
      
      
      <div class="card">
        <a href="/2023/05/24/Should-we-worry-about-AI.html">
          <div class="card-body">
            <!--
  Date format snippet
  See: /assets/js/_utils/timeage.js
-->





<span class="timeago small"
  

  
   >

  
  

  
    May 24, 2023
  

  <i class="unloaded">2023-05-24T00:00:00-04:00</i>

</span>

            <h3 class="pt-0 mt-1 mb-3" data-toc-skip>Should we worry about AI (machine-learning)?</h3>
            <div class="text-muted small">
              <p>
                





                Should we worry about AI (machine-learning)?

I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind…



Artificial intell...
              </p>
            </div>
          </div>
        </a>
      </div>
    
    </div> <!-- .card-deck -->
  </div> <!-- #related-posts -->



    <!--
  Navigation buttons at the bottom of the post.
-->

<div class="post-navigation d-flex justify-content-between">
  
  <a href="/2018/11/29/predictive-RL.html" class="btn btn-outline-primary"
    prompt="Older">
    <p>Predictive neural networks for reinforcement learning</p>
  </a>
  

  
  <a href="/2018/12/21/AI-in-2019.html" class="btn btn-outline-primary"
    prompt="Newer">
    <p>Artificial Intelligence in 2019</p>
  </a>
  

</div>


    

    </div> <!-- #post-extend-wrapper -->

  </div> <!-- .col-* -->

</div> <!-- .row -->



  <!--
  image lazy load: https://github.com/ApoorvSaxena/lozad.js
-->
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script>

<script type="text/javascript">
  const imgs = document.querySelectorAll('.post-content img');
  const observer = lozad(imgs);
  observer.observe();
</script>




        <!-- The Footer -->

<footer>
  <div class="container pl-lg-4 pr-lg-4">
    <div class="d-flex justify-content-between align-items-center text-muted ml-md-3 mr-md-3">
      <div class="footer-left">
        <p class="mb-0">
          © 2024
          <a href="https://twitter.com/culurciello">Eugenio Culurciello</a>.
          <!--  -->
        </p>
      </div>

      <div class="footer-right">
        <p class="mb-0">

          <!--Using the <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> theme <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a>. -->
        </p>
      </div>
    </div>
  </div>
</footer>


      </div>

      <!--
  The Search results
-->
<div id="search-result-wrapper" class="d-flex justify-content-center unloaded">
  <div class="col-12 col-sm-11 post-content">
    <div id="search-hints">
      <h4 class="text-muted mb-4">Trending Tags</h4>

      

















      

    </div>
    <div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div>
  </div>
</div>


    </div> <!-- #main-wrapper -->

    

    <div id="mask"></div>

    <a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button">
      <i class="fas fa-angle-up"></i>
    </a>

    <!--
  Jekyll Simple Search loader
  See: <https://github.com/christian-fei/Simple-Jekyll-Search>
-->





<script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script>

<script>
SimpleJekyllSearch({
  searchInput: document.getElementById('search-input'),
  resultsContainer: document.getElementById('search-results'),
  json: '/assets/js/data/search.json',
  searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0">  <a href="http://localhost:4000{url}">{title}</a>  <div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1">    {categories}    {tags}  </div>  <p>{snippet}</p></div>',
  noResultsText: '<p class="mt-5">Oops! No result founds.</p>',
  templateMiddleware: function(prop, value, template) {
    if (prop === 'categories') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`;
      }
    }

    if (prop === 'tags') {
      if (value === '') {
        return `${value}`;
      } else {
        return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`;
      }
    }
  }
});
</script>


  </body>

</html>

