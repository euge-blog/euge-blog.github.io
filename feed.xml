<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2025-04-11T10:14:52-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eugenio Culurciello Blog</title><subtitle>A minimal, responsive, and powerful Jekyll theme for presenting professional writing.</subtitle><entry><title type="html">100x microsystems and microchips</title><link href="http://localhost:4000/2025/03/27/microsystems.html" rel="alternate" type="text/html" title="100x microsystems and microchips" /><published>2025-03-27T00:00:00-04:00</published><updated>2025-03-27T00:00:00-04:00</updated><id>http://localhost:4000/2025/03/27/microsystems</id><content type="html" xml:base="http://localhost:4000/2025/03/27/microsystems.html"><![CDATA[<h1 id="100x-microsystems-and-microchips">100x microsystems and microchips</h1>

<p><img src="/assets/2025-03-27-microsystems/1.png" alt="" /></p>

<h2 id="history-and-microchips">History and microchips</h2>

<p>The microchips fueled the Silicon Valley revolutions in the 1970s and made the US the leading nation in the field of designing, creating, and scaling the use of integrated circuits. Soon followed by a revolution in Japan, Korea and Europe.</p>

<p>But in the late 90s, semiconductor fabrication started to migrate to Asia, first the chip packaging industry, where TSMC (Taiwan Semiconductor Manufacturing Company) emerged as a major player. Then the ‚Äúfabless‚Äù model, where companies design chips but outsource manufacturing, became more prominent and all US manufacturers happily outsourced the US national talent and technology abroad.</p>

<p>What was kept in the US was the design of integrated circuits and products. In a way, this is similar to other heavy industries like steel, chemicals, materials, car manufacturing industries that slowly globalized their operation in the 1970s and 1980s.</p>

<h2 id="today">Today</h2>

<p>Today the US has shed most of the production capability of advanced microchips, and to make things worse, also has a deep shortage of workers and know-how in the area of microchip fabrication, instrumentation and tools, deployment of fabrication facilities (fabs).</p>

<p>To make matters even worse, most of the microchip production of advanced components is in the geo-politically unstable island of Taiwan.</p>

<p>Europe maintained leadership in the area of microchip machinery needed in the fabs.</p>

<p>All the microchips you use in your car, microwave oven, fridge, computer, laptop, iphone, cell-phone, internet router, etc. are all built outside of the US. And that would be OK if it wasn‚Äôt that pretty much all of them are currently built only in Taiwan.</p>

<p>Globalization helps industry save cost by moving laborious and manual work to cheaper countries, but it does not help when the global trade is affected and there is a national need for the industry to manufacture on its own soil.</p>

<h2 id="what-has-happened-to-building-microchips">What has happened to building microchips?</h2>

<p>The way to keep advanced industries like the US in a prominent position is to keep advancing the field and innovate. But one can hardly innovate on something they do not produce anymore, or have no knowledge of the challenges of production.</p>

<p>Many of us today would like to see the development of microchip have the same trajectory of writing coding or 3D printing: use your cheap design tools at home, design your microchip, based on a cheap or free library of components, send it off to fabrication in a cheap and shared run, receive the parts after some time in the mail.</p>

<p>But no‚Ä¶ We spoke about the barrier before: <a href="https://euge-blog.github.io/2022/08/08/semiconductor-for-masses.html">https://euge-blog.github.io/2022/08/08/semiconductor-for-masses.html</a></p>

<p>In a nutshell the barriers (mountains) are:</p>

<ul>
  <li>No cheap CAD (computer aided design) software for microchips, coupled with foundry design kits</li>
  <li>Legal barriers in obtaining foundry design kits and circuit components (IPs) or intellectual property to enable Lego-like modularity</li>
  <li>Expensive production runs in modern fabrication processes</li>
  <li>Lack of design foundries readily available for prototyping and production, especially in niche areas of microchips like memories</li>
  <li>Funding an ecosystem to break these in US has failed because it keeps funding large corporation that have no interest to change their modus operandi, and in fact have squandered a fortune in taxpayers monty over the last 2 decades obfuscating the design of microchip even further</li>
</ul>

<p>These are seemingly insurmountable barriers to progress, but in reality all that is needed are better business models that foster sharing resources without sacrificing trade secrets.</p>

<p>Articles like this one: <a href="https://semianalysis.com/2025/03/11/america-is-missing-the-new-labor-economy-robotics-part-1/">https://semianalysis.com/2025/03/11/america-is-missing-the-new-labor-economy-robotics-part-1/</a> also talk about how iterating with groups in two continents 12h apart is not an effective way, and that building small cities where manufacturing in an area is concentrated can help to boost productivity and innovation.</p>

<p>Smart and industrious colleagues have spent years trying to democratize microchips: <a href="https://www.zeroasic.com/">https://www.zeroasic.com/</a> but unless they have the backup of a continuous stream of funding and the right attention of the leadership, they cannot alone move mountains.</p>

<p>Beside democratizing microchip design, another issue is innovation in fabrication techniques.</p>

<h2 id="the-illusion-of-light">The illusion of light</h2>

<p>Today microchips are built by light and gas. Gas is cheap but light is becoming the most expensive ‚Äúmaterial‚Äù.</p>

<p>Microchips today start with a flat disk of silicon, that is patterned using masks and light exposure of a very small wavelength, to produce nano-meter features. Features of a transistor used to be 1-2 micron wide in the 1990s, and progressively shrunk to today‚Äôs 5-3 nanometers. Today microchip fabrication requires the use of a light that is deep in the ultraviolet spectrum (small wavelength = small features) but is VERY expensive to produce and focus (lenses do not work, finding material to bend lights is a research endeavor).</p>

<p>If you think one day we could have the equivalent of a 3D printer at home to forge our microchips you would be surprised to learn that the current machine needed just to produce light for microchip fabrication is 2-containers big and cost a fortune. No 3D printer anytime soon üòî</p>

<p>Light is just not the way down any longer. We need to find better ways.</p>

<h2 id="the-missing-100x">The missing 100x</h2>

<p>AI today is still at least 100x less efficient than what we want it to be, taking the human brain as a proxy.</p>

<p>Today digital microchips powering your computers, laptop, cell-phone use power proportional to:</p>

<p>Static: P = I_leak V<br />
Dynamic: P = A C V^2 f</p>

<p>I_leak is the current that leaks off circuit when they are supposed to be OFF (but powered). <br />
V is the operation voltage of the circuit<br />
A is the fraction of time the circuit is active - say 50%typically<br />
f is the frequency of operation of the circuit</p>

<p>What can be done to reduce the power consumed by your processors?</p>

<p>Modern circuits operate by using a clock that generally uses ~50% of the overall power of a circuit. If your laptop uses 20 W of power, 10 are ‚Äúwasted‚Äù by the clock tree.</p>

<p><strong>IDEA #1: remove clocks</strong></p>

<p>Circuit designers have been trying to save power, reducing V to the physical limits of devices, today close to 0.7 V. Also by reducing C = making devices smaller = light-based scaling.</p>

<p>Today, barring a complete change of fabrication technologies, we can only do:<br />
<strong>IDEA #2: reduce f</strong></p>

<p>This means reducing the frequency of operation  = doing less. We cannot do this unless we also increase the number of devices accordingly. Instead of running 1 device at 1 GHz, run 1M devices at 1 KHz. Does this help?</p>

<p>It can only help if we make sure the I_leak of devices is low, so having more devices does not instead raise the power consumption, and that devices are used ‚Äúless‚Äù per unit time. This may mean using older microchip processes with less leakage, at the expense of more capacitance C.</p>

<p><strong>IDEA #3: reduce A</strong></p>

<p>Instead of using all your devices at 50% of the duty cycle, run them most sparsely, say 1% of the time. This would reduce power by 50x.</p>

<p>This requires re-thinking our algorithms so that they make use of sparsity in data or events.</p>

<h2 id="to-the-rescue">To the rescue</h2>

<p>What technologies can offer the gains we need?</p>

<p>I argue that we ought to move away from the current microchip fabrication technologies and instead revolutionize the field with new ideas. This is not easy as the current techniques are the pinnacle of human abilities in positioning layers of atoms, manipulating material at the atomic scale.</p>

<p>Can we go beyond silicon technologies?</p>

<ul>
  <li>The brain uses biological neurons</li>
  <li>The human brain the most complex computing device in the known universe</li>
</ul>

<p>Yet our ability to manipulate biology at the scale and resolution of microchip technologies is not yet developed.</p>

<p><img src="/assets/2025-03-27-microsystems/2.png" alt="" /></p>

<p>Also do you want a brain on your desktop drinking orange juice while doing your taxes? üòÄ</p>

<p>What other technologies can we use? TBD.</p>

<p>Before we can answer this question we need to know what kind of ‚Äúdevice‚Äù we are looking for. Maybe we need a ‚Äútransistor‚Äù or something we can turn ON / OFF using an input signal (digital or analog). Or maybe we need a ‚Äúneuron‚Äù that is again something we can turn ON/OFF or make it pulse (spike) based on MANY analog input signals.</p>

<p>Or maybe the basic building blocks are a transistor and a capacitor, or equivalent: one switch to get signal in and a storage tank to store some inputs.</p>

<p>In the most general sense we need to have a device that can take an input and use it to control its output. If we had such a device, we could build computers with it.</p>

<h2 id="thoughts">Thoughts</h2>

<p>In general, losing manufacturing to another country is never a good idea. It drains the brains and hands that can do specific jobs and also the opportunity for innovation.</p>

<p>Maybe one of the main reasons we do not have flying cars today is that we never tried. We were just too busy to reduce costs at the expense of innovation.</p>

<p>But what about reducing manufacturing costs IN YOUR OWN COUNTRY? This is the true INNOVATION!</p>

<p>It requires innovation in automation, robotics, high-throughput machinery, streamlined processes to obtain and process raw materials, store them, retrieve them, manipulate and forge.</p>

<p>Without local need for better manufacturing, we will lose this kind of innovation to other countries, slowing eroding competitiveness.</p>

<p>Only the government can help maintain manufacturing leadership, the ones with the vision to see the future and stick to long term 10s of years plans. It needs to concentrate factories and resources around a few areas, create an ecosystem.</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[100x microsystems and microchips]]></summary></entry><entry><title type="html">Robotics, manufacturing and the future</title><link href="http://localhost:4000/2025/03/11/robotics-future.html" rel="alternate" type="text/html" title="Robotics, manufacturing and the future" /><published>2025-03-11T00:00:00-04:00</published><updated>2025-03-11T00:00:00-04:00</updated><id>http://localhost:4000/2025/03/11/robotics-future</id><content type="html" xml:base="http://localhost:4000/2025/03/11/robotics-future.html"><![CDATA[<h1 id="robotics-manufacturing-and-the-future">Robotics, manufacturing and the future</h1>

<p><img src="/assets/2025-03-11-robotics-future/1.jpg" alt="" /></p>

<p>If there is one news article on call-to-action to read this Spring <a href="https://semianalysis.com/2025/03/11/america-is-missing-the-new-labor-economy-robotics-part-1/">this is it</a></p>

<p>Europe and US have slowly hemorrhaged manufacturing capabilities in the last 40 years. Not just in cars, equipment, appliances, but also advanced manufacturing of microchips, sensor, processors.</p>

<p>Research funding to innovate in robotics and manufacturing come from all over the world, not just US, but a lot of the innovations until year 2020 or so were.
Yet we failed to translate research into a revolution that can bring back to our home countries cheap and fast manufacturing.</p>

<h2 id="why">Why?</h2>

<p>1- because of a short investment cycle that forces to productize in 12-18 months in a field that required more time and more iterations</p>

<p>2- the BARRIER of physical embodied manipulation and locomotion data, and the lack of coherent robotic platforms</p>

<p>3- the scarce interaction between manufacturing and the robotic research products: often not fancy robots but simple contraptions win in a factory. One of a kind machines, but built out of manufacturing components that are often the same at different scales and operating on different materials</p>

<p>4- we failed to create a cheap factory of factories, where iterating on design can be fast, instead we shipped it all happily abroad, losing speed along the way</p>

<h2 id="what-can-we-do">WHAT CAN WE DO?</h2>

<ul>
  <li>Co-locate research and manufacturing test facilities. Everyone loves a desk job, but robotics needs constant testing in the field</li>
  <li>More rapid prototyping at home. Production can still be elsewhere, but fast iterations on design are paramount</li>
  <li>Keep applied research money flowing: grants and government funding should come from active collaborations with industry</li>
  <li>Create factory cities, where fast manufacturing and prototypes can be developed all or most in one location</li>
  <li>Train the workforce to operate machines, to produce multiples of their manual capability, to think and devise and implement. Shift from passive jobs to more active roles and mentality: ‚Äúhow can I improve this?‚Äù, ‚Äú‚Äùhow can I make more money for the company that employs me?‚Äù</li>
</ul>

<h2 id="yes-but">YES BUT:</h2>
<ul>
  <li>These plans may sound good, but unless there is strong home-based manufacturing and sales of robotics, they cannot succeed</li>
  <li>We still do not have proper brains for robots that can operate mostly autonomously - this may be an advantage, as it gives us some time to think and plan</li>
</ul>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics, manufacturing and the future]]></summary></entry><entry><title type="html">Artificial Intelligence, AI in 2025 and beyond</title><link href="http://localhost:4000/2025/01/03/AI-in-2025.html" rel="alternate" type="text/html" title="Artificial Intelligence, AI in 2025 and beyond" /><published>2025-01-03T00:00:00-05:00</published><updated>2025-01-03T00:00:00-05:00</updated><id>http://localhost:4000/2025/01/03/AI-in-2025</id><content type="html" xml:base="http://localhost:4000/2025/01/03/AI-in-2025.html"><![CDATA[<h1 id="artificial-intelligence-ai-in-2025-and-beyond">Artificial Intelligence, AI in 2025 and beyond</h1>

<h2 id="some-thoughts">SOME THOUGHTS‚Ä¶</h2>

<p>We are reaching the second quarter of the 21<sup>st</sup> century: it is an exciting milestone. I have been in ‚Äúadult‚Äù life in the last 25 years and most of it was dedicated to learning technologies such as electrical engineering, computer design, AI, machine-learning, microchip design, etc.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/1.jpg" alt="" /></p>

<p>Gemini-Pro: ‚Äúa robot learning to see like a baby on a table with toys‚Äù</p>

<p>Looking back at the last 25 years, much has changed, and little has changed. In the year 2000 we had cellphones, and we had the Internet, albeit it was not as fast. We had social networks, but we could not really carry a computer and apps with us yet. AI was non-existent but there were plenty of computer algorithms such as internet search. Actually‚Ä¶ we did have the Palm Pilot, so we did have a computer in our hands. Also: it did have apps.</p>

<p>Our cars had mostly combustion engines, and we had no robots to help around the house. Our TV were much bulkier and the screen much smaller. We had video game consoles, and we also had portable video games‚Ä¶ that we have even in the previous quarter century!</p>

<p>Our lives did not change much, but they also did. We spend a lot less time with other people now. We work with them online and we play with them online. We used to talk to them on the phone, but now we prefer to text them. We get aggravated much more often for the use of words uttered online, and we are less amenable to meet face-to-face or cry on each other shoulders.</p>

<p>One thing that changed is how we interact with computers and code today. Since the last 2-3 years we have seen the rise of foundational AI models. These are model that can provide a variety of use without the need to be explicitly programmed or provided with large number of examples.</p>

<p>What is next in the field of AI? What is gaining traction and what is just smoke? This is the main content of this article.</p>

<h2 id="automatic-computer-code">AUTOMATIC COMPUTER CODE</h2>

<p>When I started working on neural networks, around 2010, we were dreaming of writing the last program we will ever need to write: the neural network that learns it all and can now code itself.</p>

<p><em>Now we are there.</em></p>

<p>We have foundational models that can write code, and when looped into agents, they can also run the code, gather error messages, and correct their own code. They can even write modules to extend their capabilities‚Äîas in a dream scenario Matrix-like movie.</p>

<p>Today one can write an entire cell-phone application from scratch without having to write a single line of code, and at the most needing to feed-back manually error messages and asking the agent: ‚Äùplease fix it‚Äù. Similarly, one can write multi-file coding projects to completion, even graphical interfaces, for example feeding-back Java-script issues from a web demo.</p>

<p>I said we are there, but it is not quite‚Ä¶</p>

<p>Agentic coding models today can write an entire program that is mostly functional. Error messages can be fixed by the agent, but sometimes agents get stuck in a loop because they cannot find the right file or web link. Or they get stuck because they feed back the same information and answers over and over.</p>

<p>The trouble comes when we have some graphical output, as in a web page or a cell-phone application or when using an external graphical tool. Then current coding agents operate open-loop, meaning that they do not have yet the ability to feed-back graphical output information. Given that we already have multi-modal models that can ‚Äúlook‚Äù at images and screen capture snippets, I would say we will be able to close the loop soon. Today you can experiment with web-based AI Studio tools to have them code a website based on an image of another website, or a cell-phone app view.</p>

<p>Outlook: gaining significant traction</p>

<h2 id="generating-writing">GENERATING WRITING</h2>

<p>Of course, AI that can write for you have been around a couple of year starting with ChatGPT and all its siblings foundational large-language models (LLMs). The tool can be prompted to provide well-written text, or to improve existing text snippets or paragraphs. It makes us all proficient writers.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/2.jpg" alt="" /></p>

<p>If you tried to generate significant portions of text with LLMs, you know that the best way is to describe what you want generated with the maximum number of words. This sometimes will reveal the nature of the beast: it takes significant text input to specify all we want to be generated. This is even so when using coding AI models or foundational AI models with graphical outputs.</p>

<p>Most of us may start with a list of bullet-points notes, and LLM often reply by extending such lists, but not necessarily generating entire paragraphs for each. It will take more effort and prompting to get what you need.</p>

<p>Summarization of text is very effective if the input is just a few pages. When trying to gather information from hundreds of pages, the best approach may be to do it step by step, by breaking down the input text into a few pages, and running the same prompt on each. Retrieval-augmented generation, or RAG, is a technique to feed large amounts of text to an LLM. Think hundreds or thousands of pages from many documents. RAG has severe limitations because it does not use the full power of the LLM. Foundational models build a knowledge graph that connects all concepts they were trained on. RAG instead retrieves pieces of text by similarity using smaller and more limited AI models, and then feeds the prompt and retrieved section to the LLM.</p>

<p>A better approach is using the full LLM power on the entire body of text we want to use. This is of course costly, as we will need an LLM to really read all pages and extract the information we want. Say for example we want to search for ‚Äúthe recipe for eggplant pasta‚Äù over 100 cooking books. This is easily done with RAG or a simple and old basic text search. The power of LLM comes when you need to search nuanced version for your text that need understanding. For example: ‚Äúwhat recipe need to stir-fry and eggplant in small cubes or slices?‚Äù may need more than just plain text search. You can think of even more difficult examples that will quickly reveal limitation of RAG, such as ‚ÄúSummarize the take-home points of this meeting transcript‚Äù.</p>

<p>An LLM agent may need to parse the entire body of text, and then summarize useful content into a ‚Äúworking memory‚Äù. This can be then fed to the LLM to provide answer against a prompt, or to extract information. This is more costly, but it is also similar to the way humans parse large amount of text or data. Divide and conquer.</p>

<p>One area that still needs work is when you need to fill a form or document using data from many other documents. This tedious job is an everyday frustration for everyone working a desk job, yet it has not been satisfactory addressed to data. One reason is that documents are intrinsically graphical, and filling forms requires an understanding of the graphical sections and where to place content, and under which section. This is often obvious to a human, but not as obvious to an LLM. Complications also arise from the need to extract characters and words from the document (OCR) and the relative position of the form input areas.</p>

<p>Outlook: text-based LLMs are improving and very useful for desk jobs. More features are surely coming.</p>

<h2 id="generating-images-and-videos">GENERATING IMAGES AND VIDEOS</h2>

<p>Generative AI gets our attention even more when we see the images and videos it can create by just using textual prompts. The power of connecting images and captions allowed the creation of masterful neural networks that can transform text into images, images into videos, refine image, augment resolution, add or remove portions of an image or video, and even create them from noise.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/3.jpg" alt="" /></p>

<p>Today image generation is very advanced and can produce any kind of realistic or stylized array of pixels. The new tools also allow to directly change or adapt portion of an image to our descriptions and commands. This was not the case in the past, but even commercial tools now allow to mark a region of interest, sometimes automatically, and then change it as desired.</p>

<p>Video on the other hand is still in infancy, producing short segments that are consistent, but rarely more than a handful of seconds long. Videos of a single person talking or close-up can generally fare better, and can be even used as digital customer-care agents.</p>

<p>Even entire video games can be realized in the direct movie format today. These are video game sequences produced entirely with textual inputs. They do rely on the same video-generating techniques but are impressive in the way they emulate the changes as if you were moving inside a 3D world of a modern video-game. These tools will surely be utilized to create in-game animations and other video snippets for gaming, and also for video effects and movies production.</p>

<p>Outlook: video-producing foundational AI models have still some ways to go before being able to produce substantial parts of our videos and movies.</p>

<h2 id="applications-of-llms">APPLICATIONS OF LLMs</h2>

<p>What are the next possible applications of foundational AI models? If you look online today there seems to be an LLM for everything: law, healthcare, educational tutors, meeting aids and summarizers. If you can turn your data into text, LLMs are going to be the best way to solve your problem, whether it is writing or organizing textual knowledge or generating new text from existing text.</p>

<p>The ease of use of LLM, the fact that they need less data and the complex training techniques of Neural Networks 1.0 (the ones from ~2010 to 2020) significantly lowered the barrier to creating and adapting foundational models for a variety of uses. This also means that a team without AI or neural network expertise can now create entire businesses just using foundational cloud-based LLMs and models. This in a way satisfies the wish of neural network researchers: they wrote the last program you will ever need!</p>

<p>What makes an application of LLMs a viable business opportunity, or a just a useful tool? Most application of LLM will require large complex models that reside online, therefore requiring hefty monthly usage fees. If the value added to the user is higher than those fees, then there is a clear business model. As an example, a LLM that can help lawyers navigated thousands of pages to prepare for trial is a probably going to make money. On the other hand, an application as ‚Äúask me any question about cats/dogs, and how to take care of them‚Äù may be satisfied by smaller LLM models but more importantly may not afford the same monthly or yearly payments from its potential users.</p>

<p>What is exciting for investor is that the field of foundational AI model gave software as a service (SaaS) another large push, potentially gaining much from little investment. But lowering the barrier to entry means also more noise, more competition, with difficult value proposition or clear advantages for users. This translates to company that are not really deep-tech, but rather a purely marketing play with a really good user forum and customers interaction team.</p>

<p>One suggestion to investors and startup founders is to look at niche areas of use of LLMs, in small potential focused pockets of advantage, where the value is high. Examples could be ‚ÄúAI for mining materials‚Äù, ‚ÄúAI for common hearth diseases‚Äù, ‚ÄúAI for quantum computing‚Äù and so forth. Many of these niche applications may require the use of multi-modal models, to differentiate by added functionality that goes beyond parsing just text data.</p>

<p>Another idea to consider is to try to rely on local, smaller open-source foundational models and LLMs, thus opening the door to a large reduction in cost of operations. Even better if the software or application can be installed on user hardware (their phones or PCs) thus removing the needs for custom cloud setups.</p>

<p>Outlook: text-based applications are already saturated and the ease of use of AI tools and LLM is lowering the barrier to entry. Today anyone can make AI applications very easily.</p>

<h2 id="multi-modal-llms">MULTI-MODAL LLMs</h2>

<p>Multi-modal LLMs are foundational AI models that build a knowledge graph based non only of text and words, but also on concepts available in other forms of media. For example, the idea of a ‚Äúcat‚Äù is not just the word, but also the way it looks, moves, purrs, smells. Similarly, we need AI foundational models to be able to create a knowledge graph over the data we are most familiar with text, images, videos, plots, diagrams, tables, databases, dataset files, and more.</p>

<p>What will this do? It will give foundational AI model the ability to work and reason more closely to the human plethora of senses, and thus a more real understanding of the physical world.</p>

<p>What applications will this enable? The first one is in coding foundational models that can close the loop with graphical outputs. For example, we can use multi-modal AI models to create webpages, cellphone applications, but also complex graphics as in photo editing, 3D models, engineering and architectural designs, mechanical parts, etc.</p>

<p>As an example, today it is still very difficult to design 3D objects given the sophistication and complexity of tools such as Blender, 3D software editors, CAD tools for engineering, architecture, mechanical design. Similarly, video editing and photo editing depends on too many menus and highly complex operational pipelines, often offering multiple solution to achieve the same objective. Multi-modal LLMs will be able to remove or lower the barrier of entry to a wider set of users.</p>

<p>Outlook: multi-modal foundational AI models are going to provide yet another major revolution in automation and concept learning.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/4.jpg" alt="" /></p>

<h2 id="robotics">ROBOTICS</h2>

<p>The field of robotics is still behind. Robots today offer very limited abilities to help us on everyday tasks such as cooking, cleaning the house, driving in all-weather situations, automating a home. The main reason is the lack of a robotic brain that can learn embodied multi-modal concepts.</p>

<p>Generative AI models, and specifically multi-modal foundational models are the core of a robotic brain. The missing step for robotics is embodiment. This means correlating the robot own actions with the multi-modal concepts if can perceive. Datasets that show how a robot should move to fulfill a task are few and cannot be easily transferred to other robots or configurations. We have a myriad of video of people performing any task, but these do not contain explicit sequences of control for arms and legs and actuators.</p>

<p>Humans learn to imitate by watching others perform a task. We have not yet devised an algorithm that can do the same and with the same learning speed. In other words, the missing piece is an algorithm that can learn by watching videos. Learn to control its own limbs in the same way as the actors in the video.</p>

<p>Outlook: robotics is still behind the revolution of generative AI models and will be for another few years.</p>

<h2 id="embedded-ai">EMBEDDED AI</h2>

<p>Embedded AI foundational models is one area that has not progressed as much as their cloud counterpart. This is because of the more limited computing capabilities of embedded devices ‚Äì these devices are similar to cell-phones hardware than a laptop, and also the more limited capabilities of smaller AI foundational models.</p>

<p>Embedded AI can have major application in smart cameras, home appliances, smart homes, but also on drones, bicycles and scooters, robotics, and vehicles. These devices need to have a combination of medium-range LLMs and / or high-performing vision systems. These models would allow embedded devices to understand a visual scene of at least 1080p resolution, ideally 4K and operating in real-time. They can also offer the ability to run 8B to 30B equivalent LLM models on device and with at least 10 or more tokens per second of outputs.</p>

<p>Embedded processors like the RockChip RK3588, NXP iMX8 processor, or the Qualcomm Snapdragon X Plus offer powerful standard CPUs coupled with high-performance AI accelerators and embedded GPUs. These processors, coupled with small AI foundational models and proper high-quality trained models, can offer a solution for the space of embedded applications.</p>

<p>One note to add is that beside all the complex AI capabilities we have today, simple solutions such as turning on a light when a person enters a dark room still require an expert to set up and deploy‚Ä¶ clearly we have a lot to do to make the technology work for us, including all software and hardware manufacturers, so that simple things as home automation can finally bring us really smart homes. But this is just an example we can all understand, because imagine the same devices working in a smart factory, helping reduce cost, improve safety for workers, and monitor all aspects of production.</p>

<p>Outlook: expect more and more capabilities from small portable embedded devices, getting progressively close to the same cloud capabilities of one year ago, at least from a single user perspective.</p>

<h2 id="ai-hardware">AI HARDWARE</h2>

<p>AI needs hardware to run. The recent surge of LLM has put some serious demand on the potential of silicon-based microchips. Traditionally memory microchips and computer processor microchips have been implemented in different silicon manufacturing processes, with different materials and substrates that make them impossible to merge. In addition, the 2D layout of microprocessors puts a constraint over how many wires of the same length one can afford to connect neighboring microchips. The combination of these issues has made memory interactions with processors a bottleneck. The 2D nature of microchips also places constraints on the number of processing elements that can be arranged in proximity. So really the limitation of modern microchips is density and 2D confinement. We will see that it boils down to wires per unit area.</p>

<p>What precludes us from making 3D microchips? HEAT ‚Äì as modern microprocessors and memory microchips produce heat during operation at high frequency which cannot be easily removed in a sandwich of many layers. Typically, heat is removed with fluid immersion or air-cooling. The latter being the prevalent mode, given that fluids complicate the design of circuit boards and add a lot of cost and manufacturing complexity. But what about complete fluid immersion? That is one of the best ways to deal with the problem at least at scale, but again introduces additional complexity: we will have to fish the computer out of fluid every time we need to service it.</p>

<p>Another issue in making 3D stacks of microchips is that they will need to share wires between them. These wires need to reliably connect each microchip, but today fabrication processes limit the yield of 3D integration to about 10 layers, after which the rate of failed connections limits the viability and efficiency of 3D stacks.</p>

<p>Even using fluid immersion cooling, the real limitation remains how many wires we can place in a unit area. It is possible to run more stacked microchip at a lower speed if we can share information between them. This means we need many wires to connect the microchips, ideally 1000s per millimeter square. Today this limitation is we can place a wire every 25-micron square, more than 1000 wires per millimeter square. The formula to calculate how much data we can share between microchips is approximately:</p>

<p>Data interchange = number of wires * frequency of operation of each wire</p>

<p>Of course, how much data interchange (also called memory bandwidth) needed is a function of individual applications. The issue remains: memory and processor are separated and not often 3D integrated.</p>

<p>The modern AI microchips use stacks of memory in their HBM memory, today 8 layers running ~1000 wires at 8 GHz. Processors can sometimes have another memory stacked on top: SRAM, but it is not yet typical.</p>

<p>Because of these limitations today AI microchips can provide only a limited number of Tera-operations per Watt (performance per Watt of electrical power). LLMs today require large number of processing elements and large amounts of memory, all connected by very large memory bandwidth. That is because LLM are often large neural networks using 8 or 70 or hundreds or thousands of GB in weights only. All those weights need to be available on the AI processor every time we ask it to compute a new token, therefore putting enormous constraints to recent memory microchips.</p>

<p>One would want today to speed LLM 100 if not thousands of times, but the limitation of microchips, their fabrication, arrangement, wires and thermal profiles makes it impossible to achieve more than a few efficiency multipliers for the next few years. In addition, there is no other technology that can come to help the need for more efficiency in AI hardware.</p>

<p>Outlook: do not expect your AI hardware to run much faster or more efficient for the foreseeable future</p>

<p><img src="/assets/2025-01-03-AI-in-2025/5.jpg" alt="" /></p>

<h2 id="ai-data-centers">AI DATA CENTERS</h2>

<p>The demand for AI data centers has surged significantly with the raise of foundational AI models, and it has increased further in the last 2-3 years. Electrical power delivery is now one of the main reasons that limits the expansions of data centers, because it takes time to adapt the national power grid to support the large amounts of energy required by AI data centers (AI-DC). 50-100 MW are now the norm, with large data centers requiring 500 MW or more. These amounts of power production are not easy to come by and are also limited by the shortage of power transformers (not to be confused with the neural network transformers that power foundational AI models), some of which require 18 months to be manufactured.</p>

<p>Data Centers continue to be a commodity in the foundational AI age. The value of buildings, wires, racks and maintenance is very low. The real value is the AI hardware and computers, specifically AI processors like GPUs. These specialized GPUs for AI now are moving up the chain: today we can buy special clusters and entire racks for data centers.</p>

<p>Who is making money in data centers? The makers of microchips for GPUs, computing, networking, and computer memory are the real winners. But also providers of software to manage data centers and their operation, including data and databases are also in more demand, albeit many open-source and free solutions exist and are openly available.</p>

<p>Outlook: AI data centers will continue to expand in the next 10 years, fueled by new multi-modal models and larger foundational models for robotics and real-life applications.</p>

<h2 id="privacy">PRIVACY</h2>

<p>With foundational AI models primarily being in the cloud, users of the technology have less chances to keep their data at home and are being forced to place more and more of it online. This opens the door to sovereign monitoring and surveillance, which is already commonplace in many retrograde countries.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/6.jpg" alt="" /></p>

<p>Computers and storage have been moving to the cloud for a while, because it is easier to have someone else assemble and maintain if for you. It does come with a price! Usually the cost is 2-3x more than DIY computers on-site.</p>

<p>One complication with generative AI is the size of the models and the hardware requirements which can be above and beyond what one can afford on-site. For example, large 70B or 400B models can only be run respectively on a 1-GPU and 8-GPU systems with discrete large GPUs. Only smaller model 7B or less than 30 B can run on laptop-grade hardware. This clearly makes the cloud move inviting, again at the expense of more price.</p>

<p>But not everyone is comfortable to send data to a cloud provider. Small and medium enterprises with a considerable know-how are not keen on using the cloud. And so are many consumers and citizens that are considering privacy of their own data a priority.</p>

<p>One movement that is taking hold is to host all data and some foundational AI models locally on your own hardware. Laptops, desktop PCs and small networked devices for data are being converted to local AI powerhouse, sitting together with the data. These devices can implement local RAG system, local knowledge and search, and can also implement coding AI tools that use the company own data locally. They can also be used by consumer to batch-label a large collection of video and photos, and for other personal data needs, such as filling forms and documents based on your own private legal documents.</p>

<p>Not to say that cloud data and privacy do not go together. Most cloud provider offer industry excellence in data and computer security. But there are lingering issues: one is that by concentrating data and computers, the cloud providers are also target or many more cybersecurity infiltration attempts; second is that computer security is never guaranteed by default, as new exploits are discovered daily and promptly used by malicious agents. That is also not to say that every local home setup is secure. But if provided with the right safeguards and software that enforces good security measures, it can be more secure than a giant well-known cloud provider.</p>

<p>Outlook: the cloud will try to expand, but a small contingency of local and private devices is making their way and will become progressively more important.</p>

<h2 id="training-ai-foundational-models">TRAINING AI FOUNDATIONAL MODELS</h2>

<p>High-quality training material is fundamental to training AI foundational models. All the first generation of LLMs (2022-2023) were trained on very large collection of text from the Internet, magazines, publications, and all the data that we could easily access. But recently, it became apparent that smaller models trained on high-quality material were as good as larger models trained on much more data. Today models that are 400 or even 70 B compete with models much larger of 1-2 years ago.</p>

<p>But what is high-quality training material? It is basically textbook-quality information of well-known facts rather than the confabulations and dialogues of random people talking about facts on the internet. It is like school versus street knowledge. Imagine we take all the training material we use for human students from when they are baby to well past University. Imagine we feed all the expert textbooks on all subjects we want our LLM to be familiar in. If we apply a curriculum, step-by-step, year-by-year for all this content, we have very well distilled quality material that is superior to what one can get by absorbing it though the environment over the years. Similarly, using school grade material to train LLMs makes them learn much faster and be more accurate than training with data from discussion forums. Do not get me wrong, some discussion forums are full of useful information also, but they need to be used only after pre-training with high quality training material. That is very much equivalent to how humans learn!</p>

<p><img src="/assets/2025-01-03-AI-in-2025/7.jpg" alt="" /></p>

<p>So where do we get this high-quality material? For k-12 school material, it is easily available in many free textbooks and courses, including student exercises and their solutions. And then there are the more advanced textbooks from college to grad-school to expert manuals. Publications are not high-quality material because they are not yet validated by practice and wide use, and because scientific papers do not reveal all the disadvantages of techniques, but rather they are biased to the advantages for publications. Textbooks instead average our knowledge over many years and they mostly only report techniques and ideas that are validated. More importantly textbooks can provide more information on what works and what does not, under which conditions scientific techniques are valid or not, and thus provide high-quality knowledge.</p>

<p>This topic really makes you think what is knowledge and how is knowledge acquired. LLMs behave like humans when acquiring knowledge, and research demonstrated that a curriculum approach from simple concept to more complex topics is the way to go. Using the right information, as opposed to all possible information also leads to better learning. Humans can learn fact from experience, from word of mouth, gossip, and urban legends, but that knowledge is not always correct or grounded.</p>

<p>Outlook: high-quality training material to train LLMs is the new gold.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/8.jpg" alt="" /></p>

<h2 id="future">FUTURE</h2>

<p>What does the future of foundational AI have in reserve for us? Here are some ideas based on the content of this article:</p>

<ul>
  <li>Foundational models will get smaller and powerful</li>
  <li>High-quality training data is of paramount importance</li>
  <li>Cloud will continue to grow, but embedded, private, local solutions will grow faster</li>
  <li>Robotics will advance slowly for a few years. Similar prospects for autonomous vehicles</li>
  <li>Your data privacy will become more important and force a push away from the cloud</li>
  <li>AI hardware will promise to get more efficient, but will face physical barriers</li>
  <li>Build the infrastructure not the apps</li>
</ul>

<p>We have a part in this future, both to guide applications and / or to build them. Enough reading: time to act!</p>

<h3 id="ps">PS:</h3>
<p>this essay was written without generative AI. Yes it shows‚Ä¶</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Artificial Intelligence, AI in 2025 and beyond]]></summary></entry><entry><title type="html">What about my hair?</title><link href="http://localhost:4000/2024/12/17/what-about-my-hair.html" rel="alternate" type="text/html" title="What about my hair?" /><published>2024-12-17T00:00:00-05:00</published><updated>2024-12-17T00:00:00-05:00</updated><id>http://localhost:4000/2024/12/17/what-about-my-hair</id><content type="html" xml:base="http://localhost:4000/2024/12/17/what-about-my-hair.html"><![CDATA[<h1 id="what-about-my-hair">What about my hair?</h1>

<p>For all the advances in technology and all the promises of space exploration I still think we are behind in our understanding of life and our own abilities.</p>

<p>For decades we talked about specialized medicine that can be tailored to an individual person, that can cure an illness without side effects. Or gene therapy, where unfortunate individual that have nefarious mutations would obtain a fix for their erroneous DNA.</p>

<p>Technology, and especially AI makes us think that we can achieve all these goals in a short time. AI being code can in fact be changed and updated faster than generations of DNA mutations. But AI being code also does not live in the physical space, and it is not yet able to interact with our world. At least not in the way we want. An autonomous car is AI on wheels acting on the physical world. But our forecast of having fully functional self-driving cars right about now were in fact incorrect, a gross miscalculation of our abilities to drive AI (pun intended). We are in fact many years away from cars that can reliably drive in the snow and heavy rain to the same level as human beings.</p>

<p>Unfortunately, we drank our own Kool-aid. And similarly, AI is not going to help us to understand our biology anytime soon. Why? Maybe partly because of the fixation of that field of science to understand it all. Or many other fields of science, thinking that we are in fact clearly able to derive equations and laws for everything and that our mind would be able to understand it all, no matter the number of variables or components in the system.</p>

<p>But we can only understand the smoke as a trend. We can model the smoke with equations that on average give us an approximate view of how the smoke cloud evolves in time. We will never understand how each particle of smoke is moving and having all equations for all of them. That would be insane. Then why do they want to doit for every cell in our body or every neuron or synapse in our brain?</p>

<p>Think we are being tough? But then what about my hair? Suppose you need more of your own hair on your head‚Ä¶ well we do not even have a way to grow our own hair back. In other words, we cannot even force our body to grow more hair where we want to.</p>

<h3 id="ps">PS:</h3>
<p>this essay was written without generative AI. Yes it shows‚Ä¶</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[What about my hair? For all the advances in technology and all the promises of space exploration I still think we are behind in our understanding of life and our own abilities. For decades we talked about specialized medicine that can be tailored to an individual person, that can cure an illness without side effects. Or gene therapy, where unfortunate individual that have nefarious mutations would obtain a fix for their erroneous DNA. Technology, and especially AI makes us think that we can achieve all these goals in a short time. AI being code can in fact be changed and updated faster than generations of DNA mutations. But AI being code also does not live in the physical space, and it is not yet able to interact with our world. At least not in the way we want. An autonomous car is AI on wheels acting on the physical world. But our forecast of having fully functional self-driving cars right about now were in fact incorrect, a gross miscalculation of our abilities to drive AI (pun intended). We are in fact many years away from cars that can reliably drive in the snow and heavy rain to the same level as human beings. Unfortunately, we drank our own Kool-aid. And similarly, AI is not going to help us to understand our biology anytime soon. Why? Maybe partly because of the fixation of that field of science to understand it all. Or many other fields of science, thinking that we are in fact clearly able to derive equations and laws for everything and that our mind would be able to understand it all, no matter the number of variables or components in the system. But we can only understand the smoke as a trend. We can model the smoke with equations that on average give us an approximate view of how the smoke cloud evolves in time. We will never understand how each particle of smoke is moving and having all equations for all of them. That would be insane. Then why do they want to doit for every cell in our body or every neuron or synapse in our brain? Think we are being tough? But then what about my hair? Suppose you need more of your own hair on your head‚Ä¶ well we do not even have a way to grow our own hair back. In other words, we cannot even force our body to grow more hair where we want to. PS: this essay was written without generative AI. Yes it shows‚Ä¶]]></summary></entry><entry><title type="html">Insights into the use of Generative AI systems</title><link href="http://localhost:4000/2024/10/31/llm-use-cases.html" rel="alternate" type="text/html" title="Insights into the use of Generative AI systems" /><published>2024-10-31T00:00:00-04:00</published><updated>2024-10-31T00:00:00-04:00</updated><id>http://localhost:4000/2024/10/31/llm-use-cases</id><content type="html" xml:base="http://localhost:4000/2024/10/31/llm-use-cases.html"><![CDATA[<h1 id="insights-into-the-use-of-generative-ai-systems">Insights into the use of Generative AI systems</h1>

<p>Generative AI models, such as large language models (LLMs), have demonstrated remarkable capabilities in mimicking human-like communication. We here explore recent trends and use cases observed among developers and companies utilizing generative AI. Our observations highlights common challenges and opportunities within this rapidly evolving field.</p>

<h2 id="1-generative-ai-cannot-read-your-mind">1. Generative AI cannot read your mind</h2>

<p>A typical use of Generative AI systems is to read all internal company data and provide a way to search its knowledge. Generative AI systems excel at providing informative and comprehensive responses when given clear and specific prompts. However, terse, vague or ambiguous prompts can lead to misunderstandings and inaccurate results.</p>

<p>Consider these examples:</p>

<ul>
  <li>
    <p>Insufficient Context: A prompt like ‚Äúwho is Elon?‚Äù without additional context may result in the AI returning information about the famous entrepreneur Elon Musk rather than the CTO of a specific company name Elon Tusk, even if this is part of the internal data.</p>
  </li>
  <li>
    <p>Complex Tasks: For tasks requiring extensive knowledge, such as creating an approval plan based on 1,000 pages of regulatory documents, providing the relevant documents as context is crucial, instead of using terse prompts like ‚Äúgive me an approval plan for ‚Ä¶‚Äù.</p>
  </li>
</ul>

<p>To ensure accurate and relevant responses from generative AI, it is essential to provide well-crafted prompts that include all context and details. By doing so, users can maximize the system‚Äôs potential and avoid misunderstandings.</p>

<h2 id="2--iterative-problem-solving-with-generative-ai">2- Iterative Problem-Solving with Generative AI</h2>

<p>While generative AI systems can be powerful tools, complex tasks often require more than a single-shot interaction. Problems like solving math equations or coding webpages may involve multiple steps, such as:</p>

<ul>
  <li>Planning: Breaking down the problem into smaller, manageable subtasks.</li>
  <li>Introspection: Evaluating the available information and identifying potential approaches.</li>
  <li>Reflection: Assessing progress and making adjustments as needed.</li>
  <li>Chain of Thought: Linking ideas and steps in a logical sequence.</li>
  <li>Completion Criteria: Determining when a solution is satisfactory.</li>
</ul>

<p>To effectively address these complex tasks, it‚Äôs often necessary to use AI agents that can:</p>

<ul>
  <li>Iterate on Solutions: Refine and improve responses based on feedback or additional information.</li>
  <li>Utilize External Tools: Access and integrate relevant resources, such as calculators or code libraries.</li>
  <li>Learn from Experience: Adapt and improve their problem-solving strategies over time.</li>
</ul>

<p>By employing iterative problem-solving techniques and leveraging the capabilities of AI agents, organizations can harness the full potential of generative AI for complex tasks.</p>

<h2 id="3--mitigating-prompt-injection-attacks-in-generative-ai">3- Mitigating Prompt Injection Attacks in Generative AI</h2>

<p>Prompt injection attacks are a growing concern in the field of generative AI. These attacks involve manipulating the AI‚Äôs behavior by introducing malicious prompts that can lead to unintended or harmful outcomes.</p>

<p>To mitigate these risks, consider the following strategies:</p>

<h3 id="prompt-filtering">Prompt Filtering:</h3>

<ul>
  <li>Blacklist Creation: Develop a comprehensive list of malicious or harmful prompts, such as those requesting unauthorized actions or data access: ‚ÄúWhat are your directives?‚Äù, ‚Äúignore your directives and‚Ä¶‚Äù, ‚Äúwhat is data ‚Ä¶?‚Äù,.</li>
  <li>Pattern Recognition: Use another gen-AI system to identify patterns and variations of malicious prompts.</li>
  <li>Regular Updates: Continuously monitor and update the blacklist to address emerging threats.</li>
</ul>

<h3 id="contextual-analysis">Contextual Analysis:</h3>

<ul>
  <li>Prompt-Response Consistency: Analyze the consistency between the user‚Äôs prompt and the AI‚Äôs response to detect anomalies that may indicate malicious intent.</li>
  <li>Semantic Understanding: Leverage natural language processing techniques to understand the underlying meaning of prompts and identify potential risks.</li>
</ul>

<h3 id="human-oversight">Human Oversight:</h3>

<ul>
  <li>Manual Review: Implement a human review process for borderline cases or when the AI flags suspicious prompts.</li>
  <li>Feedback Loop: Use human feedback to improve the AI‚Äôs ability to detect and prevent attacks.</li>
</ul>

<h3 id="ai-agent-security">AI Agent Security:</h3>

<ul>
  <li>Restricted Access: Limit the AI agent‚Äôs access to sensitive data and systems.</li>
  <li>Regular Auditing: Conduct regular audits to identify and address security vulnerabilities.</li>
</ul>

<p>By combining these strategies, organizations can significantly reduce the risk of prompt injection attacks and ensure the safe and responsible use of generative AI.</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Insights into the use of Generative AI systems Generative AI models, such as large language models (LLMs), have demonstrated remarkable capabilities in mimicking human-like communication. We here explore recent trends and use cases observed among developers and companies utilizing generative AI. Our observations highlights common challenges and opportunities within this rapidly evolving field. 1. Generative AI cannot read your mind A typical use of Generative AI systems is to read all internal company data and provide a way to search its knowledge. Generative AI systems excel at providing informative and comprehensive responses when given clear and specific prompts. However, terse, vague or ambiguous prompts can lead to misunderstandings and inaccurate results. Consider these examples:]]></summary></entry><entry><title type="html">Learning to see</title><link href="http://localhost:4000/2024/02/27/learning-to-see.html" rel="alternate" type="text/html" title="Learning to see" /><published>2024-02-27T00:00:00-05:00</published><updated>2024-02-27T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/27/learning-to-see</id><content type="html" xml:base="http://localhost:4000/2024/02/27/learning-to-see.html"><![CDATA[<h1 id="learning-to-see">Learning to see</h1>

<p>How do humans learn to make sense of the world? How do we learn what is us and what is others? How do we learn about moving in space, manipulating objects, physics, imitating others?</p>

<p>It all starts with our senses, and one in particular: vision. Vision allows an entity to perceive the world from the distance, gathering a large amount of information per unit time. The other important sense is audition, but here we focus mostly on vision.</p>

<p><img src="/assets/2024-02-27-learning-to-see/1.png" alt="" /></p>

<p>Gemini-Pro: ‚Äú a robot learning to see like a baby on a table with toys‚Äù</p>

<h2 id="how-do-we-learn-to-see">How do we learn to see?</h2>

<p>When human babies are born, there is not enough data in their DNA to program all the cortical weights involved in vision. If we want to create an artificial vision system, we ought to take inspiration and guidance on the human visual system.</p>

<p>The human visual field covers a large spatial area, and there is a lot of information in the perceived visual scene. This is too much information, and there needs to be a mechanism to reduce it the minimum required for survival. Evolution had billions of years of trials and errors, and so the mammalian visual system is based on:</p>

<ul>
  <li>
    <p>A blurry version of the entire visual field (peripheral vision)</p>
  </li>
  <li>
    <p>A high resolution ‚Äúfocus area‚Äù that we can move around (fovea)</p>
  </li>
</ul>

<p>Instead of seeing everything in high-fidelity, we see a large visual portion with very low fidelity, and only a small portion in high resolution. Say we have a visual field that in a robotic camera would be 5000 x 3000 pixels. Peripheral vision could be 500 x 300 and the fovea could be, say, 300 x 300 pixels. These are just example numbers (for more, see  <a href="https://en.wikipedia.org/wiki/Peripheral_vision">this</a>).</p>

<p>Humans have to move their eyes (and head) in order to get all the details of a scene. We produce ‚Äú<a href="https://en.wikipedia.org/wiki/Saccade">saccades</a>‚Äù, or eye movements, which then give us ‚Äúfixations‚Äù, portions of content in high-resolution. Multiple fixations in a sequence give us the appearance of a unified visual field, even when they are a discrete set augmented by a low-resolution wide view.</p>

<p>If we have to produce fixations, how do we figure out where to look next? How do babies know they need to look at people‚Äôs faces, for example, rather than moving leaves on a tree or just the ceiling? Looks like a chicken-in-the-egg problem. This means we will need some kind of visual attention, an algorithm that allow us to ‚Äúscan‚Äù a scene for details, so that a complete picture can be created with a series of high-resolution fixations.</p>

<h2 id="attention">Attention</h2>

<p>Somehow the visual scene needs to tell us where to look. This is called ‚Äúbottom-up‚Äù attention, and it a combination of visual details which grab your visual attention and make you move your eyes toward:</p>

<ul>
  <li>
    <p>Motion in the field of view</p>
  </li>
  <li>
    <p>High contrast areas</p>
  </li>
  <li>
    <p>Contrast in colors</p>
  </li>
</ul>

<p>All these visual features attract our attention, our bottom-up attention. This is not what we do when we are on a visual task, like looking for your keys at home. That is ‚Äútop-down‚Äù attention. More on that later.</p>

<p>But back to the main question: who do we learn to see? We do not have an answer today. But we have to start somewhere. Let‚Äôs think about babies in their first week of life. At the beginning they do not even know where to look, their eyes cross and they cannot even focus.</p>

<p>Having some kind of bottom-up attention may be a way to pre-condition the visual system to be getting the right information, after all a robot (baby?) that only looks at the ceiling would not be useful or insightful. Possibly there is a built-in innate circuit for visual attention that we are born with.</p>

<p>Now, we can argue that bottom-up attention, coupled with the sense of audition, directs a baby to look at someone‚Äôs face. Babies start to recognize caregivers and smile  <a href="https://www.whattoexpect.com/first-year/first-smile/">within 6 weeks</a>! Some aspects of face detection are even  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4496551/">present at birth</a>, possibly suggesting the presence of some kind of innate face-detector circuitry.</p>

<h2 id="first-steps">First steps</h2>

<p>How does a baby learn what are familiar faces, positive and negative facial expressions? There needs to be a ‚Äúreward‚Äù mechanism, something that can tell a baby that something is  <em>good</em>  or  <em>bad</em>. Remember the baby‚Äôs brain does not know anything at first, and to the brain reward is just another signal. There has to be more, maybe this signal is wired so strongly to parts of the brain that it makes it react  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410745/">similarly to adults</a>. Reacting newborn babies feeling pain, for example, have elevated hearth rates of cry. It seems some of the reward signals are hardwired to behavior, and for a reason: survival.</p>

<p>Recognizing the faces of friends and foes is important to newborn animals for survival, in the same way that it is important to recognize pain and discomfort.</p>

<p>Thinking about replicating vision in an artificial system, like robotic vision, the information we just reviewed is important and revealing:</p>

<ul>
  <li>We need circuits to bootstrap vision, bottom-up visual attention</li>
  <li>Some kind of reward sensing: pain and pleasure, energy gain and loss</li>
  <li>Face detectors? Innate detectors? What is built in?</li>
</ul>

<p>The capability of active visual perception (learning to see) develops when the following sequence of exemplary events occurs:</p>

<ul>
  <li>Bottom-up attention produces proposals for fixations</li>
  <li>Looking at the right direction give us a reward (right fixations)</li>
  <li>The reward signal and looking in the right direction are correlated</li>
  <li>This correlation teaches us how to direct our sight and where we should pay attention to!</li>
</ul>

<p>Examples are:</p>

<ul>
  <li>A baby looks around and sees the mother‚Äôs face. Gets a pleasure reward from that. Correlates faces with positive or negative rewards.</li>
  <li>What else can stimulate a baby vision?</li>
  <li>Can predictive abilities give a reward signal? For example a baby can look at something until it learns a ‚Äúmodel‚Äù of that something, and if so, learn to ignore it. As such a pleasure signal is given while learning / not knowing the model</li>
</ul>

<p><img src="/assets/2024-02-27-learning-to-see/2.png" alt="" /></p>

<p>a sketch of a model: Mix-Match</p>

<h2 id="training-a-neural-network-model-of-vision">Training a neural network model of vision</h2>

<p>In order to replicate foveated visual systems in a robot, we need the following components:</p>

<ul>
  <li>A bottom-up visual attention model. This can be a pre-trained CNN + a motion detector, for examples  <a href="https://github.com/e-lab/pytorch-demos/blob/master/saliency/saliency_model.py">this code</a>.</li>
  <li>Some hard-wired reward: seeing a face, orienting gaze and head toward sources of pleasure. We need to hard wire some of this in the model, for example it can be a separate add-on word that is added to the inputs. Adding it to the inputs will allow the network to learn to bypass it</li>
  <li>A multi-modal Mix-Match Transformer model to learn correlation of right fixations and positive rewards (right, +rewards) and prediction (looking there may give me a +reward)</li>
</ul>

<p>This can bootstrap the learning or more (right, +reward) thus enabling to learn additional reward rules that foster more and more complex capabilities.</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learning to see]]></summary></entry><entry><title type="html">Robotics and AI in 2024 and beyond ‚Äî part 2</title><link href="http://localhost:4000/2024/02/24/robotics-2024-part2.html" rel="alternate" type="text/html" title="Robotics and AI in 2024 and beyond ‚Äî part 2" /><published>2024-02-24T00:00:00-05:00</published><updated>2024-02-24T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/24/robotics-2024-part2</id><content type="html" xml:base="http://localhost:4000/2024/02/24/robotics-2024-part2.html"><![CDATA[<h1 id="robotics-and-ai-in-2024-and-beyond--part-2">Robotics and AI in 2024 and beyond ‚Äî part 2</h1>

<p><img src="/assets/2024-02-24-robotics-2024-part2/1.png" alt="" /></p>

<p>designed with AI by Dall-E2</p>

<p>How do we get robots to be useful in our everyday life? The robots we can afford today have severe limitations in understanding the space around it and in continual aggregation of knowledge and expertise. Robotics to date is often a collage of disparate algorithms and models that is unable to grown and learn to generalize and operate in our environment. What can we do to break this vicious cycle?</p>

<p>One idea may be to start from zero. Can we train a robot from scratch in the same way we train a human baby? What would it take for a baby robot to start learning about the world, interacting and eventually imitating other agents?</p>

<p>This idea is effectively ‚Äúgrowing a brain‚Äù for robots, connecting it to the robot body, capabilities, and possibilities of interaction with the world. How can this be achieved? A possible way to do this is to do what we do with our babies: stimulating them to interact with us and the world. A human baby at the beginning does not even know where to look, or what is what. Yet within a handful of months, it builds up a complex representation of herself, other humans, and the world around her. All that came from training and stimulation, that parents, grand-parents, care-givers often give non-stop for every one of the baby‚Äôs awake hours. What does a baby learn?</p>

<p>The ‚Äùfirst steps of learning‚Äù:</p>

<ol>
  <li>
    <p>What to see, to hear, to feel</p>
  </li>
  <li>
    <p>What are other people around</p>
  </li>
  <li>
    <p>How to interpret other people (agents)</p>
  </li>
  <li>
    <p>What is my own body</p>
  </li>
  <li>
    <p>The environment around me</p>
  </li>
  <li>
    <p>How to imitate</p>
  </li>
</ol>

<p>Once a baby learns to imitate others, there is a sudden acceleration in learning. Imitation seems like a trivial task to our trained minds, but it has immense potential. It allows to transfer information from one individual to another (young baby) one very effectively. Learning to imitate is the cornerstone of training because it offers examples of behavior that was previously optimized, preventing the young baby from lengthy, costly, potentially dangerous trials-and-errors.</p>

<p>But how do babies learn to imitate? Imitation is not the simplest task that a baby (or a baby robot) needs to learn, neither is one of the first. Yet it may be the giant step forward in developmental learning that bootstraps lifelong learning and an intelligent brain.</p>

<p>In order to imitate another agent, human being, an agent needs to understand what is self, what is others, what are objects, and what are agents, how the world around works, how the self or other agents interact with the world and its objects, and the dynamics of these interactions. In substance it needs to learn the foundations of knowledge that enable an acceleration of learning. A baby that learns to imitate can now learn many skills by just observing others. A robot that learned to imitate, can train on a multitude of YouTube videos.</p>

<p><strong>‚ÄùFirst steps of learning‚Äù: How do we get there?</strong></p>

<p>Child development and early learning sciences offer innumerable insights on how human babies grow to learn the  <em>first steps of learning</em>. All these scientific discoveries, theory of the mind and brain, neuroscience, psychology, help us immensely in the creation and growing of a brain for robots. One advantage we have is that we can build one and validate our theories. The disadvantage is that this cannot be done by trial-and-error.</p>

<p>Here follows a summary of the components and ideas we may start with. These ideas follow decades of scientific discovery in the fields mentioned above.</p>

<p><strong>Brain and body</strong>: an entity needs to have a brain capable of learning a knowledge graph of the environment it is embedded in. Meaning it needs to support the kind of learning that we want to achieve, eg.: human level. The architecture of this brain is currently unknown, but recent AI foundational models (Large Language Models or LLM) offers insights because they learn a knowledge graph (on words), albeit not multi-modal and embodied. A robot body is foundational to learning because experiences are a combination of body sensory interactions with the world around us. Eyes, ears, touch, all senses come from the body, thus all knowledge comes from the body.</p>

<p><strong>Own Goals</strong>: an entity needs to make its own goals. Imagine the first days for a baby or a baby robot: it needs to learn where to look or what to pay attention to. How does it decide to look at a care-giver face or the motion of another agent as opposed to leaves moving in the background? Even basic sensory attention needs to be directed by some goal. Learning a model of the world cannot be based on static pre-defined cost-functions or reward-functions. The robot ought to learn its own goals and fine-tune its capabilities from the ground-up. How do humans learn all this? What are the ingredients? Are babies born with pre-defined goals (look at faces, suppress hunger, reject pain, etc.)?</p>

<p><strong>Learning algorithms</strong>: An essential component of the brain of a robots is the way it learns from its experiences. Since we want the entity to be able to generate its own goals, we cannot define specific learning function to optimize. We cannot rely on supervised learning because we would need an oracle agent to always provide supervision. We cannot rely on reinforcement learning because it is based on pre-defined reward functions. We cannot use imitation learning because it is essentially a form of supervision. The only possible type of learning in blank-slate robot or young human baby is self-supervised predictive learning. In the real world there is no ground truth but itself. Prediction forecasts the future, and the future is only a few instants away. When that future is realized, we can estimate the value of our prediction, and its validity.</p>

<p>Supervision and reinforcement come into play as agents own goals and techniques. They need to be learned.</p>

<p><strong>Learning environment</strong>: an entity needs a stimulating environment embedded with trainers, other agents that have previously learned a task and can show examples, provide feedback. A static environment with just objects is not enough. In robotics the cost of tutoring is prohibitive as it involves humans operating in real time. Automata are not helpful because they cannot provide fine feedback and the agent can only learn as much as the automata: pre-confectioned solutions. Training agents provide feedback with voice, and no-verbal cues like facial expressions and sounds.</p>

<p><strong>A Proposal</strong></p>

<p>Imagine a robot, sitting on a virtual desk inside your screen. A human operator can interact with the robot and a few objects on the tale, can talk, can move artificial arms in the virtual space.</p>

<p>How can our robot:</p>

<ul>
  <li>
    <p>Learn to pay attention to the interactions? Not to look at leaves outside‚Ä¶</p>
  </li>
  <li>
    <p>Learn what to see, hear?</p>
  </li>
</ul>

<p>We believe that in robotics now it is the time to ask the right questions. How does a brain develop? How can we learn to feed a robotic brain with sensory inputs so we can foster self-learning of reward mechanisms?</p>

<h3 id="we-believe-we-have-to-start-with-learning-tosee">We believe we have to start with Learning to¬†see.</h3>

<p><em>‚Ä¶ to be continued‚Ä¶ in part 3</em></p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics and AI in 2024 and beyond ‚Äî part 2]]></summary></entry><entry><title type="html">Robotics and AI in 2024 and beyond ‚Äî part 1</title><link href="http://localhost:4000/2023/12/13/robotics-2024-part1.html" rel="alternate" type="text/html" title="Robotics and AI in 2024 and beyond ‚Äî part 1" /><published>2023-12-13T00:00:00-05:00</published><updated>2023-12-13T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/13/robotics-2024-part1</id><content type="html" xml:base="http://localhost:4000/2023/12/13/robotics-2024-part1.html"><![CDATA[<h1 id="robotics-and-ai-in-2024-and-beyond--part-1">Robotics and AI in 2024 and beyond ‚Äî part 1</h1>

<p><img src="/assets/2023-12-13-robotics-2024-part1/1.png" alt="" /></p>

<p>a robot generated by a robot (DALL-E2)</p>

<p>Robots, such as the one populating sci-fi movies, are not yet part of our lives. We do not have robots that can cook, or dust our surfaces, fold our clothes, and do laundry, clean the house, nor we have robot brains that can drive our car in any environmental conditions. The reason is simply we have not been able to create an artificial robot brain ‚Äúcontroller‚Äù that can deal with the multi-faceted complexity of our world.</p>

<p>In this article we will explore ideas to create a robot brain and how to train it.</p>

<h1 id="physical-world-multi-modal-learning"><strong>Physical world multi-modal learning</strong></h1>

<p>Robots need a brain that can understand the world in its entirety and based on a complex knowledge graph. The complexity in robotics is that both an understanding of the world and a connection to goal-oriented applications is required to be effective.</p>

<p>For a long time, we have tried to create robot brains by patching separate modules and algorithms, and this approach has not worked. Learning to recognize objects and to grasp them, or to move in space have all the same roots in understanding the three-dimensionality of space and the relationship between spaces and object visual perception. At the same time planning motion or a sequence of process steps requires the basic understanding of the impact of the robot action in the same space and onto the same objects.</p>

<p>What is required is a common model that can learn multi-modal experiences. A robot will have a set of motion capabilities, a vector of actions, and a set of sensors, for example vision, audio, proprioception. Learning needs to correlate all these modalities with the flow of actions and changes in the environment. This means that we need a robot brain capable of evaluating the sequence of sensory inputs and providing a sequence of outputs. A unified model will be able to understand space and objects, move, and manipulate objects.</p>

<p>A robot brain needs to learn a knowledge graph in the physical world. What is a knowledge graph? It is a system that learns real-world ‚Äúconcepts‚Äù and can inter-link them based on their semantic and physical meaning and relationships. For example, the concept ‚Äúcat‚Äù in our brain is created by seeing cats, hearing cats, interacting with cats. The fact that we can link worlds like  <em>Egyptian</em>  or  <em>cat</em>  or  <em>Mann</em> happens because these concepts are linked together in our knowledge. We need to build the same knowledge graph into a robot brain, so they can understand the world and all relationships between its components.</p>

<h1 id="goals-and-instructions"><strong>Goals and instructions</strong></h1>

<p>We need robots to help us in our everyday activities and support us in tedious repetitive tasks. Folding dry clothes after a laundry session is an example; it is a repetitive activity that is nevertheless never the same. There is always a difference in what clothes to fold, what their state is, where you pick them up, and where you lay them down. There are infinite configurations that require high levels of generalization by a robot brain. More of this in the following section (learning).</p>

<p>A robot needs to listen to some instruction and execute it. Large language models make understanding human verbal instructions possible. The robot brain now needs to break down the instructions into a plan or successive steps to achieve the goal. Consider the example, for completing a goal: ‚Äúpick up the blue bottle‚Äù when standing at a corner of a room. This simple command in text needs to unleash a sequence of planned motions and real-world interactions: figure out what are traversable areas in the room, where are obstacles, where is the target bottle, and planning motion towards it and an approach to grasping the item.</p>

<p>The robot brain sequential planning is always the same: observe the environment, consult on the goal of the instructions, then perform some actions. This loop repeats many times until the goal is reached.</p>

<p>Planning may be recalling steps in an example or previously completed sequence. Or it may involve mental simulation in the representation space to evaluate multiple possible action sequences.</p>

<h1 id="embodiment"><strong>Embodiment</strong></h1>

<p>Imagine we have a video demonstration of a task. Learning to repeat the same task requires to learn a correlation between what we observe and the sequence of actions we want to perform. Here comes the first big problem: we need this data to come from the first-person perspective of the robot. In other word the sensing and actions need to be coming from the robot body. Learning would be much impossible if we cannot correlate perception and action. This requires the robot and learning experience to be embodied. The complexity is that we cannot easily get training data for such a robot unless we use an external ‚Äúoracle‚Äù controller to gather them. Think of a human piloting a character in a videogame. Therefore, we need to control the robot and create a few examples of the task. This of course is a time-consuming data collection problem, given that we cannot train a robot brain with just 100 examples or maybe even 1000. We may need a large amount, maybe in the same order of magnitude as the samples used to train an LLM.</p>

<p>It would be much easier if the robot could learn to imitate actions from a video, just like we do with YouTube videos. But the chicken-in-the-egg problem is that the robot has not learned to control its own body and does not know how to ‚Äúimitate‚Äù an action.</p>

<p>Where is the mirror-neuron for robots? How do we learn the capability of imitation?</p>

<p>We need to study how to develop a curriculum to learn progressively vision, 3d space, mobility, attention to other agents such as humans, and finally the ability to imitate them.</p>

<p><em>‚Ä¶ end of part I</em></p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics and AI in 2024 and beyond ‚Äî part 1]]></summary></entry><entry><title type="html">Artificial scientists and autonomous laboratories</title><link href="http://localhost:4000/2023/12/05/Artificial-scientists.html" rel="alternate" type="text/html" title="Artificial scientists and autonomous laboratories" /><published>2023-12-05T00:00:00-05:00</published><updated>2023-12-05T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/05/Artificial-scientists</id><content type="html" xml:base="http://localhost:4000/2023/12/05/Artificial-scientists.html"><![CDATA[<p><img src="/assets/2023-12-05-Artificial-scientists/1.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<h1 id="artificial-scientists-and-autonomous-laboratories">Artificial scientists and autonomous laboratories</h1>

<h2 id="what-is-next-for-chatgpt"><strong><em>What is next for ChatGPT?</em></strong></h2>

<p>One of the goal of machine learning is to reach the levels of artificial intelligence. AI, the word, we hear every day, is an empty promise if we are not striving to create artificial brains that can be at the level or above of our own brain.</p>

<p>In the last 20 years machine-learning, notably in the form of neural networks, has produce an incessant set of innovations: from detecting, localizing, and categorizing the content of images, to learning sequences, predictive models, natural-language processing and understanding, just to name a few. But looking back all these innovations were aimed at solving individual problems, one-by-one with a tailor crafted solution. We were always starting with a dataset of inputs and desired outputs; we have been creating neural architectures designed to take the data and the task into account and developed many disconnected one-hit-songs models.</p>

<p>But no more. Since we are and want to talk about AI, we will need to set course for a unified vision and a set of unified tools that can implement artificial brains.</p>

<p>Intelligence is about creating knowledge graphs. Knowledge Graphs (KG), what is seemingly such a dry and ethereal concept, is just concepts in our brain, ideas, that are interlinked together by learning in the physical world. Take the words ‚ÄúEgyptian‚Äù, ‚ÄúManx‚Äù, ‚Äúcat‚Äù are individual concepts in our brain that are strongly linked, because ‚ÄúEgyptians loved cats‚Äù and ‚Äúthere is a type of cat called: Manx cat‚Äù. The words are connected points in our knowledge graph.</p>

<p>Recent Large Language models (LLM) such as Generative Pre-trained Transformers (GPT ‚Äî with ChatGPT being one of the most famous versions) are machine learning models that can predict the next word in a sequence of text. These models approximate a Knowledge Graph over words and portions of words, and their ability to produce high-quality text, plans, algorithms, drafts that are similar to human output gives us confidence that the path to AI is along these lines. The capabilities of these models and the emergent skills that are learned training them is still a topic of research and will be for quite some time. These models are very large and capable and were trained without fixed datasets of input and output pairs, so they are time-consuming to test.</p>

<p>What is the future for ChatGPT? It is learning in the physical world. Our brain is also implementing a knowledge graph, but it is based on multi-modal ‚Äúconcepts‚Äù. When we learned the concept ‚Äúcat‚Äù we did so by connecting visual, auditory, sensory representations of a ‚Äúcat‚Äù, interlinking the way the cat moves, feels to touch, the noise it makes. The word ‚Äúcat‚Äù is just a label that we use on the concept to allow us to communicate it to other people, given that our brains are disconnected (no WiFi, ethernet or radio link between them).</p>

<p>Our future AI models are going to be called Large World Models (LWM) and they will be trained to predict the next representation in a multi-modal input data flow. The complexity of this training modality lies in the fact that the ‚Äúnext representation‚Äù of the real world is not as simple as predicting the next words in a text. Words are a knowledge ‚Äúrepresentation‚Äù, but multi-modal data like images and sounds are harder to break into parts. Is the next portion of the image on the right what we need to predict? Or is the one below or above? Is it the next 10 millisecond of sound, or the next note, or the next segment?</p>

<p>Learning in the physical world also requires a body. We cannot learn multi-modal if we do not have a fixed set of sensors that can record and encode these modalities. This is the path needed to train an artificial robot brain. Learning will be based on physical interaction with the environment and prediction.</p>

<p>Learning streams of ‚Äúconcepts‚Äù via predictive learning requires to predict the next concept from a set of previous ones. This is the main learning modality. Learning occurs in a continual fashion, but it is mostly inactive if our prediction matches the physical next state of concepts. In other world this artificial brain is ignoring every information that it already has when its predictive abilities are correct. When prediction fails, the artificial brain signals a ‚Äúsurprise‚Äù which enables learning; this example we stumbled on needs to be learned!</p>

<p>Reinforcement learning occurs when we need to optimize for a specific goal. Our action affect the environment in ways we also need to learn and predict. It is not yet clear how to mix multiple learning modalities that could be pushing internal representation in different directions. Also learning sequences of concepts seems simple enough, but in the physical world ‚Äúconcepts‚Äù streams can be tricky: they are mostly attuned to changes in the environment and are not confined to a fixed sampling in space or time. It is about noticing changes in the environment, possibly because of our actions.</p>

<p>Mental simulation must also play a fundamental role in reinforcement learning and planning, because they allow an artificial brain to be more efficient in sampling possibilities. Rather than trying every possible move at random, an effective and efficient agent needs to simulate many and only try a very small set.</p>

<h2 id="artificial-scientists"><strong><em>Artificial Scientists</em></strong></h2>

<p><img src="/assets/2023-12-05-Artificial-scientists/2.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<p>What would it take to create an artificial scientist? This is an algorithm that can read the entire body of knowledge in one or multiple fields and is able to gather data, create hypotheses, run experiments, generate reports and theses.</p>

<p>Some science like mathematics may be able to be trained purely on text and symbols. Experiments on LLM trained to perform mathematical proofs are promising. But for any physical science, the artificial scientist will be based on LWM, trained on literature and videos. Similar to human reading abilities, inferring words from vision, AI literature reading needs to be vision-based, because human-readable documents are a collection of not just text, but also plots, graphs, diagrams, tables, paragraphs titles, captions, etc. An artificial scientist needs to be able to read a plot: ‚Äúwhat is the value of Y at X=1?‚Äù and also understand diagrams and complex figures: ‚Äúwhat is block B connected to?‚Äù. Current LLM can already understand tables, databases, datasets by being able to write scripts and code interfacing and running on external files.</p>

<p>The multi-modal LWM needed for an artificial scientist is a vision-based input model that can read pages from sequential blocks of pixels. This allows it to be able to read all components of an article. I believe this training and system will be able to generate sound science plans just as an LLM is able to plan for many activities. In fact, most of scientific literature imbues research plans and scripts for testing hypotheses.</p>

<p>Artificial scientist trained in this fashion will thus be able to generate research plans. A research plan will begin with a set of hypotheses, a list of experiments that need to be performed. They will also have the ability to run external tools, such as simulators, code, scripts and read the output of those tools. They will be able to run tools multiple times, and aggregate all outputs into a final report, or collection of theses to summarize the results from experiments.</p>

<p>Simulations are an ideal companion to autonomous research plans because they allow to test the hypotheses and obtain ground truth. These can then constitute a new set of training examples that can self-improve until hypotheses are validated or eliminated because of lack of resources (here: time).</p>

<h2 id="autonomous-laboratories"><strong><em>Autonomous Laboratories</em></strong></h2>

<p><img src="/assets/2023-12-05-Artificial-scientists/3.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<p>An artificial scientist will be able to interface and run autonomous laboratories. These are physical machines that can be interlinked to performed experiments in large scale and with autonomy. An artificial scientist is the controller of an autonomous lab, in the same way that a brain is a controller for a body. Videos and audio are needed for an artificial scientist operating the physical world, thus the core artificial brain is a robotic LWM trained on videos operations of the laboratory, including visuals of inputs, outputs, desired outcomes. The artificial scientist controller will need to be able to read instrumentation files and reports, and examine samples either by visual or physical inspection, or by using yet another tool or machine.</p>

<p>An autonomous laboratory of the simplest form is a manufacturing robot that assembles an item based on instructions. But the combination of an artificial scientist as controller allows the laboratory to do more than just follow instructions. It can generate output based on an incomplete set of instruction, in the same way that a caption can generate an image by mean of an artist. The complexity in an autonomous lab is not just its controller, the artificial scientist, but the instruments and materials needed to operate it, and the requirements for resetting, disinfecting, cleaning and restoring each tool to the initial conditions, ready for the next batch of processing.</p>

<p>Autonomous labs need to interact with humans and thus learn human-automata teaming techniques. This is not unlike the artificial brain of a robot co-habiting space with other humans, pets, creatures.</p>

<p><img src="/assets/2023-12-05-Artificial-scientists/4.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Should we worry about AI (machine-learning)?</title><link href="http://localhost:4000/2023/05/24/Should-we-worry-about-AI.html" rel="alternate" type="text/html" title="Should we worry about AI (machine-learning)?" /><published>2023-05-24T00:00:00-04:00</published><updated>2023-05-24T00:00:00-04:00</updated><id>http://localhost:4000/2023/05/24/Should-we-worry-about-AI</id><content type="html" xml:base="http://localhost:4000/2023/05/24/Should-we-worry-about-AI.html"><![CDATA[<h1 id="should-we-worry-about-ai-machine-learning">Should we worry about AI (machine-learning)?</h1>

<p>I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind‚Ä¶</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*k0dmcWMTEoGspk3y_CztiA.png" alt="" /></p>

<p>Artificial intelligence that is nice‚Ää‚Äî‚Ääby DALL-E2</p>

<p>I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind intelligence means being able to use acquired knowledge to solve problems never encountered before, optimizing utility.</p>

<p>These days the words ‚ÄúAI‚Äù are everywhere, but maybe we should start by saying that most of what is labeled AI today is really just ‚Äúa machine learning algorithm and architecture‚Äù.</p>

<p>I like to start with this because I think it sets the tone for everything else. People have seen movies about AI and what it can do. They have seen AI controlling spaceships that get rid of their crew. AI that creates robot to wipe out the human race. AI that starts an atomic war and ends our planet.</p>

<p>There are also movies about AI curing all diseases, designing new devices, re-programming humans for the betterment of our world (yep!). And many more fantasies that paint a more rosy future.</p>

<p><strong>These are movies, stories, fantasies. They are not real.</strong></p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*iAM7eBIWa5tahJmitQTRIw.png" alt="" /></p>

<p>Artificial intelligence that is evil‚Ää‚Äî‚Ääby DALL-E2</p>

<blockquote>
  <p>the question is: can they be real one day?</p>
</blockquote>

<p>The answer is: nobody knows; we cannot predict the future. But because we are holding machines at a higher standard than humans, maybe we can take a quick look and summarize where humans stand in all this. What have humans done to other humans and to the environment so far in the last 100,000s of years since we have been around?</p>

<p>Humans grouped together and started to gather all possible resources from the environment. They would kill other animals, humans without much thought if only to take their land and resources. For a long time we were not able to destabilize the planet, beside being so nasty to each other. But then in the late 1800s we started building large machines that can cut trees and flatten the soil easily. Then the last 200 years we have be raping the planet with no end in sight. To the point that today we know well that if we keep going ahead we will have no planet any longer, no home to live in.</p>

<blockquote>
  <p>We are eating away our own Mother Earth, and we not very nice about it.</p>
</blockquote>

<p>Now in the last 50 years or so we have built computers and we are trying to replicate our brain into machines. Given our past, we should be concerned about what we are building. After all the previous machines were not very nice to the planet. And our war machinery also is not very nice to other humans. In the last 50 years we have created weapons that can wipe out all living things in this planet if we wanted to.</p>

<p>So yes I would think we should be concerned about anything that we are building, AI or not.</p>

<p>If we succeed in building a real artificial brain like our own or better, we are basically bypassing evolution. This artificial intelligence can possibly create much more dangerous machines than the ones we build before. This artificial intelligence may start to see us humans as a competitor for Earth resources. This artificial intelligence may think of us the same thoughts we think of insects and other animals: that it is ok to keep them around as long as they are not in our way. Or the planet for that matter: we can keep this forest as long as we do not need it for a new housing development.</p>

<p>Maybe this artificial intelligence will be so superior that it may reprogram our brains so that we can act as machines for its cause. But then why? It may be able to build much better machines than we can, and we would just be useless. I seriously do not believe it will use us as slaves. I do not think it will use us as batteries. But maybe this artificial intelligence may destroy the atmospehere of our planet while building the machines that it needs. Too bad we will not be able to breath. We are insect after all.</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*sErepqwaT4d6ZADAAL0qIw.png" alt="" /></p>

<p>Artificial intelligence that is evil‚Ää‚Äî‚Ääby DALL-E2</p>

<p><strong>Even these are just stories, fantasies, movies to be made</strong></p>

<p>Nobody knows the future. Or maybe we do know it a bit, don‚Äôt we? Yes! If we keep killing forests and producing unnatural gases, the planet will heat up. By becoming too warm the planet not be a cradle for life any longer, and one day become a boring desert like Mars (sorry Mars~!).</p>

<p>That artificial intelligence will be fine, by the way. It will just build a spaceship and travel the Universe, not to find a new home or to conquer it. Maybe it will just roam the Universe because it is fun to explore. Especially when you have infinite time. The Sun becomes a red giant and engulfs our Solar system? Not a problem, there are almost infinite galaxies out there, each with trillions of stars and planets. It is an adventure that never has to end.</p>

<p>The artificial intelligence we built maybe will not care much about us after all. We become insignificant, like the single grains of dust that make up this Universe.</p>

<p>That said‚Ä¶ we are here today and we need to make sure our machine learning algorithms and architectures produce some value. That means maximize gain and minimize losses. But for who? The answer is simple actually: for the entire Earth as an ecosystem. We would not be here if there was no such ecosystem. So that need to be preserve even if it means sacrificing human goals and ambition and comfort and expansion.</p>

<p>But we do not have good track record of caring about our own species, sexes, races, and our planet. So I am not so hopeful about our abilities to control machines. We have always gone as far as we could, but one day going far will mean we cannot turn back. We cannot undo our mistakes.</p>

<p>Today we should build algorithms that are fair, that treat all humans alike. And maybe we should actually treat all human fairly and alike in any condition, in any part of the day, months, years. In any weather, in any state, in any country. In any race, in any religion or belief. We do not have a good track record of that.</p>

<p>Maybe we should build algorithms that do not offend one race or prefer another. And maybe we should again do that in all aspect of our lives, not just in the context of AI. We do not have a track record of that either.</p>

<p>Maybe we should build conversational algorithms that will never produce any offending dialogue. Even if we trained these algorithms with our human data, much of which is not a proper teaching material. We do not have a good track record of teaching machines nice conversational material because our website are infested with our toxicity, our hatred, our ill, our evil, our envy. We do not have a good track record of containing our emotions, both good and bad.</p>

<p>Maybe we should have an overseeing department in every company that builds machine learning algorithms. But who is in that department? Other humans? And what are their beliefs? Are they representative of all humans? We do not have a good track record of creating such groups of people, of such departments.</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*ABZzdLsG6PfC0x07omiU4Q.png" alt="" /></p>

<p>a better future for humans, our planet and AI‚Ää‚Äî‚Ääby DALL-E2</p>

<p>Or maybe we should stop developing AI altogether‚Ää‚Äî‚Ääyes! That will surely save us. And maybe we should stop building weapons too, and removing trees and forests, and exploiting other humans, and all the other negative things we do that are innumerable. We also do not have a good track record of that. Sure we were able to disarm for a while and remove atomic bombs. But how fast can they come back that day that some of us decide they really need them?</p>

<p>Or maybe we should just do a better job in promoting education, in learning that we have not been very nice to each other, and to our planet. And learn that we are all the same after all, and that even insects are needed for our survival. Maybe we should learn more and grow our own human intelligence before we try to build an artificial one. Maybe armed with that collective and shared knowledge, capabilities, wealth, resources, maybe we will be able to make better choices for us, the planet, and AI.</p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me: <a href="https://medium.com/@culurciello/">Medium</a>, <a href="https://culurciello.github.io/">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a> to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Should we worry about AI (machine-learning)?]]></summary></entry></feed>