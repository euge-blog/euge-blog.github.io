<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2024-02-27T13:46:43-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eugenio Culurciello Blog</title><subtitle>A minimal, responsive, and powerful Jekyll theme for presenting professional writing.</subtitle><entry><title type="html">Learning to see</title><link href="http://localhost:4000/2024/02/27/learning-to-see.html" rel="alternate" type="text/html" title="Learning to see" /><published>2024-02-27T00:00:00-05:00</published><updated>2024-02-27T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/27/learning-to-see</id><content type="html" xml:base="http://localhost:4000/2024/02/27/learning-to-see.html"><![CDATA[<h1 id="learning-to-see">Learning to see</h1>

<p>How do humans learn to make sense of the world? How do we learn what is us and what is others? How do we learn about moving in space, manipulating objects, physics, imitating others?</p>

<p>It all starts with our senses, and one in particular: vision. Vision allows an entity to perceive the world from the distance, gathering a large amount of information per unit time. The other important sense is audition, but here we focus mostly on vision.</p>

<p><img src="/assets/2024-02-27-learning-to-see/1.png" alt="" /></p>

<p>Gemini-Pro: “ a robot learning to see like a baby on a table with toys”</p>

<h2 id="how-do-we-learn-to-see">How do we learn to see?</h2>

<p>When human babies are born, there is not enough data in their DNA to program all the cortical weights involved in vision. If we want to create an artificial vision system, we ought to take inspiration and guidance on the human visual system.</p>

<p>The human visual field covers a large spatial area, and there is a lot of information in the perceived visual scene. This is too much information, and there needs to be a mechanism to reduce it the minimum required for survival. Evolution had billions of years of trials and errors, and so the mammalian visual system is based on:</p>

<ul>
  <li>
    <p>A blurry version of the entire visual field (peripheral vision)</p>
  </li>
  <li>
    <p>A high resolution “focus area” that we can move around (fovea)</p>
  </li>
</ul>

<p>Instead of seeing everything in high-fidelity, we see a large visual portion with very low fidelity, and only a small portion in high resolution. Say we have a visual field that in a robotic camera would be 5000 x 3000 pixels. Peripheral vision could be 500 x 300 and the fovea could be, say, 300 x 300 pixels. These are just example numbers (for more, see  <a href="https://en.wikipedia.org/wiki/Peripheral_vision">this</a>).</p>

<p>Humans have to move their eyes (and head) in order to get all the details of a scene. We produce “<a href="https://en.wikipedia.org/wiki/Saccade">saccades</a>”, or eye movements, which then give us “fixations”, portions of content in high-resolution. Multiple fixations in a sequence give us the appearance of a unified visual field, even when they are a discrete set augmented by a low-resolution wide view.</p>

<p>If we have to produce fixations, how do we figure out where to look next? How do babies know they need to look at people’s faces, for example, rather than moving leaves on a tree or just the ceiling? Looks like a chicken-in-the-egg problem. This means we will need some kind of visual attention, an algorithm that allow us to “scan” a scene for details, so that a complete picture can be created with a series of high-resolution fixations.</p>

<h2 id="attention">Attention</h2>

<p>Somehow the visual scene needs to tell us where to look. This is called “bottom-up” attention, and it a combination of visual details which grab your visual attention and make you move your eyes toward:</p>

<ul>
  <li>
    <p>Motion in the field of view</p>
  </li>
  <li>
    <p>High contrast areas</p>
  </li>
  <li>
    <p>Contrast in colors</p>
  </li>
</ul>

<p>All these visual features attract our attention, our bottom-up attention. This is not what we do when we are on a visual task, like looking for your keys at home. That is “top-down” attention. More on that later.</p>

<p>But back to the main question: who do we learn to see? We do not have an answer today. But we have to start somewhere. Let’s think about babies in their first week of life. At the beginning they do not even know where to look, their eyes cross and they cannot even focus.</p>

<p>Having some kind of bottom-up attention may be a way to pre-condition the visual system to be getting the right information, after all a robot (baby?) that only looks at the ceiling would not be useful or insightful. Possibly there is a built-in innate circuit for visual attention that we are born with.</p>

<p>Now, we can argue that bottom-up attention, coupled with the sense of audition, directs a baby to look at someone’s face. Babies start to recognize caregivers and smile  <a href="https://www.whattoexpect.com/first-year/first-smile/">within 6 weeks</a>! Some aspects of face detection are even  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4496551/">present at birth</a>, possibly suggesting the presence of some kind of innate face-detector circuitry.</p>

<h2 id="first-steps">First steps</h2>

<p>How does a baby learn what are familiar faces, positive and negative facial expressions? There needs to be a “reward” mechanism, something that can tell a baby that something is  <em>good</em>  or  <em>bad</em>. Remember the baby’s brain does not know anything at first, and to the brain reward is just another signal. There has to be more, maybe this signal is wired so strongly to parts of the brain that it makes it react  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410745/">similarly to adults</a>. Reacting newborn babies feeling pain, for example, have elevated hearth rates of cry. It seems some of the reward signals are hardwired to behavior, and for a reason: survival.</p>

<p>Recognizing the faces of friends and foes is important to newborn animals for survival, in the same way that it is important to recognize pain and discomfort.</p>

<p>Thinking about replicating vision in an artificial system, like robotic vision, the information we just reviewed is important and revealing:</p>

<ul>
  <li>We need circuits to bootstrap vision, bottom-up visual attention</li>
  <li>Some kind of reward sensing: pain and pleasure, energy gain and loss</li>
  <li>Face detectors? Innate detectors? What is built in?</li>
</ul>

<p>The capability of active visual perception (learning to see) develops when the following sequence of exemplary events occurs:</p>

<ul>
  <li>Bottom-up attention produces proposals for fixations</li>
  <li>Looking at the right direction give us a reward (right fixations)</li>
  <li>The reward signal and looking in the right direction are correlated</li>
  <li>This correlation teaches us how to direct our sight and where we should pay attention to!</li>
</ul>

<p>Examples are:</p>

<ul>
  <li>A baby looks around and sees the mother’s face. Gets a pleasure reward from that. Correlates faces with positive or negative rewards.</li>
  <li>What else can stimulate a baby vision?</li>
  <li>Can predictive abilities give a reward signal? For example a baby can look at something until it learns a “model” of that something, and if so, learn to ignore it. As such a pleasure signal is given while learning / not knowing the model</li>
</ul>

<p><img src="/assets/2024-02-27-learning-to-see/2.png" alt="" /></p>

<p>a sketch of a model: Mix-Match</p>

<h2 id="training-a-neural-network-model-of-vision">Training a neural network model of vision</h2>

<p>In order to replicate foveated visual systems in a robot, we need the following components:</p>

<ul>
  <li>A bottom-up visual attention model. This can be a pre-trained CNN + a motion detector, for examples  <a href="https://github.com/e-lab/pytorch-demos/blob/master/saliency/saliency_model.py">this code</a>.</li>
  <li>Some hard-wired reward: seeing a face, orienting gaze and head toward sources of pleasure. We need to hard wire some of this in the model, for example it can be a separate add-on word that is added to the inputs. Adding it to the inputs will allow the network to learn to bypass it</li>
  <li>A multi-modal Mix-Match Transformer model to learn correlation of right fixations and positive rewards (right, +rewards) and prediction (looking there may give me a +reward)</li>
</ul>

<p>This can bootstrap the learning or more (right, +reward) thus enabling to learn additional reward rules that foster more and more complex capabilities.</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learning to see]]></summary></entry><entry><title type="html">Robotics and AI in 2024 and beyond — part 2</title><link href="http://localhost:4000/2024/02/24/robotics-2024-part2.html" rel="alternate" type="text/html" title="Robotics and AI in 2024 and beyond — part 2" /><published>2024-02-24T00:00:00-05:00</published><updated>2024-02-24T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/24/robotics-2024-part2</id><content type="html" xml:base="http://localhost:4000/2024/02/24/robotics-2024-part2.html"><![CDATA[<h1 id="robotics-and-ai-in-2024-and-beyond--part-2">Robotics and AI in 2024 and beyond — part 2</h1>

<p><img src="/assets/2024-02-24-robotics-2024-part2/1.png" alt="" /></p>

<p>designed with AI by Dall-E2</p>

<p>How do we get robots to be useful in our everyday life? The robots we can afford today have severe limitations in understanding the space around it and in continual aggregation of knowledge and expertise. Robotics to date is often a collage of disparate algorithms and models that is unable to grown and learn to generalize and operate in our environment. What can we do to break this vicious cycle?</p>

<p>One idea may be to start from zero. Can we train a robot from scratch in the same way we train a human baby? What would it take for a baby robot to start learning about the world, interacting and eventually imitating other agents?</p>

<p>This idea is effectively “growing a brain” for robots, connecting it to the robot body, capabilities, and possibilities of interaction with the world. How can this be achieved? A possible way to do this is to do what we do with our babies: stimulating them to interact with us and the world. A human baby at the beginning does not even know where to look, or what is what. Yet within a handful of months, it builds up a complex representation of herself, other humans, and the world around her. All that came from training and stimulation, that parents, grand-parents, care-givers often give non-stop for every one of the baby’s awake hours. What does a baby learn?</p>

<p>The ”first steps of learning”:</p>

<ol>
  <li>
    <p>What to see, to hear, to feel</p>
  </li>
  <li>
    <p>What are other people around</p>
  </li>
  <li>
    <p>How to interpret other people (agents)</p>
  </li>
  <li>
    <p>What is my own body</p>
  </li>
  <li>
    <p>The environment around me</p>
  </li>
  <li>
    <p>How to imitate</p>
  </li>
</ol>

<p>Once a baby learns to imitate others, there is a sudden acceleration in learning. Imitation seems like a trivial task to our trained minds, but it has immense potential. It allows to transfer information from one individual to another (young baby) one very effectively. Learning to imitate is the cornerstone of training because it offers examples of behavior that was previously optimized, preventing the young baby from lengthy, costly, potentially dangerous trials-and-errors.</p>

<p>But how do babies learn to imitate? Imitation is not the simplest task that a baby (or a baby robot) needs to learn, neither is one of the first. Yet it may be the giant step forward in developmental learning that bootstraps lifelong learning and an intelligent brain.</p>

<p>In order to imitate another agent, human being, an agent needs to understand what is self, what is others, what are objects, and what are agents, how the world around works, how the self or other agents interact with the world and its objects, and the dynamics of these interactions. In substance it needs to learn the foundations of knowledge that enable an acceleration of learning. A baby that learns to imitate can now learn many skills by just observing others. A robot that learned to imitate, can train on a multitude of YouTube videos.</p>

<p><strong>”First steps of learning”: How do we get there?</strong></p>

<p>Child development and early learning sciences offer innumerable insights on how human babies grow to learn the  <em>first steps of learning</em>. All these scientific discoveries, theory of the mind and brain, neuroscience, psychology, help us immensely in the creation and growing of a brain for robots. One advantage we have is that we can build one and validate our theories. The disadvantage is that this cannot be done by trial-and-error.</p>

<p>Here follows a summary of the components and ideas we may start with. These ideas follow decades of scientific discovery in the fields mentioned above.</p>

<p><strong>Brain and body</strong>: an entity needs to have a brain capable of learning a knowledge graph of the environment it is embedded in. Meaning it needs to support the kind of learning that we want to achieve, eg.: human level. The architecture of this brain is currently unknown, but recent AI foundational models (Large Language Models or LLM) offers insights because they learn a knowledge graph (on words), albeit not multi-modal and embodied. A robot body is foundational to learning because experiences are a combination of body sensory interactions with the world around us. Eyes, ears, touch, all senses come from the body, thus all knowledge comes from the body.</p>

<p><strong>Own Goals</strong>: an entity needs to make its own goals. Imagine the first days for a baby or a baby robot: it needs to learn where to look or what to pay attention to. How does it decide to look at a care-giver face or the motion of another agent as opposed to leaves moving in the background? Even basic sensory attention needs to be directed by some goal. Learning a model of the world cannot be based on static pre-defined cost-functions or reward-functions. The robot ought to learn its own goals and fine-tune its capabilities from the ground-up. How do humans learn all this? What are the ingredients? Are babies born with pre-defined goals (look at faces, suppress hunger, reject pain, etc.)?</p>

<p><strong>Learning algorithms</strong>: An essential component of the brain of a robots is the way it learns from its experiences. Since we want the entity to be able to generate its own goals, we cannot define specific learning function to optimize. We cannot rely on supervised learning because we would need an oracle agent to always provide supervision. We cannot rely on reinforcement learning because it is based on pre-defined reward functions. We cannot use imitation learning because it is essentially a form of supervision. The only possible type of learning in blank-slate robot or young human baby is self-supervised predictive learning. In the real world there is no ground truth but itself. Prediction forecasts the future, and the future is only a few instants away. When that future is realized, we can estimate the value of our prediction, and its validity.</p>

<p>Supervision and reinforcement come into play as agents own goals and techniques. They need to be learned.</p>

<p><strong>Learning environment</strong>: an entity needs a stimulating environment embedded with trainers, other agents that have previously learned a task and can show examples, provide feedback. A static environment with just objects is not enough. In robotics the cost of tutoring is prohibitive as it involves humans operating in real time. Automata are not helpful because they cannot provide fine feedback and the agent can only learn as much as the automata: pre-confectioned solutions. Training agents provide feedback with voice, and no-verbal cues like facial expressions and sounds.</p>

<p><strong>A Proposal</strong></p>

<p>Imagine a robot, sitting on a virtual desk inside your screen. A human operator can interact with the robot and a few objects on the tale, can talk, can move artificial arms in the virtual space.</p>

<p>How can our robot:</p>

<ul>
  <li>
    <p>Learn to pay attention to the interactions? Not to look at leaves outside…</p>
  </li>
  <li>
    <p>Learn what to see, hear?</p>
  </li>
</ul>

<p>We believe that in robotics now it is the time to ask the right questions. How does a brain develop? How can we learn to feed a robotic brain with sensory inputs so we can foster self-learning of reward mechanisms?</p>

<h3 id="we-believe-we-have-to-start-with-learning-tosee">We believe we have to start with Learning to see.</h3>

<p><em>… to be continued… in part 3</em></p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics and AI in 2024 and beyond — part 2]]></summary></entry><entry><title type="html">Robotics and AI in 2024 and beyond — part 1</title><link href="http://localhost:4000/2023/12/13/robotics-2024-part1.html" rel="alternate" type="text/html" title="Robotics and AI in 2024 and beyond — part 1" /><published>2023-12-13T00:00:00-05:00</published><updated>2023-12-13T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/13/robotics-2024-part1</id><content type="html" xml:base="http://localhost:4000/2023/12/13/robotics-2024-part1.html"><![CDATA[<h1 id="robotics-and-ai-in-2024-and-beyond--part-1">Robotics and AI in 2024 and beyond — part 1</h1>

<p><img src="/assets/2023-12-13-robotics-2024-part1/1.png" alt="" /></p>

<p>a robot generated by a robot (DALL-E2)</p>

<p>Robots, such as the one populating sci-fi movies, are not yet part of our lives. We do not have robots that can cook, or dust our surfaces, fold our clothes, and do laundry, clean the house, nor we have robot brains that can drive our car in any environmental conditions. The reason is simply we have not been able to create an artificial robot brain “controller” that can deal with the multi-faceted complexity of our world.</p>

<p>In this article we will explore ideas to create a robot brain and how to train it.</p>

<h1 id="physical-world-multi-modal-learning"><strong>Physical world multi-modal learning</strong></h1>

<p>Robots need a brain that can understand the world in its entirety and based on a complex knowledge graph. The complexity in robotics is that both an understanding of the world and a connection to goal-oriented applications is required to be effective.</p>

<p>For a long time, we have tried to create robot brains by patching separate modules and algorithms, and this approach has not worked. Learning to recognize objects and to grasp them, or to move in space have all the same roots in understanding the three-dimensionality of space and the relationship between spaces and object visual perception. At the same time planning motion or a sequence of process steps requires the basic understanding of the impact of the robot action in the same space and onto the same objects.</p>

<p>What is required is a common model that can learn multi-modal experiences. A robot will have a set of motion capabilities, a vector of actions, and a set of sensors, for example vision, audio, proprioception. Learning needs to correlate all these modalities with the flow of actions and changes in the environment. This means that we need a robot brain capable of evaluating the sequence of sensory inputs and providing a sequence of outputs. A unified model will be able to understand space and objects, move, and manipulate objects.</p>

<p>A robot brain needs to learn a knowledge graph in the physical world. What is a knowledge graph? It is a system that learns real-world “concepts” and can inter-link them based on their semantic and physical meaning and relationships. For example, the concept “cat” in our brain is created by seeing cats, hearing cats, interacting with cats. The fact that we can link worlds like  <em>Egyptian</em>  or  <em>cat</em>  or  <em>Mann</em> happens because these concepts are linked together in our knowledge. We need to build the same knowledge graph into a robot brain, so they can understand the world and all relationships between its components.</p>

<h1 id="goals-and-instructions"><strong>Goals and instructions</strong></h1>

<p>We need robots to help us in our everyday activities and support us in tedious repetitive tasks. Folding dry clothes after a laundry session is an example; it is a repetitive activity that is nevertheless never the same. There is always a difference in what clothes to fold, what their state is, where you pick them up, and where you lay them down. There are infinite configurations that require high levels of generalization by a robot brain. More of this in the following section (learning).</p>

<p>A robot needs to listen to some instruction and execute it. Large language models make understanding human verbal instructions possible. The robot brain now needs to break down the instructions into a plan or successive steps to achieve the goal. Consider the example, for completing a goal: “pick up the blue bottle” when standing at a corner of a room. This simple command in text needs to unleash a sequence of planned motions and real-world interactions: figure out what are traversable areas in the room, where are obstacles, where is the target bottle, and planning motion towards it and an approach to grasping the item.</p>

<p>The robot brain sequential planning is always the same: observe the environment, consult on the goal of the instructions, then perform some actions. This loop repeats many times until the goal is reached.</p>

<p>Planning may be recalling steps in an example or previously completed sequence. Or it may involve mental simulation in the representation space to evaluate multiple possible action sequences.</p>

<h1 id="embodiment"><strong>Embodiment</strong></h1>

<p>Imagine we have a video demonstration of a task. Learning to repeat the same task requires to learn a correlation between what we observe and the sequence of actions we want to perform. Here comes the first big problem: we need this data to come from the first-person perspective of the robot. In other word the sensing and actions need to be coming from the robot body. Learning would be much impossible if we cannot correlate perception and action. This requires the robot and learning experience to be embodied. The complexity is that we cannot easily get training data for such a robot unless we use an external “oracle” controller to gather them. Think of a human piloting a character in a videogame. Therefore, we need to control the robot and create a few examples of the task. This of course is a time-consuming data collection problem, given that we cannot train a robot brain with just 100 examples or maybe even 1000. We may need a large amount, maybe in the same order of magnitude as the samples used to train an LLM.</p>

<p>It would be much easier if the robot could learn to imitate actions from a video, just like we do with YouTube videos. But the chicken-in-the-egg problem is that the robot has not learned to control its own body and does not know how to “imitate” an action.</p>

<p>Where is the mirror-neuron for robots? How do we learn the capability of imitation?</p>

<p>We need to study how to develop a curriculum to learn progressively vision, 3d space, mobility, attention to other agents such as humans, and finally the ability to imitate them.</p>

<p><em>… end of part I</em></p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics and AI in 2024 and beyond — part 1]]></summary></entry><entry><title type="html">Artificial scientists and autonomous laboratories</title><link href="http://localhost:4000/2023/12/05/Artificial-scientists.html" rel="alternate" type="text/html" title="Artificial scientists and autonomous laboratories" /><published>2023-12-05T00:00:00-05:00</published><updated>2023-12-05T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/05/Artificial-scientists</id><content type="html" xml:base="http://localhost:4000/2023/12/05/Artificial-scientists.html"><![CDATA[<p><img src="/assets/2023-12-05-Artificial-scientists/1.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<h1 id="artificial-scientists-and-autonomous-laboratories">Artificial scientists and autonomous laboratories</h1>

<h2 id="what-is-next-for-chatgpt"><strong><em>What is next for ChatGPT?</em></strong></h2>

<p>One of the goal of machine learning is to reach the levels of artificial intelligence. AI, the word, we hear every day, is an empty promise if we are not striving to create artificial brains that can be at the level or above of our own brain.</p>

<p>In the last 20 years machine-learning, notably in the form of neural networks, has produce an incessant set of innovations: from detecting, localizing, and categorizing the content of images, to learning sequences, predictive models, natural-language processing and understanding, just to name a few. But looking back all these innovations were aimed at solving individual problems, one-by-one with a tailor crafted solution. We were always starting with a dataset of inputs and desired outputs; we have been creating neural architectures designed to take the data and the task into account and developed many disconnected one-hit-songs models.</p>

<p>But no more. Since we are and want to talk about AI, we will need to set course for a unified vision and a set of unified tools that can implement artificial brains.</p>

<p>Intelligence is about creating knowledge graphs. Knowledge Graphs (KG), what is seemingly such a dry and ethereal concept, is just concepts in our brain, ideas, that are interlinked together by learning in the physical world. Take the words “Egyptian”, “Manx”, “cat” are individual concepts in our brain that are strongly linked, because “Egyptians loved cats” and “there is a type of cat called: Manx cat”. The words are connected points in our knowledge graph.</p>

<p>Recent Large Language models (LLM) such as Generative Pre-trained Transformers (GPT — with ChatGPT being one of the most famous versions) are machine learning models that can predict the next word in a sequence of text. These models approximate a Knowledge Graph over words and portions of words, and their ability to produce high-quality text, plans, algorithms, drafts that are similar to human output gives us confidence that the path to AI is along these lines. The capabilities of these models and the emergent skills that are learned training them is still a topic of research and will be for quite some time. These models are very large and capable and were trained without fixed datasets of input and output pairs, so they are time-consuming to test.</p>

<p>What is the future for ChatGPT? It is learning in the physical world. Our brain is also implementing a knowledge graph, but it is based on multi-modal “concepts”. When we learned the concept “cat” we did so by connecting visual, auditory, sensory representations of a “cat”, interlinking the way the cat moves, feels to touch, the noise it makes. The word “cat” is just a label that we use on the concept to allow us to communicate it to other people, given that our brains are disconnected (no WiFi, ethernet or radio link between them).</p>

<p>Our future AI models are going to be called Large World Models (LWM) and they will be trained to predict the next representation in a multi-modal input data flow. The complexity of this training modality lies in the fact that the “next representation” of the real world is not as simple as predicting the next words in a text. Words are a knowledge “representation”, but multi-modal data like images and sounds are harder to break into parts. Is the next portion of the image on the right what we need to predict? Or is the one below or above? Is it the next 10 millisecond of sound, or the next note, or the next segment?</p>

<p>Learning in the physical world also requires a body. We cannot learn multi-modal if we do not have a fixed set of sensors that can record and encode these modalities. This is the path needed to train an artificial robot brain. Learning will be based on physical interaction with the environment and prediction.</p>

<p>Learning streams of “concepts” via predictive learning requires to predict the next concept from a set of previous ones. This is the main learning modality. Learning occurs in a continual fashion, but it is mostly inactive if our prediction matches the physical next state of concepts. In other world this artificial brain is ignoring every information that it already has when its predictive abilities are correct. When prediction fails, the artificial brain signals a “surprise” which enables learning; this example we stumbled on needs to be learned!</p>

<p>Reinforcement learning occurs when we need to optimize for a specific goal. Our action affect the environment in ways we also need to learn and predict. It is not yet clear how to mix multiple learning modalities that could be pushing internal representation in different directions. Also learning sequences of concepts seems simple enough, but in the physical world “concepts” streams can be tricky: they are mostly attuned to changes in the environment and are not confined to a fixed sampling in space or time. It is about noticing changes in the environment, possibly because of our actions.</p>

<p>Mental simulation must also play a fundamental role in reinforcement learning and planning, because they allow an artificial brain to be more efficient in sampling possibilities. Rather than trying every possible move at random, an effective and efficient agent needs to simulate many and only try a very small set.</p>

<h2 id="artificial-scientists"><strong><em>Artificial Scientists</em></strong></h2>

<p><img src="/assets/2023-12-05-Artificial-scientists/2.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<p>What would it take to create an artificial scientist? This is an algorithm that can read the entire body of knowledge in one or multiple fields and is able to gather data, create hypotheses, run experiments, generate reports and theses.</p>

<p>Some science like mathematics may be able to be trained purely on text and symbols. Experiments on LLM trained to perform mathematical proofs are promising. But for any physical science, the artificial scientist will be based on LWM, trained on literature and videos. Similar to human reading abilities, inferring words from vision, AI literature reading needs to be vision-based, because human-readable documents are a collection of not just text, but also plots, graphs, diagrams, tables, paragraphs titles, captions, etc. An artificial scientist needs to be able to read a plot: “what is the value of Y at X=1?” and also understand diagrams and complex figures: “what is block B connected to?”. Current LLM can already understand tables, databases, datasets by being able to write scripts and code interfacing and running on external files.</p>

<p>The multi-modal LWM needed for an artificial scientist is a vision-based input model that can read pages from sequential blocks of pixels. This allows it to be able to read all components of an article. I believe this training and system will be able to generate sound science plans just as an LLM is able to plan for many activities. In fact, most of scientific literature imbues research plans and scripts for testing hypotheses.</p>

<p>Artificial scientist trained in this fashion will thus be able to generate research plans. A research plan will begin with a set of hypotheses, a list of experiments that need to be performed. They will also have the ability to run external tools, such as simulators, code, scripts and read the output of those tools. They will be able to run tools multiple times, and aggregate all outputs into a final report, or collection of theses to summarize the results from experiments.</p>

<p>Simulations are an ideal companion to autonomous research plans because they allow to test the hypotheses and obtain ground truth. These can then constitute a new set of training examples that can self-improve until hypotheses are validated or eliminated because of lack of resources (here: time).</p>

<h2 id="autonomous-laboratories"><strong><em>Autonomous Laboratories</em></strong></h2>

<p><img src="/assets/2023-12-05-Artificial-scientists/3.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<p>An artificial scientist will be able to interface and run autonomous laboratories. These are physical machines that can be interlinked to performed experiments in large scale and with autonomy. An artificial scientist is the controller of an autonomous lab, in the same way that a brain is a controller for a body. Videos and audio are needed for an artificial scientist operating the physical world, thus the core artificial brain is a robotic LWM trained on videos operations of the laboratory, including visuals of inputs, outputs, desired outcomes. The artificial scientist controller will need to be able to read instrumentation files and reports, and examine samples either by visual or physical inspection, or by using yet another tool or machine.</p>

<p>An autonomous laboratory of the simplest form is a manufacturing robot that assembles an item based on instructions. But the combination of an artificial scientist as controller allows the laboratory to do more than just follow instructions. It can generate output based on an incomplete set of instruction, in the same way that a caption can generate an image by mean of an artist. The complexity in an autonomous lab is not just its controller, the artificial scientist, but the instruments and materials needed to operate it, and the requirements for resetting, disinfecting, cleaning and restoring each tool to the initial conditions, ready for the next batch of processing.</p>

<p>Autonomous labs need to interact with humans and thus learn human-automata teaming techniques. This is not unlike the artificial brain of a robot co-habiting space with other humans, pets, creatures.</p>

<p><img src="/assets/2023-12-05-Artificial-scientists/4.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Should we worry about AI (machine-learning)?</title><link href="http://localhost:4000/2023/05/24/Should-we-worry-about-AI.html" rel="alternate" type="text/html" title="Should we worry about AI (machine-learning)?" /><published>2023-05-24T00:00:00-04:00</published><updated>2023-05-24T00:00:00-04:00</updated><id>http://localhost:4000/2023/05/24/Should-we-worry-about-AI</id><content type="html" xml:base="http://localhost:4000/2023/05/24/Should-we-worry-about-AI.html"><![CDATA[<h1 id="should-we-worry-about-ai-machine-learning">Should we worry about AI (machine-learning)?</h1>

<p>I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind…</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*k0dmcWMTEoGspk3y_CztiA.png" alt="" /></p>

<p>Artificial intelligence that is nice — by DALL-E2</p>

<p>I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind intelligence means being able to use acquired knowledge to solve problems never encountered before, optimizing utility.</p>

<p>These days the words “AI” are everywhere, but maybe we should start by saying that most of what is labeled AI today is really just “a machine learning algorithm and architecture”.</p>

<p>I like to start with this because I think it sets the tone for everything else. People have seen movies about AI and what it can do. They have seen AI controlling spaceships that get rid of their crew. AI that creates robot to wipe out the human race. AI that starts an atomic war and ends our planet.</p>

<p>There are also movies about AI curing all diseases, designing new devices, re-programming humans for the betterment of our world (yep!). And many more fantasies that paint a more rosy future.</p>

<p><strong>These are movies, stories, fantasies. They are not real.</strong></p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*iAM7eBIWa5tahJmitQTRIw.png" alt="" /></p>

<p>Artificial intelligence that is evil — by DALL-E2</p>

<blockquote>
  <p>the question is: can they be real one day?</p>
</blockquote>

<p>The answer is: nobody knows; we cannot predict the future. But because we are holding machines at a higher standard than humans, maybe we can take a quick look and summarize where humans stand in all this. What have humans done to other humans and to the environment so far in the last 100,000s of years since we have been around?</p>

<p>Humans grouped together and started to gather all possible resources from the environment. They would kill other animals, humans without much thought if only to take their land and resources. For a long time we were not able to destabilize the planet, beside being so nasty to each other. But then in the late 1800s we started building large machines that can cut trees and flatten the soil easily. Then the last 200 years we have be raping the planet with no end in sight. To the point that today we know well that if we keep going ahead we will have no planet any longer, no home to live in.</p>

<blockquote>
  <p>We are eating away our own Mother Earth, and we not very nice about it.</p>
</blockquote>

<p>Now in the last 50 years or so we have built computers and we are trying to replicate our brain into machines. Given our past, we should be concerned about what we are building. After all the previous machines were not very nice to the planet. And our war machinery also is not very nice to other humans. In the last 50 years we have created weapons that can wipe out all living things in this planet if we wanted to.</p>

<p>So yes I would think we should be concerned about anything that we are building, AI or not.</p>

<p>If we succeed in building a real artificial brain like our own or better, we are basically bypassing evolution. This artificial intelligence can possibly create much more dangerous machines than the ones we build before. This artificial intelligence may start to see us humans as a competitor for Earth resources. This artificial intelligence may think of us the same thoughts we think of insects and other animals: that it is ok to keep them around as long as they are not in our way. Or the planet for that matter: we can keep this forest as long as we do not need it for a new housing development.</p>

<p>Maybe this artificial intelligence will be so superior that it may reprogram our brains so that we can act as machines for its cause. But then why? It may be able to build much better machines than we can, and we would just be useless. I seriously do not believe it will use us as slaves. I do not think it will use us as batteries. But maybe this artificial intelligence may destroy the atmospehere of our planet while building the machines that it needs. Too bad we will not be able to breath. We are insect after all.</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*sErepqwaT4d6ZADAAL0qIw.png" alt="" /></p>

<p>Artificial intelligence that is evil — by DALL-E2</p>

<p><strong>Even these are just stories, fantasies, movies to be made</strong></p>

<p>Nobody knows the future. Or maybe we do know it a bit, don’t we? Yes! If we keep killing forests and producing unnatural gases, the planet will heat up. By becoming too warm the planet not be a cradle for life any longer, and one day become a boring desert like Mars (sorry Mars~!).</p>

<p>That artificial intelligence will be fine, by the way. It will just build a spaceship and travel the Universe, not to find a new home or to conquer it. Maybe it will just roam the Universe because it is fun to explore. Especially when you have infinite time. The Sun becomes a red giant and engulfs our Solar system? Not a problem, there are almost infinite galaxies out there, each with trillions of stars and planets. It is an adventure that never has to end.</p>

<p>The artificial intelligence we built maybe will not care much about us after all. We become insignificant, like the single grains of dust that make up this Universe.</p>

<p>That said… we are here today and we need to make sure our machine learning algorithms and architectures produce some value. That means maximize gain and minimize losses. But for who? The answer is simple actually: for the entire Earth as an ecosystem. We would not be here if there was no such ecosystem. So that need to be preserve even if it means sacrificing human goals and ambition and comfort and expansion.</p>

<p>But we do not have good track record of caring about our own species, sexes, races, and our planet. So I am not so hopeful about our abilities to control machines. We have always gone as far as we could, but one day going far will mean we cannot turn back. We cannot undo our mistakes.</p>

<p>Today we should build algorithms that are fair, that treat all humans alike. And maybe we should actually treat all human fairly and alike in any condition, in any part of the day, months, years. In any weather, in any state, in any country. In any race, in any religion or belief. We do not have a good track record of that.</p>

<p>Maybe we should build algorithms that do not offend one race or prefer another. And maybe we should again do that in all aspect of our lives, not just in the context of AI. We do not have a track record of that either.</p>

<p>Maybe we should build conversational algorithms that will never produce any offending dialogue. Even if we trained these algorithms with our human data, much of which is not a proper teaching material. We do not have a good track record of teaching machines nice conversational material because our website are infested with our toxicity, our hatred, our ill, our evil, our envy. We do not have a good track record of containing our emotions, both good and bad.</p>

<p>Maybe we should have an overseeing department in every company that builds machine learning algorithms. But who is in that department? Other humans? And what are their beliefs? Are they representative of all humans? We do not have a good track record of creating such groups of people, of such departments.</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*ABZzdLsG6PfC0x07omiU4Q.png" alt="" /></p>

<p>a better future for humans, our planet and AI — by DALL-E2</p>

<p>Or maybe we should stop developing AI altogether — yes! That will surely save us. And maybe we should stop building weapons too, and removing trees and forests, and exploiting other humans, and all the other negative things we do that are innumerable. We also do not have a good track record of that. Sure we were able to disarm for a while and remove atomic bombs. But how fast can they come back that day that some of us decide they really need them?</p>

<p>Or maybe we should just do a better job in promoting education, in learning that we have not been very nice to each other, and to our planet. And learn that we are all the same after all, and that even insects are needed for our survival. Maybe we should learn more and grow our own human intelligence before we try to build an artificial one. Maybe armed with that collective and shared knowledge, capabilities, wealth, resources, maybe we will be able to make better choices for us, the planet, and AI.</p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me: <a href="https://medium.com/@culurciello/">Medium</a>, <a href="https://culurciello.github.io/">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a> to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Should we worry about AI (machine-learning)?]]></summary></entry><entry><title type="html">A picture is worth a 1000 words</title><link href="http://localhost:4000/2023/03/30/A-picture-is-worth-a-1000-words.html" rel="alternate" type="text/html" title="A picture is worth a 1000 words" /><published>2023-03-30T00:00:00-04:00</published><updated>2023-03-30T00:00:00-04:00</updated><id>http://localhost:4000/2023/03/30/A-picture-is-worth-a-1000-words</id><content type="html" xml:base="http://localhost:4000/2023/03/30/A-picture-is-worth-a-1000-words.html"><![CDATA[<h1 id="a-picture-is-worth-a-1000-words">A picture is worth a 1000 words</h1>

<p>Evolution of large language models into artificial brains through multiple sensory modalities</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*aO15HKrFXKKk6OOzjPOQGA.jpg" alt="" /></p>

<p>As surprising as it is our very human predictive abilities hide a path to understanding our intelligence. How do we solve so many tasks, learn so many new experiences, how do we so easily adapt to new environments?</p>

<p>The recent (2022–23) success of large language models (LLM) like ChatGPT and GPT-X paints a story. Trained to predict the next words in a sentence, these models showed superb language proficiency. The key ingredients are large amounts of data — more than you can read in a lifetime, and a simple predictive algorithm.</p>

<p>When we think of language we often forget it is an abstract representation of reality, of the world we live in, used to tag concepts and ideas of the real world into a set of labels we can use to communicate. But “the rose exist before its name”, meaning that any object in the real world, like a rose, is real even without a word for it. What is a “rose” then?</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*zYre8KPdW4OXRLcgwFE0tw.jpg" alt="" /></p>

<blockquote>
  <p>A picture is worth a thousand words</p>
</blockquote>

<p>A concept in our environment is just a collection of data from our sensors. For us humans a rose is equivalent to its image, its perfume, its delicate touch, its movement in the wind. That “concept” is the essence of an object, or our qualia, our subjective ensemble of sensory information fused into one token of knowledge.</p>

<p>This is the future of artificial brains and the evolution of LLM: being able to represent “concepts” from their multi-modal information.</p>

<h4 id="multiple-modalities">Multiple modalities</h4>

<p>LLM are trained with text only, here instead we propose to use:</p>

<ul>
  <li>visual information</li>
  <li>audio</li>
  <li>touch, proprioception</li>
  <li>text, tags, etc.</li>
</ul>

<p>This requires the artificial brain to be embodied, or at very least have a fixed set of sensors that maintain consistency over the entire learning life.</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*mkHVjGCOhUAaAjaRmt9XpA.jpg" alt="" /></p>

<h4 id="learning">Learning</h4>

<p>Training these new multi-modal artificial brains uses powerful unsupervised learning algorithms. Or should we call these self-supervised? Since there is a undeniable need for data to train on, but not one that comes with labels.</p>

<p>How do learning algorithms evolve from predicting the next word in a sentence? There are multiple ways to predict concepts:</p>

<ul>
  <li>predicting the next concept in a video or audio stream</li>
  <li>predicting what is around a concept in visual space</li>
</ul>

<p>Self-supervision is in essence the search for sequential data by using time or by using data perturbation in space that occur naturally, such as noise masking in audio, occlusion in vision. Turning static data into sequence can be done by:</p>

<ul>
  <li>manipulating an object, changing point of view</li>
  <li>listening to the concept multiple times</li>
  <li>masking portions of the data</li>
</ul>

<p>See Note 1 below for additional comments.</p>

<p>Of course there is the problem of training cost and scale. Few institution can train models that cost many M$. For example GPT cost &gt; 5M$ to train. We need a distributed and open system to create foundational models, one that is operated and serves a large majority of people. See <a href="https://medium.com/@culurciello/how-can-we-make-transformers-large-language-models-gpt-more-sharable-how-can-we-make-them-grow-6b144f0b0d7c">this post</a> also for more details.</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*u9XjIYBoUfw2k4GOyJKr_g.jpg" alt="" /></p>

<p>The co-occurence of multiple sensory domains in space or time is another foundational learning algorithm that can be used to learn “concepts”. See <a href="https://medium.com/@culurciello/drop-the-dataset-b3ed418bd69d">this post</a> and <a href="https://medium.com/@culurciello/drop-your-dataset-part-2-c97433765998">this post</a> about this.</p>

<h4 id="what-makes-these-multi-modal-models-candidates-for-artificial-brains">What makes these multi-modal models candidates for artificial brains?</h4>

<p>Knowledge is organized in a graph where nodes are concepts and links form relationships between concepts. These links connect ideas and experiences into sets that are relationally-linked to each other, can summon and elicit each other.</p>

<p>This is not much different from the language models that arise from training an LLM. Language models are also a graph that connects words together and sets the rules for creating sentences, give rise to semantical meaning, group information.</p>

<p>We are fascinated by the 0-shot abilities of LLM today, their ability to perform tasks for which they were not explicitly trained for. This ability will be even more pronounced in multi-modal artificial brains because knowledge will be inherently connected across domains, allowing to reason about images and videos and audio, correlating speech to words, words to frames, lyrics to music.</p>

<p>A good definition of intelligence, after all, may very well just be our ability to connect concepts from multiple ares of knowledge without an explicit prompt.</p>

<h4 id="lifelong-learning">Lifelong Learning</h4>

<p>Ask an LLM what happened to the World Cup Soccer of 2022, and it will tell you that its factual knowledge stops in mid-2021. How do we keep artificial brains up-to-date? The same way we humans keep abreast of new knowledge: by continuing to learn.</p>

<p>It is easy to keep training a neural network, you just give it more samples every day. What is important to address in research and development is the drift in knowledge that can be caused by an unbalanced training. This happens for humans too, when environmental changes can lead knowledge and ideas to drift. Forgetting is also common when a set of knowledge is not refreshed for some time. Artificial brains will have the same and identical problems.</p>

<h4 id="using-tools">Using tools</h4>

<p>An artificial brain will not need to become a clone of each and every tool. It will not need to memorize all Earth’s knowledge but rather be able to retrieve it. Not become a calculator, but rather learn to use one. Not dig holes, but rather drive tractors and machines.</p>

<p>By showing examples sequences of how to use a tool and how to extract knowledge from a website, as an example, is again an example of parallel with human training. Are these applications that can be built on top of these artificial brains? Yes most definitely, as the predictive ability of the model is able to learn a sequence of steps to accomplish goals. Again, similarly to training a human to use a tool.</p>

<p>The interesting thought that arises now is that these artificial brains are in fact starting to be closer and closer to a human brain, with parallels on curriculum training to incrementally increase abilities and knowledge.</p>

<h4 id="final-notes">Final notes</h4>

<p>So yep — multi-modal, prediction, co-occurence. All ingredient of artificial brains implemented with neural networks. All this is the future you will see happen in just a few months.</p>

<p>The quick development of artificial brains that are becoming more and more capable is scaring a lot of people, as it happened before for any new and disruptive technology. Yet even if all of us wanted to use these models at large-scale, there are significant hurdles we need to pass. One is the cost of training deploying models. Two is the inability to share models and keep on training them while sharing the benefits. Three is the centralization of such model because of the cost and expertise needed to train them.</p>

<p>The rise of artificial brains will bring out at once all the problems we have in current neural networks: continual learning, training and inference efficiency, distributed learning, knowledge drift and forgetting, dataset biases, ethical use, toxic behavior, and more.</p>

<p>Much work to be done, between little and big steps forward. Are you ready to join us for the research ahead?</p>

<h4 id="note-1">Note 1</h4>

<p>One complication is that in a stream of concepts from a video, for example, predicting the next concepts does not mean predicting the next frame in a video, but rather predict how concepts morph in time. Prediction in video does not translate to fixed windows, but needs to understand when concepts changed “enough” from each other. A bit of a chicken-in-egg problem, isn’t it? But this is a good examples of the pitfalls of self-supervised learning and current methods in AI/ML. More insights are needed here, more research.</p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me: <a href="https://culurciello.medium.com/">Medium</a>, <a href="https://culurciello.github.io/">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a> to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A picture is worth a 1000 words]]></summary></entry><entry><title type="html">How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?</title><link href="http://localhost:4000/2023/01/25/Transformers-GPT.html" rel="alternate" type="text/html" title="How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?" /><published>2023-01-25T00:00:00-05:00</published><updated>2023-01-25T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/25/Transformers-GPT</id><content type="html" xml:base="http://localhost:4000/2023/01/25/Transformers-GPT.html"><![CDATA[<h1 id="how-can-we-make-transformers-large-language-models-gpt-more-sharable-how-can-we-make-them-grow">How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?</h1>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*vd1bjSTykGLQEIcqP6d15A.jpg" alt="" /></p>

<p>language models and transformers in  <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">https://huggingface.co/spaces/stabilityai/stable-diffusion</a></p>

<p>Today I was reading the excellent list of Transformers here:  <a href="https://github.com/NielsRogge/Transformers-Tutorials">https://github.com/NielsRogge/Transformers-Tutorials</a>  and soon enough realized how many separate applications and separate datasets these have been trained on. Most of these neural network are offered as pre-trained and can be used and fine-tuned for your custom application. That is super great for someone that is looking for a specific problem to solve.</p>

<p>For example, you can get a model to turn an image of a receipt into a JSON table, or you can extract text from a document, or get depth estimation from a single image, etc.</p>

<blockquote>
  <p>All of these are separate tasks trained in isolation</p>
</blockquote>

<p>One thing that one realizes pretty quickly by training neural networks is that training on more data, more tasks, more abilities always gets you a better model. One that can do much better in the one application you care about, even if you are not looking for a do-it-all model.</p>

<blockquote>
  <p>So for me this is really  <strong>a big problem!</strong></p>
</blockquote>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*Y0EevVSx0gY82J23st4QTg.jpg" alt="" /></p>

<h4 id="a-set-of-problems">A set of problems:</h4>

<ul>
  <li>models are trained in isolation</li>
  <li>use a lot of computing resources to train</li>
  <li>are only effective in their own domain</li>
  <li>use many custom input and output encodings, symbols, techniques</li>
  <li>are not able to share knowledge model to model</li>
</ul>

<blockquote>
  <p>All the knowledge these models learned, it is not shared!</p>
</blockquote>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*W7MnOXOw3LfnrFVJz_mosw.jpg" alt="" /></p>

<h4 id="a-set-of-questions">A set of questions:</h4>

<p>How can we share these models and what they learn?</p>

<p>How can we build a unified model?</p>

<p>How can we make sure all data uses the same formats and is compatible? Should we learn those transformations also?</p>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*W-t7a4jHBYp7ZbKQindF5w.jpg" alt="" /></p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://culurciello.medium.com/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?]]></summary></entry><entry><title type="html">Drop your dataset — part 2</title><link href="http://localhost:4000/2022/12/06/Drop-dataset-part-2.html" rel="alternate" type="text/html" title="Drop your dataset — part 2" /><published>2022-12-06T00:00:00-05:00</published><updated>2022-12-06T00:00:00-05:00</updated><id>http://localhost:4000/2022/12/06/Drop-dataset-part-2</id><content type="html" xml:base="http://localhost:4000/2022/12/06/Drop-dataset-part-2.html"><![CDATA[<h1 id="drop-your-dataset--part-2">Drop your dataset — part 2</h1>

<p><img src="/assets/2022-12-06-Drop-dataset-part-2/1*0DJ3aWBrsOsU7Iky8ePimA.png" alt="" /></p>

<p>In the previous post I have given some ideas on how we can overcome the limitation of current neural network training without using supervised datasets. I want to go into more details here to provide more ideas and insights into the evolution of neural network training: neural nets v2.0 or software v3.0.</p>

<h2 id="relationships">relationships</h2>

<p>First of all let us remember that in machine learning the focus is to learn relationships in the data. Training data is in the form of one or multiple samples: {input, desired_output} and the machine learning algorithm has to learn a relationship between “input” and “desired_output”. That is to say that if you give it: “input”, the machine learning algorithm will provide you with: “desired_output”.</p>

<p>This is simple dataset and a point-to-point application of machine learning. A neural network model can be used as “black box” to learn the above relationship. Other machine learning models can be used also but I will not discuss those here.</p>

<h2 id="generalization">generalization</h2>

<p>Neural networks today are mostly trained using the “relationships” approach of learning, fed with supervised data from a static dataset. We have hoped that point-to-point application of our dataset would generalize and interpret a large number of new data even if it was not identical to the exact examples in the dataset.</p>

<p>The issues with a static dataset and current neural network training techniques are:</p>

<ul>
  <li>dataset generally do not evolve, are static</li>
  <li>neural network training cannot use new samples because of  <a href="https://en.wikipedia.org/wiki/Catastrophic_interference">forgetting</a></li>
  <li>we do not have labels for  <a href="https://en.wikipedia.org/wiki/Online_machine_learning">online learning</a> — who labels unseen data?</li>
</ul>

<p>These issues limit applicability only in confined and fixed domains where a dataset can be provided and such dataset is applicable to future online data.</p>

<p><img src="/assets/2022-12-06-Drop-dataset-part-2/1*4chmqjWyBwZdG3JPcRFQmQ.jpeg" alt="" /></p>

<p>label “cat”</p>

<h2 id="what-is-a-label-anyway">what is a label anyway?</h2>

<p>But it may be interesting to try to first think of what is a “label” for our data. For example the input is a picture of a cat, and the label is the word “cat”.</p>

<p>The concept of “cat” exists even without the word and if you happened to grow up alone observing the world, you would likely know what a cat is, but maybe have no word for it. Humans use word to communicate between each other because they all have disconnected brains and need words to share knowledge.</p>

<p>So in a way, we are training neural networks with supervised dataset just to learn our language! So that the neural network output is something we can understand. But this is not the only way. We can use unsupervised learning techniques like clustering which will, for example, put all pictures of cats in a category, and other pictures of dogs in another, and so forth. But these categories will not have a name because we have not given it one yet. But they will exist and be useful nevertheless. After all all we need is to see one sample input from the cat category to recognize: yes, that is a picture of a cat, and all pictures in this category are pictures of a cat, so this category is “cat”!</p>

<h2 id="language">language</h2>

<p>At the beginning of neural networks, late 2000s we started learning the relationship {images, label}. In teh dataset  <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a>, for example, there are pictures of digits and the labels are “0”, ”1”, ”2”, etc. Later in 2002 onward, we had ImageNet with picture of dogs and the labels were “terrrier”, “pomeranian”, etc. Then in 2014 or so we moved to image captioning, to learn more complex relationship between our human language and some data (pictures). We are thus forming a multi-modal interaction of images and words.</p>

<p>The large language models neural networks that are popular these days (12/2022  <a href="https://openai.com/blog/chatgpt/">ChatGPT</a>, for example) are not trained supervised on couple of {input, desired_outputs}. They are trained to predict the next word in a sentence, unsing single-modal training on text only. This self-supervised technique is much preferred to a curated dataset, but will require more data to learn — not a problem for text, as we have huge amount of human data online and digitized!</p>

<p>The large neural network language models are impressive because they somehow look like conversational entities that are human-like, and can report and provide useful information through dialogues-like interactions and even some reasoning!</p>

<p>But these models were trained on text only, so we cannot the expect to really understand the world we live in without all the rest of sensing modalities. So if you find the recent large language models impressive, imagine what they could be if they were trained on text, images, videos, audio, more all together!</p>

<h2 id="multiple-modalities">multiple modalities</h2>

<p>We want to use neural networks applied to our world, and therefore we will likely start from a human-like perception. Vision, auditory, touch, smell, etc. there will be many sensing modalities that we need to use to acquire a large knowledge of the world in a way that we can scale it and continue to adapt it to the world during the neural network “life”.</p>

<p>I proposed the architecture  <a href="https://medium.com/@culurciello/how-can-we-build-an-artificial-brain-from-our-knowledge-of-the-human-brain-48fbd1ed82ca">Mix-Match</a>  before in another post to be able to absorb all the world’s data into a single neural network. This network is trained on a dataset, yes, but not a curated dataset of inputs/outputs samples. Rather is fed multi-modal data that is sparsely labeled, and mostly self-labeled by co-occurrence of multi-modal data in pace or time.</p>

<p>I believe that the future of applicable neural networks is in the training with multi-modal data, where the relationship between different data modalities are combined into powerful  embedding of concepts. This can be done by simply applying a  Hebbian-like learning  regularization. This regularization strengthens the connection of multiple concepts occurring closely in space or in time, as for example a figure in a paper and its caption, or a video of walking and the sound of feet hitting the ground.</p>

<p>Supervision happens automatically in multi-modal data trained this way because all the modality, including textual descriptions are embedded in the same space. This is a great advantage to a curated but limited dataset.</p>

<h2 id="intelligence">intelligence</h2>

<p>Suppose we define intelligence as the ability to solve new problem based on our acquired knowledge. Without knowledge of any sort there is no intelligence, so we will have to start from somewhere.</p>

<p>The question one can ask on the Mix-Match model is how do we encode data from ultiple modalities, can we use pre-trained encoders. And the answer is we should not, because by doing so we are constrained to a set of neural networks that are trained supervised. But if we fine-tune or let these encoder evolve, maybe… I think these encoders should be trained from scratch on the new multi-modal set of data, and maybe shared across multiple training systems.</p>

<p>The other bug question is: will Mix-Match experience final concepts embeddings drift when trained? For example if we embed the knowledge of “cat” via image, videos, text, will a new set of documents and data move this embedding so much that it will be “forgotten”? I do not not yet have an answer to this question, but I see that data trained with contrastive embeddings in a similar way is more robust to learning shifts than training on a standard supervised dataset (see  <a href="https://arxiv.org/abs/2111.01124">here</a>  for example).</p>

<p>If we think that new large natural language models with neural network provide some kind of intelligent behavior (for example solving simple math problems), I think you can understand that Mix-Match models will be able to do much more of that, and also reason about what they see and hear, thus providing a real robot brain!</p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://culurciello.medium.com/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Drop your dataset — part 2]]></summary></entry><entry><title type="html">Why we fill our day by filling forms?</title><link href="http://localhost:4000/2022/11/16/Why-filling-forms.html" rel="alternate" type="text/html" title="Why we fill our day by filling forms?" /><published>2022-11-16T00:00:00-05:00</published><updated>2022-11-16T00:00:00-05:00</updated><id>http://localhost:4000/2022/11/16/Why-filling-forms</id><content type="html" xml:base="http://localhost:4000/2022/11/16/Why-filling-forms.html"><![CDATA[<h1 id="why-we-fill-our-day-by-filling-forms">Why we fill our day by filling forms?</h1>

<p><img src="/assets/2022-11-16-Why-filling-forms/1*aRItl7e-elF90HHF1hhcdg.jpeg" alt="" /></p>

<p>I do not know about you, but one of the worst way to spend time for me is  <em>filling forms</em>.</p>

<p>Yet it almost looks like we humans have evolved to hurt ourselves, and this time by creating more and more forms to fill!</p>

<blockquote>
  <p>Why? Cry!</p>
</blockquote>

<p><img src="/assets/2022-11-16-Why-filling-forms/1*-oXHcLdb8MA03aJttbYvpg.jpeg" alt="" /></p>

<p>You get a form on the web, fill your shipping information, your credit card, etc. Luckily most computers OS does this for us to some extend. It is not confident enough sometimes, even though filling web forms is the easiest today for a computer. Yet it still ask us input by input, click away!</p>

<p>And yes I complain much less about that, because we do have some automation that somehow works (not all the times) and save us time.</p>

<blockquote>
  <p>Kudos to us! We did that!</p>
</blockquote>

<p>On the other hand, you sometimes get actual documents to fill today, in the old times you had to do it with paper and pencil, and there we go: there was and there is no automation there in sight. None has seen a robot, let alone one capable of writing. Luckily again we have computers, so many of these forms we can fill today on a computer. Most of these are PDF forms, some fillable — great! Some are computer generated and you can jump form to form and input text by typing, but some forms are scanned and they do not allow you to fill them easily. You have to create boxes and input text, manually — that is super painful!</p>

<p>And yet we still communicate with this inferior form of data transmission to this day! Let alone we have millions of forms scanned or in paper form sitting in our data warehouses, and one day we will have to digitize them and even transcribe the… good luck. We do not have any of these tools today, even though computer vision experts have been at it for decades!</p>

<p><img src="/assets/2022-11-16-Why-filling-forms/1*rFlUGS0GDoobPGJMI67xdw.png" alt="" /></p>

<p>But back to filling forms!!! Imagine a dream system:</p>

<ul>
  <li>you put all your private document into a folder</li>
  <li>you input a form to the dream system</li>
  <li>the dream system goes and fetches all data the form needs from your folder of documents</li>
  <li>it then pastes all in the form</li>
  <li>and presents the form to you, filled</li>
  <li>heart full of joy!</li>
</ul>

<p>Why, oh why can’t we have that today?</p>

<p>Oh I guess I’ll have to work on it…</p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://culurciello.medium.com/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Why we fill our day by filling forms?]]></summary></entry><entry><title type="html">Clean your social network</title><link href="http://localhost:4000/2022/11/13/Clean-your-social-network.html" rel="alternate" type="text/html" title="Clean your social network" /><published>2022-11-13T00:00:00-05:00</published><updated>2022-11-13T00:00:00-05:00</updated><id>http://localhost:4000/2022/11/13/Clean%20your%20social%20network</id><content type="html" xml:base="http://localhost:4000/2022/11/13/Clean-your-social-network.html"><![CDATA[<h1 id="clean-your-social-network">Clean your social network</h1>

<p><img src="/assets/2022-11-13-Clean your social network/1*vBzxRgRTXb2lYpOGDypyug.png" alt="" /></p>

<p>Do you feel like your social network is not what it used to be? The messages and community you once enjoyed are not giving you the warm feeling they once did?</p>

<blockquote>
  <p>it may be partly your fault…</p>
</blockquote>

<p>Selecting the right people and connection after all, is the foundation of a social network. Initially, in the early days of your social networking, you had fewer connections and it was about sharing passion and good times. But then, over time you connected with people that think more like you in one extreme or the other. Oftentimes in periods of negativity you can accumulate several negative connections that will then bring you down and keep you down even in more positive times!</p>

<p><img src="/assets/2022-11-13-Clean your social network/1*bXhIwXwdHC6bmd7f4_8d-Q.jpeg" alt="" /></p>

<p>But there is an easy fix: every once in a while,  <em>clean your social network!</em></p>

<p>What do I mean by clean? You may need to go over all of the people or groups you follow, and ask yourself:</p>

<ul>
  <li>do I recognize this person?</li>
  <li>does this person bring a positive feeling?</li>
  <li>does this person contribute to my learning or self-improvement?</li>
  <li>does this person connect me with other like-minded positive people?</li>
</ul>

<p>If your answers are mostly no, whether with solid proof or by gut feeling, then remove than social connection from your network!</p>

<p>It may take a while, as year by year you have accumulated 10s, hundreds, thousands of connections. It may take 2–3 hours of work, sifting through all your social network, but the good news is that it does not need to be completed in one session. You can take a few minutes per day over some time, and get the cleaning done!</p>

<p>In some cases it may be easier to just delete your account and start fresh, but it is not always easy or for the faint of heart!</p>

<p>Regardless, make sure you complete the process and repeat it once a year or two. This will clean your social network!</p>

<p>What should you expect after?</p>

<ul>
  <li>you will be connected to more likely minded people</li>
  <li>you will feel part of a close-knit again</li>
  <li>you will receive the information you want</li>
  <li>you will find it warmer in your heart</li>
  <li>it may be like an extension of your home</li>
</ul>

<p>This should be a Spring-cleaning type of yearly task that we should often perform to feel better and restore our network!</p>

<p>9</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Clean your social network]]></summary></entry></feed>