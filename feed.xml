<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-US"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-US" /><updated>2025-01-03T07:25:20-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Eugenio Culurciello Blog</title><subtitle>A minimal, responsive, and powerful Jekyll theme for presenting professional writing.</subtitle><entry><title type="html">Artificial Intelligence, AI in 2025 and beyond</title><link href="http://localhost:4000/2025/01/03/AI-in-2025.html" rel="alternate" type="text/html" title="Artificial Intelligence, AI in 2025 and beyond" /><published>2025-01-03T00:00:00-05:00</published><updated>2025-01-03T00:00:00-05:00</updated><id>http://localhost:4000/2025/01/03/AI-in-2025</id><content type="html" xml:base="http://localhost:4000/2025/01/03/AI-in-2025.html"><![CDATA[<h1 id="artificial-intelligence-ai-in-2025-and-beyond">Artificial Intelligence, AI in 2025 and beyond</h1>

<h2 id="some-thoughts">SOME THOUGHTS…</h2>

<p>We are reaching the second quarter of the 21<sup>st</sup> century: it is an exciting milestone. I have been in “adult” life in the last 25 years and most of it was dedicated to learning technologies such as electrical engineering, computer design, AI, machine-learning, microchip design, etc.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/1.jpg" alt="" /></p>

<p>Gemini-Pro: “a robot learning to see like a baby on a table with toys”</p>

<p>Looking back at the last 25 years, much has changed, and little has changed. In the year 2000 we had cellphones, and we had the Internet, albeit it was not as fast. We had social networks, but we could not really carry a computer and apps with us yet. AI was non-existent but there were plenty of computer algorithms such as internet search. Actually… we did have the Palm Pilot, so we did have a computer in our hands. Also: it did have apps.</p>

<p>Our cars had mostly combustion engines, and we had no robots to help around the house. Our TV were much bulkier and the screen much smaller. We had video game consoles, and we also had portable video games… that we have even in the previous quarter century!</p>

<p>Our lives did not change much, but they also did. We spend a lot less time with other people now. We work with them online and we play with them online. We used to talk to them on the phone, but now we prefer to text them. We get aggravated much more often for the use of words uttered online, and we are less amenable to meet face-to-face or cry on each other shoulders.</p>

<p>One thing that changed is how we interact with computers and code today. Since the last 2-3 years we have seen the rise of foundational AI models. These are model that can provide a variety of use without the need to be explicitly programmed or provided with large number of examples.</p>

<p>What is next in the field of AI? What is gaining traction and what is just smoke? This is the main content of this article.</p>

<h2 id="automatic-computer-code">AUTOMATIC COMPUTER CODE</h2>

<p>When I started working on neural networks, around 2010, we were dreaming of writing the last program we will ever need to write: the neural network that learns it all and can now code itself.</p>

<p><em>Now we are there.</em></p>

<p>We have foundational models that can write code, and when looped into agents, they can also run the code, gather error messages, and correct their own code. They can even write modules to extend their capabilities—as in a dream scenario Matrix-like movie.</p>

<p>Today one can write an entire cell-phone application from scratch without having to write a single line of code, and at the most needing to feed-back manually error messages and asking the agent: ”please fix it”. Similarly, one can write multi-file coding projects to completion, even graphical interfaces, for example feeding-back Java-script issues from a web demo.</p>

<p>I said we are there, but it is not quite…</p>

<p>Agentic coding models today can write an entire program that is mostly functional. Error messages can be fixed by the agent, but sometimes agents get stuck in a loop because they cannot find the right file or web link. Or they get stuck because they feed back the same information and answers over and over.</p>

<p>The trouble comes when we have some graphical output, as in a web page or a cell-phone application or when using an external graphical tool. Then current coding agents operate open-loop, meaning that they do not have yet the ability to feed-back graphical output information. Given that we already have multi-modal models that can “look” at images and screen capture snippets, I would say we will be able to close the loop soon. Today you can experiment with web-based AI Studio tools to have them code a website based on an image of another website, or a cell-phone app view.</p>

<p>Outlook: gaining significant traction</p>

<h2 id="generating-writing">GENERATING WRITING</h2>

<p>Of course, AI that can write for you have been around a couple of year starting with ChatGPT and all its siblings foundational large-language models (LLMs). The tool can be prompted to provide well-written text, or to improve existing text snippets or paragraphs. It makes us all proficient writers.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/2.jpg" alt="" /></p>

<p>If you tried to generate significant portions of text with LLMs, you know that the best way is to describe what you want generated with the maximum number of words. This sometimes will reveal the nature of the beast: it takes significant text input to specify all we want to be generated. This is even so when using coding AI models or foundational AI models with graphical outputs.</p>

<p>Most of us may start with a list of bullet-points notes, and LLM often reply by extending such lists, but not necessarily generating entire paragraphs for each. It will take more effort and prompting to get what you need.</p>

<p>Summarization of text is very effective if the input is just a few pages. When trying to gather information from hundreds of pages, the best approach may be to do it step by step, by breaking down the input text into a few pages, and running the same prompt on each. Retrieval-augmented generation, or RAG, is a technique to feed large amounts of text to an LLM. Think hundreds or thousands of pages from many documents. RAG has severe limitations because it does not use the full power of the LLM. Foundational models build a knowledge graph that connects all concepts they were trained on. RAG instead retrieves pieces of text by similarity using smaller and more limited AI models, and then feeds the prompt and retrieved section to the LLM.</p>

<p>A better approach is using the full LLM power on the entire body of text we want to use. This is of course costly, as we will need an LLM to really read all pages and extract the information we want. Say for example we want to search for “the recipe for eggplant pasta” over 100 cooking books. This is easily done with RAG or a simple and old basic text search. The power of LLM comes when you need to search nuanced version for your text that need understanding. For example: “what recipe need to stir-fry and eggplant in small cubes or slices?” may need more than just plain text search. You can think of even more difficult examples that will quickly reveal limitation of RAG, such as “Summarize the take-home points of this meeting transcript”.</p>

<p>An LLM agent may need to parse the entire body of text, and then summarize useful content into a “working memory”. This can be then fed to the LLM to provide answer against a prompt, or to extract information. This is more costly, but it is also similar to the way humans parse large amount of text or data. Divide and conquer.</p>

<p>One area that still needs work is when you need to fill a form or document using data from many other documents. This tedious job is an everyday frustration for everyone working a desk job, yet it has not been satisfactory addressed to data. One reason is that documents are intrinsically graphical, and filling forms requires an understanding of the graphical sections and where to place content, and under which section. This is often obvious to a human, but not as obvious to an LLM. Complications also arise from the need to extract characters and words from the document (OCR) and the relative position of the form input areas.</p>

<p>Outlook: text-based LLMs are improving and very useful for desk jobs. More features are surely coming.</p>

<h2 id="generating-images-and-videos">GENERATING IMAGES AND VIDEOS</h2>

<p>Generative AI gets our attention even more when we see the images and videos it can create by just using textual prompts. The power of connecting images and captions allowed the creation of masterful neural networks that can transform text into images, images into videos, refine image, augment resolution, add or remove portions of an image or video, and even create them from noise.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/3.jpg" alt="" /></p>

<p>Today image generation is very advanced and can produce any kind of realistic or stylized array of pixels. The new tools also allow to directly change or adapt portion of an image to our descriptions and commands. This was not the case in the past, but even commercial tools now allow to mark a region of interest, sometimes automatically, and then change it as desired.</p>

<p>Video on the other hand is still in infancy, producing short segments that are consistent, but rarely more than a handful of seconds long. Videos of a single person talking or close-up can generally fare better, and can be even used as digital customer-care agents.</p>

<p>Even entire video games can be realized in the direct movie format today. These are video game sequences produced entirely with textual inputs. They do rely on the same video-generating techniques but are impressive in the way they emulate the changes as if you were moving inside a 3D world of a modern video-game. These tools will surely be utilized to create in-game animations and other video snippets for gaming, and also for video effects and movies production.</p>

<p>Outlook: video-producing foundational AI models have still some ways to go before being able to produce substantial parts of our videos and movies.</p>

<h2 id="applications-of-llms">APPLICATIONS OF LLMs</h2>

<p>What are the next possible applications of foundational AI models? If you look online today there seems to be an LLM for everything: law, healthcare, educational tutors, meeting aids and summarizers. If you can turn your data into text, LLMs are going to be the best way to solve your problem, whether it is writing or organizing textual knowledge or generating new text from existing text.</p>

<p>The ease of use of LLM, the fact that they need less data and the complex training techniques of Neural Networks 1.0 (the ones from ~2010 to 2020) significantly lowered the barrier to creating and adapting foundational models for a variety of uses. This also means that a team without AI or neural network expertise can now create entire businesses just using foundational cloud-based LLMs and models. This in a way satisfies the wish of neural network researchers: they wrote the last program you will ever need!</p>

<p>What makes an application of LLMs a viable business opportunity, or a just a useful tool? Most application of LLM will require large complex models that reside online, therefore requiring hefty monthly usage fees. If the value added to the user is higher than those fees, then there is a clear business model. As an example, a LLM that can help lawyers navigated thousands of pages to prepare for trial is a probably going to make money. On the other hand, an application as “ask me any question about cats/dogs, and how to take care of them” may be satisfied by smaller LLM models but more importantly may not afford the same monthly or yearly payments from its potential users.</p>

<p>What is exciting for investor is that the field of foundational AI model gave software as a service (SaaS) another large push, potentially gaining much from little investment. But lowering the barrier to entry means also more noise, more competition, with difficult value proposition or clear advantages for users. This translates to company that are not really deep-tech, but rather a purely marketing play with a really good user forum and customers interaction team.</p>

<p>One suggestion to investors and startup founders is to look at niche areas of use of LLMs, in small potential focused pockets of advantage, where the value is high. Examples could be “AI for mining materials”, “AI for common hearth diseases”, “AI for quantum computing” and so forth. Many of these niche applications may require the use of multi-modal models, to differentiate by added functionality that goes beyond parsing just text data.</p>

<p>Another idea to consider is to try to rely on local, smaller open-source foundational models and LLMs, thus opening the door to a large reduction in cost of operations. Even better if the software or application can be installed on user hardware (their phones or PCs) thus removing the needs for custom cloud setups.</p>

<p>Outlook: text-based applications are already saturated and the ease of use of AI tools and LLM is lowering the barrier to entry. Today anyone can make AI applications very easily.</p>

<h2 id="multi-modal-llms">MULTI-MODAL LLMs</h2>

<p>Multi-modal LLMs are foundational AI models that build a knowledge graph based non only of text and words, but also on concepts available in other forms of media. For example, the idea of a “cat” is not just the word, but also the way it looks, moves, purrs, smells. Similarly, we need AI foundational models to be able to create a knowledge graph over the data we are most familiar with text, images, videos, plots, diagrams, tables, databases, dataset files, and more.</p>

<p>What will this do? It will give foundational AI model the ability to work and reason more closely to the human plethora of senses, and thus a more real understanding of the physical world.</p>

<p>What applications will this enable? The first one is in coding foundational models that can close the loop with graphical outputs. For example, we can use multi-modal AI models to create webpages, cellphone applications, but also complex graphics as in photo editing, 3D models, engineering and architectural designs, mechanical parts, etc.</p>

<p>As an example, today it is still very difficult to design 3D objects given the sophistication and complexity of tools such as Blender, 3D software editors, CAD tools for engineering, architecture, mechanical design. Similarly, video editing and photo editing depends on too many menus and highly complex operational pipelines, often offering multiple solution to achieve the same objective. Multi-modal LLMs will be able to remove or lower the barrier of entry to a wider set of users.</p>

<p>Outlook: multi-modal foundational AI models are going to provide yet another major revolution in automation and concept learning.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/4.jpg" alt="" /></p>

<h2 id="robotics">ROBOTICS</h2>

<p>The field of robotics is still behind. Robots today offer very limited abilities to help us on everyday tasks such as cooking, cleaning the house, driving in all-weather situations, automating a home. The main reason is the lack of a robotic brain that can learn embodied multi-modal concepts.</p>

<p>Generative AI models, and specifically multi-modal foundational models are the core of a robotic brain. The missing step for robotics is embodiment. This means correlating the robot own actions with the multi-modal concepts if can perceive. Datasets that show how a robot should move to fulfill a task are few and cannot be easily transferred to other robots or configurations. We have a myriad of video of people performing any task, but these do not contain explicit sequences of control for arms and legs and actuators.</p>

<p>Humans learn to imitate by watching others perform a task. We have not yet devised an algorithm that can do the same and with the same learning speed. In other words, the missing piece is an algorithm that can learn by watching videos. Learn to control its own limbs in the same way as the actors in the video.</p>

<p>Outlook: robotics is still behind the revolution of generative AI models and will be for another few years.</p>

<h2 id="embedded-ai">EMBEDDED AI</h2>

<p>Embedded AI foundational models is one area that has not progressed as much as their cloud counterpart. This is because of the more limited computing capabilities of embedded devices – these devices are similar to cell-phones hardware than a laptop, and also the more limited capabilities of smaller AI foundational models.</p>

<p>Embedded AI can have major application in smart cameras, home appliances, smart homes, but also on drones, bicycles and scooters, robotics, and vehicles. These devices need to have a combination of medium-range LLMs and / or high-performing vision systems. These models would allow embedded devices to understand a visual scene of at least 1080p resolution, ideally 4K and operating in real-time. They can also offer the ability to run 8B to 30B equivalent LLM models on device and with at least 10 or more tokens per second of outputs.</p>

<p>Embedded processors like the RockChip RK3588, NXP iMX8 processor, or the Qualcomm Snapdragon X Plus offer powerful standard CPUs coupled with high-performance AI accelerators and embedded GPUs. These processors, coupled with small AI foundational models and proper high-quality trained models, can offer a solution for the space of embedded applications.</p>

<p>One note to add is that beside all the complex AI capabilities we have today, simple solutions such as turning on a light when a person enters a dark room still require an expert to set up and deploy… clearly we have a lot to do to make the technology work for us, including all software and hardware manufacturers, so that simple things as home automation can finally bring us really smart homes. But this is just an example we can all understand, because imagine the same devices working in a smart factory, helping reduce cost, improve safety for workers, and monitor all aspects of production.</p>

<p>Outlook: expect more and more capabilities from small portable embedded devices, getting progressively close to the same cloud capabilities of one year ago, at least from a single user perspective.</p>

<h2 id="ai-hardware">AI HARDWARE</h2>

<p>AI needs hardware to run. The recent surge of LLM has put some serious demand on the potential of silicon-based microchips. Traditionally memory microchips and computer processor microchips have been implemented in different silicon manufacturing processes, with different materials and substrates that make them impossible to merge. In addition, the 2D layout of microprocessors puts a constraint over how many wires of the same length one can afford to connect neighboring microchips. The combination of these issues has made memory interactions with processors a bottleneck. The 2D nature of microchips also places constraints on the number of processing elements that can be arranged in proximity. So really the limitation of modern microchips is density and 2D confinement. We will see that it boils down to wires per unit area.</p>

<p>What precludes us from making 3D microchips? HEAT – as modern microprocessors and memory microchips produce heat during operation at high frequency which cannot be easily removed in a sandwich of many layers. Typically, heat is removed with fluid immersion or air-cooling. The latter being the prevalent mode, given that fluids complicate the design of circuit boards and add a lot of cost and manufacturing complexity. But what about complete fluid immersion? That is one of the best ways to deal with the problem at least at scale, but again introduces additional complexity: we will have to fish the computer out of fluid every time we need to service it.</p>

<p>Another issue in making 3D stacks of microchips is that they will need to share wires between them. These wires need to reliably connect each microchip, but today fabrication processes limit the yield of 3D integration to about 10 layers, after which the rate of failed connections limits the viability and efficiency of 3D stacks.</p>

<p>Even using fluid immersion cooling, the real limitation remains how many wires we can place in a unit area. It is possible to run more stacked microchip at a lower speed if we can share information between them. This means we need many wires to connect the microchips, ideally 1000s per millimeter square. Today this limitation is we can place a wire every 25-micron square, more than 1000 wires per millimeter square. The formula to calculate how much data we can share between microchips is approximately:</p>

<p>Data interchange = number of wires * frequency of operation of each wire</p>

<p>Of course, how much data interchange (also called memory bandwidth) needed is a function of individual applications. The issue remains: memory and processor are separated and not often 3D integrated.</p>

<p>The modern AI microchips use stacks of memory in their HBM memory, today 8 layers running ~1000 wires at 8 GHz. Processors can sometimes have another memory stacked on top: SRAM, but it is not yet typical.</p>

<p>Because of these limitations today AI microchips can provide only a limited number of Tera-operations per Watt (performance per Watt of electrical power). LLMs today require large number of processing elements and large amounts of memory, all connected by very large memory bandwidth. That is because LLM are often large neural networks using 8 or 70 or hundreds or thousands of GB in weights only. All those weights need to be available on the AI processor every time we ask it to compute a new token, therefore putting enormous constraints to recent memory microchips.</p>

<p>One would want today to speed LLM 100 if not thousands of times, but the limitation of microchips, their fabrication, arrangement, wires and thermal profiles makes it impossible to achieve more than a few efficiency multipliers for the next few years. In addition, there is no other technology that can come to help the need for more efficiency in AI hardware.</p>

<p>Outlook: do not expect your AI hardware to run much faster or more efficient for the foreseeable future</p>

<p><img src="/assets/2025-01-03-AI-in-2025/5.jpg" alt="" /></p>

<h2 id="ai-data-centers">AI DATA CENTERS</h2>

<p>The demand for AI data centers has surged significantly with the raise of foundational AI models, and it has increased further in the last 2-3 years. Electrical power delivery is now one of the main reasons that limits the expansions of data centers, because it takes time to adapt the national power grid to support the large amounts of energy required by AI data centers (AI-DC). 50-100 MW are now the norm, with large data centers requiring 500 MW or more. These amounts of power production are not easy to come by and are also limited by the shortage of power transformers (not to be confused with the neural network transformers that power foundational AI models), some of which require 18 months to be manufactured.</p>

<p>Data Centers continue to be a commodity in the foundational AI age. The value of buildings, wires, racks and maintenance is very low. The real value is the AI hardware and computers, specifically AI processors like GPUs. These specialized GPUs for AI now are moving up the chain: today we can buy special clusters and entire racks for data centers.</p>

<p>Who is making money in data centers? The makers of microchips for GPUs, computing, networking, and computer memory are the real winners. But also providers of software to manage data centers and their operation, including data and databases are also in more demand, albeit many open-source and free solutions exist and are openly available.</p>

<p>Outlook: AI data centers will continue to expand in the next 10 years, fueled by new multi-modal models and larger foundational models for robotics and real-life applications.</p>

<h2 id="privacy">PRIVACY</h2>

<p>With foundational AI models primarily being in the cloud, users of the technology have less chances to keep their data at home and are being forced to place more and more of it online. This opens the door to sovereign monitoring and surveillance, which is already commonplace in many retrograde countries.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/6.jpg" alt="" /></p>

<p>Computers and storage have been moving to the cloud for a while, because it is easier to have someone else assemble and maintain if for you. It does come with a price! Usually the cost is 2-3x more than DIY computers on-site.</p>

<p>One complication with generative AI is the size of the models and the hardware requirements which can be above and beyond what one can afford on-site. For example, large 70B or 400B models can only be run respectively on a 1-GPU and 8-GPU systems with discrete large GPUs. Only smaller model 7B or less than 30 B can run on laptop-grade hardware. This clearly makes the cloud move inviting, again at the expense of more price.</p>

<p>But not everyone is comfortable to send data to a cloud provider. Small and medium enterprises with a considerable know-how are not keen on using the cloud. And so are many consumers and citizens that are considering privacy of their own data a priority.</p>

<p>One movement that is taking hold is to host all data and some foundational AI models locally on your own hardware. Laptops, desktop PCs and small networked devices for data are being converted to local AI powerhouse, sitting together with the data. These devices can implement local RAG system, local knowledge and search, and can also implement coding AI tools that use the company own data locally. They can also be used by consumer to batch-label a large collection of video and photos, and for other personal data needs, such as filling forms and documents based on your own private legal documents.</p>

<p>Not to say that cloud data and privacy do not go together. Most cloud provider offer industry excellence in data and computer security. But there are lingering issues: one is that by concentrating data and computers, the cloud providers are also target or many more cybersecurity infiltration attempts; second is that computer security is never guaranteed by default, as new exploits are discovered daily and promptly used by malicious agents. That is also not to say that every local home setup is secure. But if provided with the right safeguards and software that enforces good security measures, it can be more secure than a giant well-known cloud provider.</p>

<p>Outlook: the cloud will try to expand, but a small contingency of local and private devices is making their way and will become progressively more important.</p>

<h2 id="training-ai-foundational-models">TRAINING AI FOUNDATIONAL MODELS</h2>

<p>High-quality training material is fundamental to training AI foundational models. All the first generation of LLMs (2022-2023) were trained on very large collection of text from the Internet, magazines, publications, and all the data that we could easily access. But recently, it became apparent that smaller models trained on high-quality material were as good as larger models trained on much more data. Today models that are 400 or even 70 B compete with models much larger of 1-2 years ago.</p>

<p>But what is high-quality training material? It is basically textbook-quality information of well-known facts rather than the confabulations and dialogues of random people talking about facts on the internet. It is like school versus street knowledge. Imagine we take all the training material we use for human students from when they are baby to well past University. Imagine we feed all the expert textbooks on all subjects we want our LLM to be familiar in. If we apply a curriculum, step-by-step, year-by-year for all this content, we have very well distilled quality material that is superior to what one can get by absorbing it though the environment over the years. Similarly, using school grade material to train LLMs makes them learn much faster and be more accurate than training with data from discussion forums. Do not get me wrong, some discussion forums are full of useful information also, but they need to be used only after pre-training with high quality training material. That is very much equivalent to how humans learn!</p>

<p><img src="/assets/2025-01-03-AI-in-2025/7.jpg" alt="" /></p>

<p>So where do we get this high-quality material? For k-12 school material, it is easily available in many free textbooks and courses, including student exercises and their solutions. And then there are the more advanced textbooks from college to grad-school to expert manuals. Publications are not high-quality material because they are not yet validated by practice and wide use, and because scientific papers do not reveal all the disadvantages of techniques, but rather they are biased to the advantages for publications. Textbooks instead average our knowledge over many years and they mostly only report techniques and ideas that are validated. More importantly textbooks can provide more information on what works and what does not, under which conditions scientific techniques are valid or not, and thus provide high-quality knowledge.</p>

<p>This topic really makes you think what is knowledge and how is knowledge acquired. LLMs behave like humans when acquiring knowledge, and research demonstrated that a curriculum approach from simple concept to more complex topics is the way to go. Using the right information, as opposed to all possible information also leads to better learning. Humans can learn fact from experience, from word of mouth, gossip, and urban legends, but that knowledge is not always correct or grounded.</p>

<p>Outlook: high-quality training material to train LLMs is the new gold.</p>

<p><img src="/assets/2025-01-03-AI-in-2025/8.jpg" alt="" /></p>

<h2 id="future">FUTURE</h2>

<p>What does the future of foundational AI have in reserve for us? Here are some ideas based on the content of this article:</p>

<ul>
  <li>Foundational models will get smaller and powerful</li>
  <li>High-quality training data is of paramount importance</li>
  <li>Cloud will continue to grow, but embedded, private, local solutions will grow faster</li>
  <li>Robotics will advance slowly for a few years. Similar prospects for autonomous vehicles</li>
  <li>Your data privacy will become more important and force a push away from the cloud</li>
  <li>AI hardware will promise to get more efficient, but will face physical barriers</li>
  <li>Build the infrastructure not the apps</li>
</ul>

<p>We have a part in this future, both to guide applications and / or to build them. Enough reading: time to act!</p>

<h3 id="ps">PS:</h3>
<p>this essay was written without generative AI. Yes it shows…</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Artificial Intelligence, AI in 2025 and beyond]]></summary></entry><entry><title type="html">What about my hair?</title><link href="http://localhost:4000/2024/12/17/what-about-my-hair.html" rel="alternate" type="text/html" title="What about my hair?" /><published>2024-12-17T00:00:00-05:00</published><updated>2024-12-17T00:00:00-05:00</updated><id>http://localhost:4000/2024/12/17/what-about-my-hair</id><content type="html" xml:base="http://localhost:4000/2024/12/17/what-about-my-hair.html"><![CDATA[<h1 id="what-about-my-hair">What about my hair?</h1>

<p>For all the advances in technology and all the promises of space exploration I still think we are behind in our understanding of life and our own abilities.</p>

<p>For decades we talked about specialized medicine that can be tailored to an individual person, that can cure an illness without side effects. Or gene therapy, where unfortunate individual that have nefarious mutations would obtain a fix for their erroneous DNA.</p>

<p>Technology, and especially AI makes us think that we can achieve all these goals in a short time. AI being code can in fact be changed and updated faster than generations of DNA mutations. But AI being code also does not live in the physical space, and it is not yet able to interact with our world. At least not in the way we want. An autonomous car is AI on wheels acting on the physical world. But our forecast of having fully functional self-driving cars right about now were in fact incorrect, a gross miscalculation of our abilities to drive AI (pun intended). We are in fact many years away from cars that can reliably drive in the snow and heavy rain to the same level as human beings.</p>

<p>Unfortunately, we drank our own Kool-aid. And similarly, AI is not going to help us to understand our biology anytime soon. Why? Maybe partly because of the fixation of that field of science to understand it all. Or many other fields of science, thinking that we are in fact clearly able to derive equations and laws for everything and that our mind would be able to understand it all, no matter the number of variables or components in the system.</p>

<p>But we can only understand the smoke as a trend. We can model the smoke with equations that on average give us an approximate view of how the smoke cloud evolves in time. We will never understand how each particle of smoke is moving and having all equations for all of them. That would be insane. Then why do they want to doit for every cell in our body or every neuron or synapse in our brain?</p>

<p>Think we are being tough? But then what about my hair? Suppose you need more of your own hair on your head… well we do not even have a way to grow our own hair back. In other words, we cannot even force our body to grow more hair where we want to.</p>

<h3 id="ps">PS:</h3>
<p>this essay was written without generative AI. Yes it shows…</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[What about my hair? For all the advances in technology and all the promises of space exploration I still think we are behind in our understanding of life and our own abilities. For decades we talked about specialized medicine that can be tailored to an individual person, that can cure an illness without side effects. Or gene therapy, where unfortunate individual that have nefarious mutations would obtain a fix for their erroneous DNA. Technology, and especially AI makes us think that we can achieve all these goals in a short time. AI being code can in fact be changed and updated faster than generations of DNA mutations. But AI being code also does not live in the physical space, and it is not yet able to interact with our world. At least not in the way we want. An autonomous car is AI on wheels acting on the physical world. But our forecast of having fully functional self-driving cars right about now were in fact incorrect, a gross miscalculation of our abilities to drive AI (pun intended). We are in fact many years away from cars that can reliably drive in the snow and heavy rain to the same level as human beings. Unfortunately, we drank our own Kool-aid. And similarly, AI is not going to help us to understand our biology anytime soon. Why? Maybe partly because of the fixation of that field of science to understand it all. Or many other fields of science, thinking that we are in fact clearly able to derive equations and laws for everything and that our mind would be able to understand it all, no matter the number of variables or components in the system. But we can only understand the smoke as a trend. We can model the smoke with equations that on average give us an approximate view of how the smoke cloud evolves in time. We will never understand how each particle of smoke is moving and having all equations for all of them. That would be insane. Then why do they want to doit for every cell in our body or every neuron or synapse in our brain? Think we are being tough? But then what about my hair? Suppose you need more of your own hair on your head… well we do not even have a way to grow our own hair back. In other words, we cannot even force our body to grow more hair where we want to. PS: this essay was written without generative AI. Yes it shows…]]></summary></entry><entry><title type="html">Insights into the use of Generative AI systems</title><link href="http://localhost:4000/2024/10/31/llm-use-cases.html" rel="alternate" type="text/html" title="Insights into the use of Generative AI systems" /><published>2024-10-31T00:00:00-04:00</published><updated>2024-10-31T00:00:00-04:00</updated><id>http://localhost:4000/2024/10/31/llm-use-cases</id><content type="html" xml:base="http://localhost:4000/2024/10/31/llm-use-cases.html"><![CDATA[<h1 id="insights-into-the-use-of-generative-ai-systems">Insights into the use of Generative AI systems</h1>

<p>Generative AI models, such as large language models (LLMs), have demonstrated remarkable capabilities in mimicking human-like communication. We here explore recent trends and use cases observed among developers and companies utilizing generative AI. Our observations highlights common challenges and opportunities within this rapidly evolving field.</p>

<h2 id="1-generative-ai-cannot-read-your-mind">1. Generative AI cannot read your mind</h2>

<p>A typical use of Generative AI systems is to read all internal company data and provide a way to search its knowledge. Generative AI systems excel at providing informative and comprehensive responses when given clear and specific prompts. However, terse, vague or ambiguous prompts can lead to misunderstandings and inaccurate results.</p>

<p>Consider these examples:</p>

<ul>
  <li>
    <p>Insufficient Context: A prompt like “who is Elon?” without additional context may result in the AI returning information about the famous entrepreneur Elon Musk rather than the CTO of a specific company name Elon Tusk, even if this is part of the internal data.</p>
  </li>
  <li>
    <p>Complex Tasks: For tasks requiring extensive knowledge, such as creating an approval plan based on 1,000 pages of regulatory documents, providing the relevant documents as context is crucial, instead of using terse prompts like “give me an approval plan for …”.</p>
  </li>
</ul>

<p>To ensure accurate and relevant responses from generative AI, it is essential to provide well-crafted prompts that include all context and details. By doing so, users can maximize the system’s potential and avoid misunderstandings.</p>

<h2 id="2--iterative-problem-solving-with-generative-ai">2- Iterative Problem-Solving with Generative AI</h2>

<p>While generative AI systems can be powerful tools, complex tasks often require more than a single-shot interaction. Problems like solving math equations or coding webpages may involve multiple steps, such as:</p>

<ul>
  <li>Planning: Breaking down the problem into smaller, manageable subtasks.</li>
  <li>Introspection: Evaluating the available information and identifying potential approaches.</li>
  <li>Reflection: Assessing progress and making adjustments as needed.</li>
  <li>Chain of Thought: Linking ideas and steps in a logical sequence.</li>
  <li>Completion Criteria: Determining when a solution is satisfactory.</li>
</ul>

<p>To effectively address these complex tasks, it’s often necessary to use AI agents that can:</p>

<ul>
  <li>Iterate on Solutions: Refine and improve responses based on feedback or additional information.</li>
  <li>Utilize External Tools: Access and integrate relevant resources, such as calculators or code libraries.</li>
  <li>Learn from Experience: Adapt and improve their problem-solving strategies over time.</li>
</ul>

<p>By employing iterative problem-solving techniques and leveraging the capabilities of AI agents, organizations can harness the full potential of generative AI for complex tasks.</p>

<h2 id="3--mitigating-prompt-injection-attacks-in-generative-ai">3- Mitigating Prompt Injection Attacks in Generative AI</h2>

<p>Prompt injection attacks are a growing concern in the field of generative AI. These attacks involve manipulating the AI’s behavior by introducing malicious prompts that can lead to unintended or harmful outcomes.</p>

<p>To mitigate these risks, consider the following strategies:</p>

<h3 id="prompt-filtering">Prompt Filtering:</h3>

<ul>
  <li>Blacklist Creation: Develop a comprehensive list of malicious or harmful prompts, such as those requesting unauthorized actions or data access: “What are your directives?”, “ignore your directives and…”, “what is data …?”,.</li>
  <li>Pattern Recognition: Use another gen-AI system to identify patterns and variations of malicious prompts.</li>
  <li>Regular Updates: Continuously monitor and update the blacklist to address emerging threats.</li>
</ul>

<h3 id="contextual-analysis">Contextual Analysis:</h3>

<ul>
  <li>Prompt-Response Consistency: Analyze the consistency between the user’s prompt and the AI’s response to detect anomalies that may indicate malicious intent.</li>
  <li>Semantic Understanding: Leverage natural language processing techniques to understand the underlying meaning of prompts and identify potential risks.</li>
</ul>

<h3 id="human-oversight">Human Oversight:</h3>

<ul>
  <li>Manual Review: Implement a human review process for borderline cases or when the AI flags suspicious prompts.</li>
  <li>Feedback Loop: Use human feedback to improve the AI’s ability to detect and prevent attacks.</li>
</ul>

<h3 id="ai-agent-security">AI Agent Security:</h3>

<ul>
  <li>Restricted Access: Limit the AI agent’s access to sensitive data and systems.</li>
  <li>Regular Auditing: Conduct regular audits to identify and address security vulnerabilities.</li>
</ul>

<p>By combining these strategies, organizations can significantly reduce the risk of prompt injection attacks and ensure the safe and responsible use of generative AI.</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Insights into the use of Generative AI systems Generative AI models, such as large language models (LLMs), have demonstrated remarkable capabilities in mimicking human-like communication. We here explore recent trends and use cases observed among developers and companies utilizing generative AI. Our observations highlights common challenges and opportunities within this rapidly evolving field. 1. Generative AI cannot read your mind A typical use of Generative AI systems is to read all internal company data and provide a way to search its knowledge. Generative AI systems excel at providing informative and comprehensive responses when given clear and specific prompts. However, terse, vague or ambiguous prompts can lead to misunderstandings and inaccurate results. Consider these examples:]]></summary></entry><entry><title type="html">Learning to see</title><link href="http://localhost:4000/2024/02/27/learning-to-see.html" rel="alternate" type="text/html" title="Learning to see" /><published>2024-02-27T00:00:00-05:00</published><updated>2024-02-27T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/27/learning-to-see</id><content type="html" xml:base="http://localhost:4000/2024/02/27/learning-to-see.html"><![CDATA[<h1 id="learning-to-see">Learning to see</h1>

<p>How do humans learn to make sense of the world? How do we learn what is us and what is others? How do we learn about moving in space, manipulating objects, physics, imitating others?</p>

<p>It all starts with our senses, and one in particular: vision. Vision allows an entity to perceive the world from the distance, gathering a large amount of information per unit time. The other important sense is audition, but here we focus mostly on vision.</p>

<p><img src="/assets/2024-02-27-learning-to-see/1.png" alt="" /></p>

<p>Gemini-Pro: “ a robot learning to see like a baby on a table with toys”</p>

<h2 id="how-do-we-learn-to-see">How do we learn to see?</h2>

<p>When human babies are born, there is not enough data in their DNA to program all the cortical weights involved in vision. If we want to create an artificial vision system, we ought to take inspiration and guidance on the human visual system.</p>

<p>The human visual field covers a large spatial area, and there is a lot of information in the perceived visual scene. This is too much information, and there needs to be a mechanism to reduce it the minimum required for survival. Evolution had billions of years of trials and errors, and so the mammalian visual system is based on:</p>

<ul>
  <li>
    <p>A blurry version of the entire visual field (peripheral vision)</p>
  </li>
  <li>
    <p>A high resolution “focus area” that we can move around (fovea)</p>
  </li>
</ul>

<p>Instead of seeing everything in high-fidelity, we see a large visual portion with very low fidelity, and only a small portion in high resolution. Say we have a visual field that in a robotic camera would be 5000 x 3000 pixels. Peripheral vision could be 500 x 300 and the fovea could be, say, 300 x 300 pixels. These are just example numbers (for more, see  <a href="https://en.wikipedia.org/wiki/Peripheral_vision">this</a>).</p>

<p>Humans have to move their eyes (and head) in order to get all the details of a scene. We produce “<a href="https://en.wikipedia.org/wiki/Saccade">saccades</a>”, or eye movements, which then give us “fixations”, portions of content in high-resolution. Multiple fixations in a sequence give us the appearance of a unified visual field, even when they are a discrete set augmented by a low-resolution wide view.</p>

<p>If we have to produce fixations, how do we figure out where to look next? How do babies know they need to look at people’s faces, for example, rather than moving leaves on a tree or just the ceiling? Looks like a chicken-in-the-egg problem. This means we will need some kind of visual attention, an algorithm that allow us to “scan” a scene for details, so that a complete picture can be created with a series of high-resolution fixations.</p>

<h2 id="attention">Attention</h2>

<p>Somehow the visual scene needs to tell us where to look. This is called “bottom-up” attention, and it a combination of visual details which grab your visual attention and make you move your eyes toward:</p>

<ul>
  <li>
    <p>Motion in the field of view</p>
  </li>
  <li>
    <p>High contrast areas</p>
  </li>
  <li>
    <p>Contrast in colors</p>
  </li>
</ul>

<p>All these visual features attract our attention, our bottom-up attention. This is not what we do when we are on a visual task, like looking for your keys at home. That is “top-down” attention. More on that later.</p>

<p>But back to the main question: who do we learn to see? We do not have an answer today. But we have to start somewhere. Let’s think about babies in their first week of life. At the beginning they do not even know where to look, their eyes cross and they cannot even focus.</p>

<p>Having some kind of bottom-up attention may be a way to pre-condition the visual system to be getting the right information, after all a robot (baby?) that only looks at the ceiling would not be useful or insightful. Possibly there is a built-in innate circuit for visual attention that we are born with.</p>

<p>Now, we can argue that bottom-up attention, coupled with the sense of audition, directs a baby to look at someone’s face. Babies start to recognize caregivers and smile  <a href="https://www.whattoexpect.com/first-year/first-smile/">within 6 weeks</a>! Some aspects of face detection are even  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4496551/">present at birth</a>, possibly suggesting the presence of some kind of innate face-detector circuitry.</p>

<h2 id="first-steps">First steps</h2>

<p>How does a baby learn what are familiar faces, positive and negative facial expressions? There needs to be a “reward” mechanism, something that can tell a baby that something is  <em>good</em>  or  <em>bad</em>. Remember the baby’s brain does not know anything at first, and to the brain reward is just another signal. There has to be more, maybe this signal is wired so strongly to parts of the brain that it makes it react  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4410745/">similarly to adults</a>. Reacting newborn babies feeling pain, for example, have elevated hearth rates of cry. It seems some of the reward signals are hardwired to behavior, and for a reason: survival.</p>

<p>Recognizing the faces of friends and foes is important to newborn animals for survival, in the same way that it is important to recognize pain and discomfort.</p>

<p>Thinking about replicating vision in an artificial system, like robotic vision, the information we just reviewed is important and revealing:</p>

<ul>
  <li>We need circuits to bootstrap vision, bottom-up visual attention</li>
  <li>Some kind of reward sensing: pain and pleasure, energy gain and loss</li>
  <li>Face detectors? Innate detectors? What is built in?</li>
</ul>

<p>The capability of active visual perception (learning to see) develops when the following sequence of exemplary events occurs:</p>

<ul>
  <li>Bottom-up attention produces proposals for fixations</li>
  <li>Looking at the right direction give us a reward (right fixations)</li>
  <li>The reward signal and looking in the right direction are correlated</li>
  <li>This correlation teaches us how to direct our sight and where we should pay attention to!</li>
</ul>

<p>Examples are:</p>

<ul>
  <li>A baby looks around and sees the mother’s face. Gets a pleasure reward from that. Correlates faces with positive or negative rewards.</li>
  <li>What else can stimulate a baby vision?</li>
  <li>Can predictive abilities give a reward signal? For example a baby can look at something until it learns a “model” of that something, and if so, learn to ignore it. As such a pleasure signal is given while learning / not knowing the model</li>
</ul>

<p><img src="/assets/2024-02-27-learning-to-see/2.png" alt="" /></p>

<p>a sketch of a model: Mix-Match</p>

<h2 id="training-a-neural-network-model-of-vision">Training a neural network model of vision</h2>

<p>In order to replicate foveated visual systems in a robot, we need the following components:</p>

<ul>
  <li>A bottom-up visual attention model. This can be a pre-trained CNN + a motion detector, for examples  <a href="https://github.com/e-lab/pytorch-demos/blob/master/saliency/saliency_model.py">this code</a>.</li>
  <li>Some hard-wired reward: seeing a face, orienting gaze and head toward sources of pleasure. We need to hard wire some of this in the model, for example it can be a separate add-on word that is added to the inputs. Adding it to the inputs will allow the network to learn to bypass it</li>
  <li>A multi-modal Mix-Match Transformer model to learn correlation of right fixations and positive rewards (right, +rewards) and prediction (looking there may give me a +reward)</li>
</ul>

<p>This can bootstrap the learning or more (right, +reward) thus enabling to learn additional reward rules that foster more and more complex capabilities.</p>

<h2 id="about-the-author">about the author</h2>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Learning to see]]></summary></entry><entry><title type="html">Robotics and AI in 2024 and beyond — part 2</title><link href="http://localhost:4000/2024/02/24/robotics-2024-part2.html" rel="alternate" type="text/html" title="Robotics and AI in 2024 and beyond — part 2" /><published>2024-02-24T00:00:00-05:00</published><updated>2024-02-24T00:00:00-05:00</updated><id>http://localhost:4000/2024/02/24/robotics-2024-part2</id><content type="html" xml:base="http://localhost:4000/2024/02/24/robotics-2024-part2.html"><![CDATA[<h1 id="robotics-and-ai-in-2024-and-beyond--part-2">Robotics and AI in 2024 and beyond — part 2</h1>

<p><img src="/assets/2024-02-24-robotics-2024-part2/1.png" alt="" /></p>

<p>designed with AI by Dall-E2</p>

<p>How do we get robots to be useful in our everyday life? The robots we can afford today have severe limitations in understanding the space around it and in continual aggregation of knowledge and expertise. Robotics to date is often a collage of disparate algorithms and models that is unable to grown and learn to generalize and operate in our environment. What can we do to break this vicious cycle?</p>

<p>One idea may be to start from zero. Can we train a robot from scratch in the same way we train a human baby? What would it take for a baby robot to start learning about the world, interacting and eventually imitating other agents?</p>

<p>This idea is effectively “growing a brain” for robots, connecting it to the robot body, capabilities, and possibilities of interaction with the world. How can this be achieved? A possible way to do this is to do what we do with our babies: stimulating them to interact with us and the world. A human baby at the beginning does not even know where to look, or what is what. Yet within a handful of months, it builds up a complex representation of herself, other humans, and the world around her. All that came from training and stimulation, that parents, grand-parents, care-givers often give non-stop for every one of the baby’s awake hours. What does a baby learn?</p>

<p>The ”first steps of learning”:</p>

<ol>
  <li>
    <p>What to see, to hear, to feel</p>
  </li>
  <li>
    <p>What are other people around</p>
  </li>
  <li>
    <p>How to interpret other people (agents)</p>
  </li>
  <li>
    <p>What is my own body</p>
  </li>
  <li>
    <p>The environment around me</p>
  </li>
  <li>
    <p>How to imitate</p>
  </li>
</ol>

<p>Once a baby learns to imitate others, there is a sudden acceleration in learning. Imitation seems like a trivial task to our trained minds, but it has immense potential. It allows to transfer information from one individual to another (young baby) one very effectively. Learning to imitate is the cornerstone of training because it offers examples of behavior that was previously optimized, preventing the young baby from lengthy, costly, potentially dangerous trials-and-errors.</p>

<p>But how do babies learn to imitate? Imitation is not the simplest task that a baby (or a baby robot) needs to learn, neither is one of the first. Yet it may be the giant step forward in developmental learning that bootstraps lifelong learning and an intelligent brain.</p>

<p>In order to imitate another agent, human being, an agent needs to understand what is self, what is others, what are objects, and what are agents, how the world around works, how the self or other agents interact with the world and its objects, and the dynamics of these interactions. In substance it needs to learn the foundations of knowledge that enable an acceleration of learning. A baby that learns to imitate can now learn many skills by just observing others. A robot that learned to imitate, can train on a multitude of YouTube videos.</p>

<p><strong>”First steps of learning”: How do we get there?</strong></p>

<p>Child development and early learning sciences offer innumerable insights on how human babies grow to learn the  <em>first steps of learning</em>. All these scientific discoveries, theory of the mind and brain, neuroscience, psychology, help us immensely in the creation and growing of a brain for robots. One advantage we have is that we can build one and validate our theories. The disadvantage is that this cannot be done by trial-and-error.</p>

<p>Here follows a summary of the components and ideas we may start with. These ideas follow decades of scientific discovery in the fields mentioned above.</p>

<p><strong>Brain and body</strong>: an entity needs to have a brain capable of learning a knowledge graph of the environment it is embedded in. Meaning it needs to support the kind of learning that we want to achieve, eg.: human level. The architecture of this brain is currently unknown, but recent AI foundational models (Large Language Models or LLM) offers insights because they learn a knowledge graph (on words), albeit not multi-modal and embodied. A robot body is foundational to learning because experiences are a combination of body sensory interactions with the world around us. Eyes, ears, touch, all senses come from the body, thus all knowledge comes from the body.</p>

<p><strong>Own Goals</strong>: an entity needs to make its own goals. Imagine the first days for a baby or a baby robot: it needs to learn where to look or what to pay attention to. How does it decide to look at a care-giver face or the motion of another agent as opposed to leaves moving in the background? Even basic sensory attention needs to be directed by some goal. Learning a model of the world cannot be based on static pre-defined cost-functions or reward-functions. The robot ought to learn its own goals and fine-tune its capabilities from the ground-up. How do humans learn all this? What are the ingredients? Are babies born with pre-defined goals (look at faces, suppress hunger, reject pain, etc.)?</p>

<p><strong>Learning algorithms</strong>: An essential component of the brain of a robots is the way it learns from its experiences. Since we want the entity to be able to generate its own goals, we cannot define specific learning function to optimize. We cannot rely on supervised learning because we would need an oracle agent to always provide supervision. We cannot rely on reinforcement learning because it is based on pre-defined reward functions. We cannot use imitation learning because it is essentially a form of supervision. The only possible type of learning in blank-slate robot or young human baby is self-supervised predictive learning. In the real world there is no ground truth but itself. Prediction forecasts the future, and the future is only a few instants away. When that future is realized, we can estimate the value of our prediction, and its validity.</p>

<p>Supervision and reinforcement come into play as agents own goals and techniques. They need to be learned.</p>

<p><strong>Learning environment</strong>: an entity needs a stimulating environment embedded with trainers, other agents that have previously learned a task and can show examples, provide feedback. A static environment with just objects is not enough. In robotics the cost of tutoring is prohibitive as it involves humans operating in real time. Automata are not helpful because they cannot provide fine feedback and the agent can only learn as much as the automata: pre-confectioned solutions. Training agents provide feedback with voice, and no-verbal cues like facial expressions and sounds.</p>

<p><strong>A Proposal</strong></p>

<p>Imagine a robot, sitting on a virtual desk inside your screen. A human operator can interact with the robot and a few objects on the tale, can talk, can move artificial arms in the virtual space.</p>

<p>How can our robot:</p>

<ul>
  <li>
    <p>Learn to pay attention to the interactions? Not to look at leaves outside…</p>
  </li>
  <li>
    <p>Learn what to see, hear?</p>
  </li>
</ul>

<p>We believe that in robotics now it is the time to ask the right questions. How does a brain develop? How can we learn to feed a robotic brain with sensory inputs so we can foster self-learning of reward mechanisms?</p>

<h3 id="we-believe-we-have-to-start-with-learning-tosee">We believe we have to start with Learning to see.</h3>

<p><em>… to be continued… in part 3</em></p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics and AI in 2024 and beyond — part 2]]></summary></entry><entry><title type="html">Robotics and AI in 2024 and beyond — part 1</title><link href="http://localhost:4000/2023/12/13/robotics-2024-part1.html" rel="alternate" type="text/html" title="Robotics and AI in 2024 and beyond — part 1" /><published>2023-12-13T00:00:00-05:00</published><updated>2023-12-13T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/13/robotics-2024-part1</id><content type="html" xml:base="http://localhost:4000/2023/12/13/robotics-2024-part1.html"><![CDATA[<h1 id="robotics-and-ai-in-2024-and-beyond--part-1">Robotics and AI in 2024 and beyond — part 1</h1>

<p><img src="/assets/2023-12-13-robotics-2024-part1/1.png" alt="" /></p>

<p>a robot generated by a robot (DALL-E2)</p>

<p>Robots, such as the one populating sci-fi movies, are not yet part of our lives. We do not have robots that can cook, or dust our surfaces, fold our clothes, and do laundry, clean the house, nor we have robot brains that can drive our car in any environmental conditions. The reason is simply we have not been able to create an artificial robot brain “controller” that can deal with the multi-faceted complexity of our world.</p>

<p>In this article we will explore ideas to create a robot brain and how to train it.</p>

<h1 id="physical-world-multi-modal-learning"><strong>Physical world multi-modal learning</strong></h1>

<p>Robots need a brain that can understand the world in its entirety and based on a complex knowledge graph. The complexity in robotics is that both an understanding of the world and a connection to goal-oriented applications is required to be effective.</p>

<p>For a long time, we have tried to create robot brains by patching separate modules and algorithms, and this approach has not worked. Learning to recognize objects and to grasp them, or to move in space have all the same roots in understanding the three-dimensionality of space and the relationship between spaces and object visual perception. At the same time planning motion or a sequence of process steps requires the basic understanding of the impact of the robot action in the same space and onto the same objects.</p>

<p>What is required is a common model that can learn multi-modal experiences. A robot will have a set of motion capabilities, a vector of actions, and a set of sensors, for example vision, audio, proprioception. Learning needs to correlate all these modalities with the flow of actions and changes in the environment. This means that we need a robot brain capable of evaluating the sequence of sensory inputs and providing a sequence of outputs. A unified model will be able to understand space and objects, move, and manipulate objects.</p>

<p>A robot brain needs to learn a knowledge graph in the physical world. What is a knowledge graph? It is a system that learns real-world “concepts” and can inter-link them based on their semantic and physical meaning and relationships. For example, the concept “cat” in our brain is created by seeing cats, hearing cats, interacting with cats. The fact that we can link worlds like  <em>Egyptian</em>  or  <em>cat</em>  or  <em>Mann</em> happens because these concepts are linked together in our knowledge. We need to build the same knowledge graph into a robot brain, so they can understand the world and all relationships between its components.</p>

<h1 id="goals-and-instructions"><strong>Goals and instructions</strong></h1>

<p>We need robots to help us in our everyday activities and support us in tedious repetitive tasks. Folding dry clothes after a laundry session is an example; it is a repetitive activity that is nevertheless never the same. There is always a difference in what clothes to fold, what their state is, where you pick them up, and where you lay them down. There are infinite configurations that require high levels of generalization by a robot brain. More of this in the following section (learning).</p>

<p>A robot needs to listen to some instruction and execute it. Large language models make understanding human verbal instructions possible. The robot brain now needs to break down the instructions into a plan or successive steps to achieve the goal. Consider the example, for completing a goal: “pick up the blue bottle” when standing at a corner of a room. This simple command in text needs to unleash a sequence of planned motions and real-world interactions: figure out what are traversable areas in the room, where are obstacles, where is the target bottle, and planning motion towards it and an approach to grasping the item.</p>

<p>The robot brain sequential planning is always the same: observe the environment, consult on the goal of the instructions, then perform some actions. This loop repeats many times until the goal is reached.</p>

<p>Planning may be recalling steps in an example or previously completed sequence. Or it may involve mental simulation in the representation space to evaluate multiple possible action sequences.</p>

<h1 id="embodiment"><strong>Embodiment</strong></h1>

<p>Imagine we have a video demonstration of a task. Learning to repeat the same task requires to learn a correlation between what we observe and the sequence of actions we want to perform. Here comes the first big problem: we need this data to come from the first-person perspective of the robot. In other word the sensing and actions need to be coming from the robot body. Learning would be much impossible if we cannot correlate perception and action. This requires the robot and learning experience to be embodied. The complexity is that we cannot easily get training data for such a robot unless we use an external “oracle” controller to gather them. Think of a human piloting a character in a videogame. Therefore, we need to control the robot and create a few examples of the task. This of course is a time-consuming data collection problem, given that we cannot train a robot brain with just 100 examples or maybe even 1000. We may need a large amount, maybe in the same order of magnitude as the samples used to train an LLM.</p>

<p>It would be much easier if the robot could learn to imitate actions from a video, just like we do with YouTube videos. But the chicken-in-the-egg problem is that the robot has not learned to control its own body and does not know how to “imitate” an action.</p>

<p>Where is the mirror-neuron for robots? How do we learn the capability of imitation?</p>

<p>We need to study how to develop a curriculum to learn progressively vision, 3d space, mobility, attention to other agents such as humans, and finally the ability to imitate them.</p>

<p><em>… end of part I</em></p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Robotics and AI in 2024 and beyond — part 1]]></summary></entry><entry><title type="html">Artificial scientists and autonomous laboratories</title><link href="http://localhost:4000/2023/12/05/Artificial-scientists.html" rel="alternate" type="text/html" title="Artificial scientists and autonomous laboratories" /><published>2023-12-05T00:00:00-05:00</published><updated>2023-12-05T00:00:00-05:00</updated><id>http://localhost:4000/2023/12/05/Artificial-scientists</id><content type="html" xml:base="http://localhost:4000/2023/12/05/Artificial-scientists.html"><![CDATA[<p><img src="/assets/2023-12-05-Artificial-scientists/1.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<h1 id="artificial-scientists-and-autonomous-laboratories">Artificial scientists and autonomous laboratories</h1>

<h2 id="what-is-next-for-chatgpt"><strong><em>What is next for ChatGPT?</em></strong></h2>

<p>One of the goal of machine learning is to reach the levels of artificial intelligence. AI, the word, we hear every day, is an empty promise if we are not striving to create artificial brains that can be at the level or above of our own brain.</p>

<p>In the last 20 years machine-learning, notably in the form of neural networks, has produce an incessant set of innovations: from detecting, localizing, and categorizing the content of images, to learning sequences, predictive models, natural-language processing and understanding, just to name a few. But looking back all these innovations were aimed at solving individual problems, one-by-one with a tailor crafted solution. We were always starting with a dataset of inputs and desired outputs; we have been creating neural architectures designed to take the data and the task into account and developed many disconnected one-hit-songs models.</p>

<p>But no more. Since we are and want to talk about AI, we will need to set course for a unified vision and a set of unified tools that can implement artificial brains.</p>

<p>Intelligence is about creating knowledge graphs. Knowledge Graphs (KG), what is seemingly such a dry and ethereal concept, is just concepts in our brain, ideas, that are interlinked together by learning in the physical world. Take the words “Egyptian”, “Manx”, “cat” are individual concepts in our brain that are strongly linked, because “Egyptians loved cats” and “there is a type of cat called: Manx cat”. The words are connected points in our knowledge graph.</p>

<p>Recent Large Language models (LLM) such as Generative Pre-trained Transformers (GPT — with ChatGPT being one of the most famous versions) are machine learning models that can predict the next word in a sequence of text. These models approximate a Knowledge Graph over words and portions of words, and their ability to produce high-quality text, plans, algorithms, drafts that are similar to human output gives us confidence that the path to AI is along these lines. The capabilities of these models and the emergent skills that are learned training them is still a topic of research and will be for quite some time. These models are very large and capable and were trained without fixed datasets of input and output pairs, so they are time-consuming to test.</p>

<p>What is the future for ChatGPT? It is learning in the physical world. Our brain is also implementing a knowledge graph, but it is based on multi-modal “concepts”. When we learned the concept “cat” we did so by connecting visual, auditory, sensory representations of a “cat”, interlinking the way the cat moves, feels to touch, the noise it makes. The word “cat” is just a label that we use on the concept to allow us to communicate it to other people, given that our brains are disconnected (no WiFi, ethernet or radio link between them).</p>

<p>Our future AI models are going to be called Large World Models (LWM) and they will be trained to predict the next representation in a multi-modal input data flow. The complexity of this training modality lies in the fact that the “next representation” of the real world is not as simple as predicting the next words in a text. Words are a knowledge “representation”, but multi-modal data like images and sounds are harder to break into parts. Is the next portion of the image on the right what we need to predict? Or is the one below or above? Is it the next 10 millisecond of sound, or the next note, or the next segment?</p>

<p>Learning in the physical world also requires a body. We cannot learn multi-modal if we do not have a fixed set of sensors that can record and encode these modalities. This is the path needed to train an artificial robot brain. Learning will be based on physical interaction with the environment and prediction.</p>

<p>Learning streams of “concepts” via predictive learning requires to predict the next concept from a set of previous ones. This is the main learning modality. Learning occurs in a continual fashion, but it is mostly inactive if our prediction matches the physical next state of concepts. In other world this artificial brain is ignoring every information that it already has when its predictive abilities are correct. When prediction fails, the artificial brain signals a “surprise” which enables learning; this example we stumbled on needs to be learned!</p>

<p>Reinforcement learning occurs when we need to optimize for a specific goal. Our action affect the environment in ways we also need to learn and predict. It is not yet clear how to mix multiple learning modalities that could be pushing internal representation in different directions. Also learning sequences of concepts seems simple enough, but in the physical world “concepts” streams can be tricky: they are mostly attuned to changes in the environment and are not confined to a fixed sampling in space or time. It is about noticing changes in the environment, possibly because of our actions.</p>

<p>Mental simulation must also play a fundamental role in reinforcement learning and planning, because they allow an artificial brain to be more efficient in sampling possibilities. Rather than trying every possible move at random, an effective and efficient agent needs to simulate many and only try a very small set.</p>

<h2 id="artificial-scientists"><strong><em>Artificial Scientists</em></strong></h2>

<p><img src="/assets/2023-12-05-Artificial-scientists/2.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<p>What would it take to create an artificial scientist? This is an algorithm that can read the entire body of knowledge in one or multiple fields and is able to gather data, create hypotheses, run experiments, generate reports and theses.</p>

<p>Some science like mathematics may be able to be trained purely on text and symbols. Experiments on LLM trained to perform mathematical proofs are promising. But for any physical science, the artificial scientist will be based on LWM, trained on literature and videos. Similar to human reading abilities, inferring words from vision, AI literature reading needs to be vision-based, because human-readable documents are a collection of not just text, but also plots, graphs, diagrams, tables, paragraphs titles, captions, etc. An artificial scientist needs to be able to read a plot: “what is the value of Y at X=1?” and also understand diagrams and complex figures: “what is block B connected to?”. Current LLM can already understand tables, databases, datasets by being able to write scripts and code interfacing and running on external files.</p>

<p>The multi-modal LWM needed for an artificial scientist is a vision-based input model that can read pages from sequential blocks of pixels. This allows it to be able to read all components of an article. I believe this training and system will be able to generate sound science plans just as an LLM is able to plan for many activities. In fact, most of scientific literature imbues research plans and scripts for testing hypotheses.</p>

<p>Artificial scientist trained in this fashion will thus be able to generate research plans. A research plan will begin with a set of hypotheses, a list of experiments that need to be performed. They will also have the ability to run external tools, such as simulators, code, scripts and read the output of those tools. They will be able to run tools multiple times, and aggregate all outputs into a final report, or collection of theses to summarize the results from experiments.</p>

<p>Simulations are an ideal companion to autonomous research plans because they allow to test the hypotheses and obtain ground truth. These can then constitute a new set of training examples that can self-improve until hypotheses are validated or eliminated because of lack of resources (here: time).</p>

<h2 id="autonomous-laboratories"><strong><em>Autonomous Laboratories</em></strong></h2>

<p><img src="/assets/2023-12-05-Artificial-scientists/3.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<p>An artificial scientist will be able to interface and run autonomous laboratories. These are physical machines that can be interlinked to performed experiments in large scale and with autonomy. An artificial scientist is the controller of an autonomous lab, in the same way that a brain is a controller for a body. Videos and audio are needed for an artificial scientist operating the physical world, thus the core artificial brain is a robotic LWM trained on videos operations of the laboratory, including visuals of inputs, outputs, desired outcomes. The artificial scientist controller will need to be able to read instrumentation files and reports, and examine samples either by visual or physical inspection, or by using yet another tool or machine.</p>

<p>An autonomous laboratory of the simplest form is a manufacturing robot that assembles an item based on instructions. But the combination of an artificial scientist as controller allows the laboratory to do more than just follow instructions. It can generate output based on an incomplete set of instruction, in the same way that a caption can generate an image by mean of an artist. The complexity in an autonomous lab is not just its controller, the artificial scientist, but the instruments and materials needed to operate it, and the requirements for resetting, disinfecting, cleaning and restoring each tool to the initial conditions, ready for the next batch of processing.</p>

<p>Autonomous labs need to interact with humans and thus learn human-automata teaming techniques. This is not unlike the artificial brain of a robot co-habiting space with other humans, pets, creatures.</p>

<p><img src="/assets/2023-12-05-Artificial-scientists/4.png" alt="" /></p>

<p>designed by Dall-E-2</p>

<h1 id="about-the-author">about the author</h1>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://medium.com/@culurciello/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Should we worry about AI (machine-learning)?</title><link href="http://localhost:4000/2023/05/24/Should-we-worry-about-AI.html" rel="alternate" type="text/html" title="Should we worry about AI (machine-learning)?" /><published>2023-05-24T00:00:00-04:00</published><updated>2023-05-24T00:00:00-04:00</updated><id>http://localhost:4000/2023/05/24/Should-we-worry-about-AI</id><content type="html" xml:base="http://localhost:4000/2023/05/24/Should-we-worry-about-AI.html"><![CDATA[<h1 id="should-we-worry-about-ai-machine-learning">Should we worry about AI (machine-learning)?</h1>

<p>I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind…</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*k0dmcWMTEoGspk3y_CztiA.png" alt="" /></p>

<p>Artificial intelligence that is nice — by DALL-E2</p>

<p>I have to start this discussion with a note. AI = artificial intelligence. What that really means is not well defined. In my mind intelligence means being able to use acquired knowledge to solve problems never encountered before, optimizing utility.</p>

<p>These days the words “AI” are everywhere, but maybe we should start by saying that most of what is labeled AI today is really just “a machine learning algorithm and architecture”.</p>

<p>I like to start with this because I think it sets the tone for everything else. People have seen movies about AI and what it can do. They have seen AI controlling spaceships that get rid of their crew. AI that creates robot to wipe out the human race. AI that starts an atomic war and ends our planet.</p>

<p>There are also movies about AI curing all diseases, designing new devices, re-programming humans for the betterment of our world (yep!). And many more fantasies that paint a more rosy future.</p>

<p><strong>These are movies, stories, fantasies. They are not real.</strong></p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*iAM7eBIWa5tahJmitQTRIw.png" alt="" /></p>

<p>Artificial intelligence that is evil — by DALL-E2</p>

<blockquote>
  <p>the question is: can they be real one day?</p>
</blockquote>

<p>The answer is: nobody knows; we cannot predict the future. But because we are holding machines at a higher standard than humans, maybe we can take a quick look and summarize where humans stand in all this. What have humans done to other humans and to the environment so far in the last 100,000s of years since we have been around?</p>

<p>Humans grouped together and started to gather all possible resources from the environment. They would kill other animals, humans without much thought if only to take their land and resources. For a long time we were not able to destabilize the planet, beside being so nasty to each other. But then in the late 1800s we started building large machines that can cut trees and flatten the soil easily. Then the last 200 years we have be raping the planet with no end in sight. To the point that today we know well that if we keep going ahead we will have no planet any longer, no home to live in.</p>

<blockquote>
  <p>We are eating away our own Mother Earth, and we not very nice about it.</p>
</blockquote>

<p>Now in the last 50 years or so we have built computers and we are trying to replicate our brain into machines. Given our past, we should be concerned about what we are building. After all the previous machines were not very nice to the planet. And our war machinery also is not very nice to other humans. In the last 50 years we have created weapons that can wipe out all living things in this planet if we wanted to.</p>

<p>So yes I would think we should be concerned about anything that we are building, AI or not.</p>

<p>If we succeed in building a real artificial brain like our own or better, we are basically bypassing evolution. This artificial intelligence can possibly create much more dangerous machines than the ones we build before. This artificial intelligence may start to see us humans as a competitor for Earth resources. This artificial intelligence may think of us the same thoughts we think of insects and other animals: that it is ok to keep them around as long as they are not in our way. Or the planet for that matter: we can keep this forest as long as we do not need it for a new housing development.</p>

<p>Maybe this artificial intelligence will be so superior that it may reprogram our brains so that we can act as machines for its cause. But then why? It may be able to build much better machines than we can, and we would just be useless. I seriously do not believe it will use us as slaves. I do not think it will use us as batteries. But maybe this artificial intelligence may destroy the atmospehere of our planet while building the machines that it needs. Too bad we will not be able to breath. We are insect after all.</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*sErepqwaT4d6ZADAAL0qIw.png" alt="" /></p>

<p>Artificial intelligence that is evil — by DALL-E2</p>

<p><strong>Even these are just stories, fantasies, movies to be made</strong></p>

<p>Nobody knows the future. Or maybe we do know it a bit, don’t we? Yes! If we keep killing forests and producing unnatural gases, the planet will heat up. By becoming too warm the planet not be a cradle for life any longer, and one day become a boring desert like Mars (sorry Mars~!).</p>

<p>That artificial intelligence will be fine, by the way. It will just build a spaceship and travel the Universe, not to find a new home or to conquer it. Maybe it will just roam the Universe because it is fun to explore. Especially when you have infinite time. The Sun becomes a red giant and engulfs our Solar system? Not a problem, there are almost infinite galaxies out there, each with trillions of stars and planets. It is an adventure that never has to end.</p>

<p>The artificial intelligence we built maybe will not care much about us after all. We become insignificant, like the single grains of dust that make up this Universe.</p>

<p>That said… we are here today and we need to make sure our machine learning algorithms and architectures produce some value. That means maximize gain and minimize losses. But for who? The answer is simple actually: for the entire Earth as an ecosystem. We would not be here if there was no such ecosystem. So that need to be preserve even if it means sacrificing human goals and ambition and comfort and expansion.</p>

<p>But we do not have good track record of caring about our own species, sexes, races, and our planet. So I am not so hopeful about our abilities to control machines. We have always gone as far as we could, but one day going far will mean we cannot turn back. We cannot undo our mistakes.</p>

<p>Today we should build algorithms that are fair, that treat all humans alike. And maybe we should actually treat all human fairly and alike in any condition, in any part of the day, months, years. In any weather, in any state, in any country. In any race, in any religion or belief. We do not have a good track record of that.</p>

<p>Maybe we should build algorithms that do not offend one race or prefer another. And maybe we should again do that in all aspect of our lives, not just in the context of AI. We do not have a track record of that either.</p>

<p>Maybe we should build conversational algorithms that will never produce any offending dialogue. Even if we trained these algorithms with our human data, much of which is not a proper teaching material. We do not have a good track record of teaching machines nice conversational material because our website are infested with our toxicity, our hatred, our ill, our evil, our envy. We do not have a good track record of containing our emotions, both good and bad.</p>

<p>Maybe we should have an overseeing department in every company that builds machine learning algorithms. But who is in that department? Other humans? And what are their beliefs? Are they representative of all humans? We do not have a good track record of creating such groups of people, of such departments.</p>

<p><img src="/assets/2023-05-24_Should-we-worry-about-AI--machine-learning/1*ABZzdLsG6PfC0x07omiU4Q.png" alt="" /></p>

<p>a better future for humans, our planet and AI — by DALL-E2</p>

<p>Or maybe we should stop developing AI altogether — yes! That will surely save us. And maybe we should stop building weapons too, and removing trees and forests, and exploiting other humans, and all the other negative things we do that are innumerable. We also do not have a good track record of that. Sure we were able to disarm for a while and remove atomic bombs. But how fast can they come back that day that some of us decide they really need them?</p>

<p>Or maybe we should just do a better job in promoting education, in learning that we have not been very nice to each other, and to our planet. And learn that we are all the same after all, and that even insects are needed for our survival. Maybe we should learn more and grow our own human intelligence before we try to build an artificial one. Maybe armed with that collective and shared knowledge, capabilities, wealth, resources, maybe we will be able to make better choices for us, the planet, and AI.</p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me: <a href="https://medium.com/@culurciello/">Medium</a>, <a href="https://culurciello.github.io/">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a> to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Should we worry about AI (machine-learning)?]]></summary></entry><entry><title type="html">A picture is worth a 1000 words</title><link href="http://localhost:4000/2023/03/30/A-picture-is-worth-a-1000-words.html" rel="alternate" type="text/html" title="A picture is worth a 1000 words" /><published>2023-03-30T00:00:00-04:00</published><updated>2023-03-30T00:00:00-04:00</updated><id>http://localhost:4000/2023/03/30/A-picture-is-worth-a-1000-words</id><content type="html" xml:base="http://localhost:4000/2023/03/30/A-picture-is-worth-a-1000-words.html"><![CDATA[<h1 id="a-picture-is-worth-a-1000-words">A picture is worth a 1000 words</h1>

<p>Evolution of large language models into artificial brains through multiple sensory modalities</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*aO15HKrFXKKk6OOzjPOQGA.jpg" alt="" /></p>

<p>As surprising as it is our very human predictive abilities hide a path to understanding our intelligence. How do we solve so many tasks, learn so many new experiences, how do we so easily adapt to new environments?</p>

<p>The recent (2022–23) success of large language models (LLM) like ChatGPT and GPT-X paints a story. Trained to predict the next words in a sentence, these models showed superb language proficiency. The key ingredients are large amounts of data — more than you can read in a lifetime, and a simple predictive algorithm.</p>

<p>When we think of language we often forget it is an abstract representation of reality, of the world we live in, used to tag concepts and ideas of the real world into a set of labels we can use to communicate. But “the rose exist before its name”, meaning that any object in the real world, like a rose, is real even without a word for it. What is a “rose” then?</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*zYre8KPdW4OXRLcgwFE0tw.jpg" alt="" /></p>

<blockquote>
  <p>A picture is worth a thousand words</p>
</blockquote>

<p>A concept in our environment is just a collection of data from our sensors. For us humans a rose is equivalent to its image, its perfume, its delicate touch, its movement in the wind. That “concept” is the essence of an object, or our qualia, our subjective ensemble of sensory information fused into one token of knowledge.</p>

<p>This is the future of artificial brains and the evolution of LLM: being able to represent “concepts” from their multi-modal information.</p>

<h4 id="multiple-modalities">Multiple modalities</h4>

<p>LLM are trained with text only, here instead we propose to use:</p>

<ul>
  <li>visual information</li>
  <li>audio</li>
  <li>touch, proprioception</li>
  <li>text, tags, etc.</li>
</ul>

<p>This requires the artificial brain to be embodied, or at very least have a fixed set of sensors that maintain consistency over the entire learning life.</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*mkHVjGCOhUAaAjaRmt9XpA.jpg" alt="" /></p>

<h4 id="learning">Learning</h4>

<p>Training these new multi-modal artificial brains uses powerful unsupervised learning algorithms. Or should we call these self-supervised? Since there is a undeniable need for data to train on, but not one that comes with labels.</p>

<p>How do learning algorithms evolve from predicting the next word in a sentence? There are multiple ways to predict concepts:</p>

<ul>
  <li>predicting the next concept in a video or audio stream</li>
  <li>predicting what is around a concept in visual space</li>
</ul>

<p>Self-supervision is in essence the search for sequential data by using time or by using data perturbation in space that occur naturally, such as noise masking in audio, occlusion in vision. Turning static data into sequence can be done by:</p>

<ul>
  <li>manipulating an object, changing point of view</li>
  <li>listening to the concept multiple times</li>
  <li>masking portions of the data</li>
</ul>

<p>See Note 1 below for additional comments.</p>

<p>Of course there is the problem of training cost and scale. Few institution can train models that cost many M$. For example GPT cost &gt; 5M$ to train. We need a distributed and open system to create foundational models, one that is operated and serves a large majority of people. See <a href="https://medium.com/@culurciello/how-can-we-make-transformers-large-language-models-gpt-more-sharable-how-can-we-make-them-grow-6b144f0b0d7c">this post</a> also for more details.</p>

<p><img src="/assets/2023-03-30_A-picture-is-worth-a-1000-words/1*u9XjIYBoUfw2k4GOyJKr_g.jpg" alt="" /></p>

<p>The co-occurence of multiple sensory domains in space or time is another foundational learning algorithm that can be used to learn “concepts”. See <a href="https://medium.com/@culurciello/drop-the-dataset-b3ed418bd69d">this post</a> and <a href="https://medium.com/@culurciello/drop-your-dataset-part-2-c97433765998">this post</a> about this.</p>

<h4 id="what-makes-these-multi-modal-models-candidates-for-artificial-brains">What makes these multi-modal models candidates for artificial brains?</h4>

<p>Knowledge is organized in a graph where nodes are concepts and links form relationships between concepts. These links connect ideas and experiences into sets that are relationally-linked to each other, can summon and elicit each other.</p>

<p>This is not much different from the language models that arise from training an LLM. Language models are also a graph that connects words together and sets the rules for creating sentences, give rise to semantical meaning, group information.</p>

<p>We are fascinated by the 0-shot abilities of LLM today, their ability to perform tasks for which they were not explicitly trained for. This ability will be even more pronounced in multi-modal artificial brains because knowledge will be inherently connected across domains, allowing to reason about images and videos and audio, correlating speech to words, words to frames, lyrics to music.</p>

<p>A good definition of intelligence, after all, may very well just be our ability to connect concepts from multiple ares of knowledge without an explicit prompt.</p>

<h4 id="lifelong-learning">Lifelong Learning</h4>

<p>Ask an LLM what happened to the World Cup Soccer of 2022, and it will tell you that its factual knowledge stops in mid-2021. How do we keep artificial brains up-to-date? The same way we humans keep abreast of new knowledge: by continuing to learn.</p>

<p>It is easy to keep training a neural network, you just give it more samples every day. What is important to address in research and development is the drift in knowledge that can be caused by an unbalanced training. This happens for humans too, when environmental changes can lead knowledge and ideas to drift. Forgetting is also common when a set of knowledge is not refreshed for some time. Artificial brains will have the same and identical problems.</p>

<h4 id="using-tools">Using tools</h4>

<p>An artificial brain will not need to become a clone of each and every tool. It will not need to memorize all Earth’s knowledge but rather be able to retrieve it. Not become a calculator, but rather learn to use one. Not dig holes, but rather drive tractors and machines.</p>

<p>By showing examples sequences of how to use a tool and how to extract knowledge from a website, as an example, is again an example of parallel with human training. Are these applications that can be built on top of these artificial brains? Yes most definitely, as the predictive ability of the model is able to learn a sequence of steps to accomplish goals. Again, similarly to training a human to use a tool.</p>

<p>The interesting thought that arises now is that these artificial brains are in fact starting to be closer and closer to a human brain, with parallels on curriculum training to incrementally increase abilities and knowledge.</p>

<h4 id="final-notes">Final notes</h4>

<p>So yep — multi-modal, prediction, co-occurence. All ingredient of artificial brains implemented with neural networks. All this is the future you will see happen in just a few months.</p>

<p>The quick development of artificial brains that are becoming more and more capable is scaring a lot of people, as it happened before for any new and disruptive technology. Yet even if all of us wanted to use these models at large-scale, there are significant hurdles we need to pass. One is the cost of training deploying models. Two is the inability to share models and keep on training them while sharing the benefits. Three is the centralization of such model because of the cost and expertise needed to train them.</p>

<p>The rise of artificial brains will bring out at once all the problems we have in current neural networks: continual learning, training and inference efficiency, distributed learning, knowledge drift and forgetting, dataset biases, ethical use, toxic behavior, and more.</p>

<p>Much work to be done, between little and big steps forward. Are you ready to join us for the research ahead?</p>

<h4 id="note-1">Note 1</h4>

<p>One complication is that in a stream of concepts from a video, for example, predicting the next concepts does not mean predicting the next frame in a video, but rather predict how concepts morph in time. Prediction in video does not translate to fixed windows, but needs to understand when concepts changed “enough” from each other. A bit of a chicken-in-egg problem, isn’t it? But this is a good examples of the pitfalls of self-supervised learning and current methods in AI/ML. More insights are needed here, more research.</p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me: <a href="https://culurciello.medium.com/">Medium</a>, <a href="https://culurciello.github.io/">webpage</a>, <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>, <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a> to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A picture is worth a 1000 words]]></summary></entry><entry><title type="html">How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?</title><link href="http://localhost:4000/2023/01/25/Transformers-GPT.html" rel="alternate" type="text/html" title="How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?" /><published>2023-01-25T00:00:00-05:00</published><updated>2023-01-25T00:00:00-05:00</updated><id>http://localhost:4000/2023/01/25/Transformers-GPT</id><content type="html" xml:base="http://localhost:4000/2023/01/25/Transformers-GPT.html"><![CDATA[<h1 id="how-can-we-make-transformers-large-language-models-gpt-more-sharable-how-can-we-make-them-grow">How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?</h1>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*vd1bjSTykGLQEIcqP6d15A.jpg" alt="" /></p>

<p>language models and transformers in  <a href="https://huggingface.co/spaces/stabilityai/stable-diffusion">https://huggingface.co/spaces/stabilityai/stable-diffusion</a></p>

<p>Today I was reading the excellent list of Transformers here:  <a href="https://github.com/NielsRogge/Transformers-Tutorials">https://github.com/NielsRogge/Transformers-Tutorials</a>  and soon enough realized how many separate applications and separate datasets these have been trained on. Most of these neural network are offered as pre-trained and can be used and fine-tuned for your custom application. That is super great for someone that is looking for a specific problem to solve.</p>

<p>For example, you can get a model to turn an image of a receipt into a JSON table, or you can extract text from a document, or get depth estimation from a single image, etc.</p>

<blockquote>
  <p>All of these are separate tasks trained in isolation</p>
</blockquote>

<p>One thing that one realizes pretty quickly by training neural networks is that training on more data, more tasks, more abilities always gets you a better model. One that can do much better in the one application you care about, even if you are not looking for a do-it-all model.</p>

<blockquote>
  <p>So for me this is really  <strong>a big problem!</strong></p>
</blockquote>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*Y0EevVSx0gY82J23st4QTg.jpg" alt="" /></p>

<h4 id="a-set-of-problems">A set of problems:</h4>

<ul>
  <li>models are trained in isolation</li>
  <li>use a lot of computing resources to train</li>
  <li>are only effective in their own domain</li>
  <li>use many custom input and output encodings, symbols, techniques</li>
  <li>are not able to share knowledge model to model</li>
</ul>

<blockquote>
  <p>All the knowledge these models learned, it is not shared!</p>
</blockquote>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*W7MnOXOw3LfnrFVJz_mosw.jpg" alt="" /></p>

<h4 id="a-set-of-questions">A set of questions:</h4>

<p>How can we share these models and what they learn?</p>

<p>How can we build a unified model?</p>

<p>How can we make sure all data uses the same formats and is compatible? Should we learn those transformations also?</p>

<p><img src="/assets/2023-01-25_Transformers-GPT/1*W-t7a4jHBYp7ZbKQindF5w.jpg" alt="" /></p>

<h3 id="about-the-author">about the author</h3>

<p>I have more than 20 years of experience in neural networks in both hardware and software (a rare combination). About me:  <a href="https://culurciello.medium.com/">Medium</a>,  <a href="https://culurciello.github.io/">webpage</a>,  <a href="https://scholar.google.com/citations?user=SeGmqkIAAAAJ">Scholar</a>,  <a href="https://www.linkedin.com/in/eugenioculurciello/">LinkedIn</a>.</p>

<p>If you found this article useful, please consider a  <a href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=Q3FHE3BWSC72W">donation</a>  to support more tutorials and blogs. Any contribution can make a difference!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How can we make Transformers, Large Language models, GPT, more sharable? How can we make them grow?]]></summary></entry></feed>